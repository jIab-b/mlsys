# AUTO-GENERATED by run.py
# Do not edit directly; edit kernel.cu/bindings.py/ptx_lib/*.cuh instead.

CUDA_SRC = r'''
// ----- ptx_common.cuh -----
#pragma once

#include <stdint.h>

// Common helpers for PTX inline asm wrappers.

#if defined(__CUDA_ARCH__)
#define PTX_DEVICE __device__ __forceinline__

PTX_DEVICE inline uint32_t ptx_laneid() {
  uint32_t lane;
  asm volatile("mov.u32 %0, %laneid;" : "=r"(lane));
  return lane;
}

PTX_DEVICE inline uint32_t ptx_activemask() {
  uint32_t mask;
  asm volatile("activemask.b32 %0;" : "=r"(mask));
  return mask;
}

PTX_DEVICE inline bool ptx_elect_one_sync() {
  uint32_t mask = ptx_activemask();
  int leader = __ffs(mask) - 1;
  return (int)ptx_laneid() == leader;
}

#else
#define PTX_DEVICE inline

PTX_DEVICE inline bool ptx_elect_one_sync() { return true; }
#endif

#ifndef PTX_NO_ELECT
#define PTX_ELECT_ONE()          \
  do {                           \
    if (!ptx_elect_one_sync()) { \
      return;                    \
    }                            \
  } while (0)
#else
#define PTX_ELECT_ONE() do { } while (0)
#endif

// ----- ptx_mbarrier.cuh -----
#pragma once

#include "ptx_common.cuh"

// mbarrier helpers (CTA scope)

PTX_DEVICE inline void mbarrier_init(int mbar_addr, int count) {
  PTX_ELECT_ONE();
  asm volatile("mbarrier.init.shared::cta.b64 [%0], %1;" :: "r"(mbar_addr), "r"(count));
}

PTX_DEVICE inline void mbarrier_arrive_expect_tx(int mbar_addr, int size) {
  PTX_ELECT_ONE();
  asm volatile("mbarrier.arrive.expect_tx.release.cta.shared::cluster.b64 _, [%0], %1;"
              :: "r"(mbar_addr), "r"(size) : "memory");
}

PTX_DEVICE inline void mbarrier_wait(int mbar_addr, int phase) {
  asm volatile(
      "{\n\t"
      ".reg .pred P1;\n\t"
      "WAIT: \n\t"
      "mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 P1, [%0], %1, %2;\n\t"
      "@P1 bra WAIT;\n\t"
      "}\n\t"
      :: "r"(mbar_addr), "r"(phase), "r"(0xFFFFFFFF));
}

PTX_DEVICE inline void mbarrier_wait_relaxed(int mbar_addr, int phase) {
  asm volatile(
      "{\n\t"
      ".reg .pred P1;\n\t"
      "WAIT: \n\t"
      "mbarrier.try_wait.parity.relaxed.cta.shared::cta.b64 P1, [%0], %1, %2;\n\t"
      "@P1 bra WAIT;\n\t"
      "}\n\t"
      :: "r"(mbar_addr), "r"(phase), "r"(0xFFFFFFFF));
}

PTX_DEVICE inline void mbarrier_fence_init_release() {
  PTX_ELECT_ONE();
  asm volatile("fence.mbarrier_init.release.cluster;" ::: "memory");
}


// ----- ptx_tcgen05_cp.cuh -----
#pragma once

#include "ptx_common.cuh"

// tcgen05.cp wrappers (CTA group 1 only for now)

template <int CTA_GROUP = 1>
PTX_DEVICE inline void tcgen05_cp_32x128b_warpx4(int taddr, uint64_t s_desc) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile("tcgen05.cp.cta_group::1.32x128b.warpx4 [%0], %1;" :: "r"(taddr), "l"(s_desc));
}

template <int CTA_GROUP = 1>
PTX_DEVICE inline void tcgen05_cp_128x128b(int taddr, uint64_t s_desc) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile("tcgen05.cp.cta_group::1.128x128b [%0], %1;" :: "r"(taddr), "l"(s_desc));
}

template <int CTA_GROUP = 1>
PTX_DEVICE inline void tcgen05_cp_128x256b(int taddr, uint64_t s_desc) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile("tcgen05.cp.cta_group::1.128x256b [%0], %1;" :: "r"(taddr), "l"(s_desc));
}


// ----- ptx_tcgen05_ldst.cuh -----
#pragma once

#include "ptx_common.cuh"

// tcgen05.ld / tcgen05.st wrappers

// see https://docs.nvidia.com/cuda/inline-ptx-assembly/index.html
struct SHAPE {
  static constexpr char _32x32b[]  = ".32x32b";
  static constexpr char _16x128b[] = ".16x128b";
  static constexpr char _16x256b[] = ".16x256b";
};

struct NUM {
  static constexpr char x1[]   = ".x1";
  static constexpr char x2[]   = ".x2";
  static constexpr char x4[]   = ".x4";
  static constexpr char x8[]   = ".x8";
  static constexpr char x16[]  = ".x16";
  static constexpr char x32[]  = ".x32";
  static constexpr char x64[]  = ".x64";
  static constexpr char x128[] = ".x128";
};

template <int NUM_REGS, const char *SHAPE, int NUM>
PTX_DEVICE inline void tcgen05_ld(float *tmp, int row, int col) {
  int addr = (row << 16) | col;

  if constexpr (NUM_REGS == 1)
  asm volatile("tcgen05.ld.sync.aligned%2.x%3.b32 {%0}, [%1];"
              : "=f"(tmp[0]) : "r"(addr), "C"(SHAPE), "n"(NUM));
  if constexpr (NUM_REGS == 2)
  asm volatile("tcgen05.ld.sync.aligned%3.x%4.b32 {%0, %1}, [%2];"
              : "=f"(tmp[0]), "=f"(tmp[1]) : "r"(addr), "C"(SHAPE), "n"(NUM));
  if constexpr (NUM_REGS == 4)
  asm volatile("tcgen05.ld.sync.aligned%5.x%6.b32 "
              "{%0, %1, %2, %3}, [%4];"
              : "=f"(tmp[0]), "=f"(tmp[1]), "=f"(tmp[2]), "=f"(tmp[3])
              : "r"(addr), "C"(SHAPE), "n"(NUM));
  if constexpr (NUM_REGS == 8)
  asm volatile("tcgen05.ld.sync.aligned%9.x%10.b32 "
              "{ %0,  %1,  %2,  %3,  %4,  %5,  %6,  %7}, [%8];"
              : "=f"(tmp[0]), "=f"(tmp[1]), "=f"(tmp[2]), "=f"(tmp[3]), "=f"(tmp[4]), "=f"(tmp[5]), "=f"(tmp[6]), "=f"(tmp[7])
              : "r"(addr), "C"(SHAPE), "n"(NUM));
  if constexpr (NUM_REGS == 16)
  asm volatile("tcgen05.ld.sync.aligned%17.x%18.b32 "
              "{ %0,  %1,  %2,  %3,  %4,  %5,  %6,  %7, "
              "  %8,  %9, %10, %11, %12, %13, %14, %15}, [%16];"
              : "=f"(tmp[ 0]), "=f"(tmp[ 1]), "=f"(tmp[ 2]), "=f"(tmp[ 3]), "=f"(tmp[ 4]), "=f"(tmp[ 5]), "=f"(tmp[ 6]), "=f"(tmp[ 7]),
                "=f"(tmp[ 8]), "=f"(tmp[ 9]), "=f"(tmp[10]), "=f"(tmp[11]), "=f"(tmp[12]), "=f"(tmp[13]), "=f"(tmp[14]), "=f"(tmp[15])
              : "r"(addr), "C"(SHAPE), "n"(NUM));
  if constexpr (NUM_REGS == 32)
  asm volatile("tcgen05.ld.sync.aligned%33.x%34.b32 "
              "{ %0,  %1,  %2,  %3,  %4,  %5,  %6,  %7, "
              "  %8,  %9, %10, %11, %12, %13, %14, %15, "
              " %16, %17, %18, %19, %20, %21, %22, %23, "
              " %24, %25, %26, %27, %28, %29, %30, %31}, [%32];"
              : "=f"(tmp[ 0]), "=f"(tmp[ 1]), "=f"(tmp[ 2]), "=f"(tmp[ 3]), "=f"(tmp[ 4]), "=f"(tmp[ 5]), "=f"(tmp[ 6]), "=f"(tmp[ 7]),
                "=f"(tmp[ 8]), "=f"(tmp[ 9]), "=f"(tmp[10]), "=f"(tmp[11]), "=f"(tmp[12]), "=f"(tmp[13]), "=f"(tmp[14]), "=f"(tmp[15]),
                "=f"(tmp[16]), "=f"(tmp[17]), "=f"(tmp[18]), "=f"(tmp[19]), "=f"(tmp[20]), "=f"(tmp[21]), "=f"(tmp[22]), "=f"(tmp[23]),
                "=f"(tmp[24]), "=f"(tmp[25]), "=f"(tmp[26]), "=f"(tmp[27]), "=f"(tmp[28]), "=f"(tmp[29]), "=f"(tmp[30]), "=f"(tmp[31])
              : "r"(addr), "C"(SHAPE), "n"(NUM));
}

// Limited tcgen05.st variants (b32). Use uint32_t registers.
template <int NUM_REGS, const char *SHAPE, int NUM>
PTX_DEVICE inline void tcgen05_st(uint32_t const* tmp, int row, int col) {
  int addr = (row << 16) | col;

  if constexpr (NUM_REGS == 1)
  asm volatile("tcgen05.st.sync.aligned%2.x%3.b32 [%0], {%1};"
              :: "r"(addr), "r"(tmp[0]), "C"(SHAPE), "n"(NUM));
  if constexpr (NUM_REGS == 2)
  asm volatile("tcgen05.st.sync.aligned%3.x%4.b32 [%0], {%1, %2};"
              :: "r"(addr), "r"(tmp[0]), "r"(tmp[1]), "C"(SHAPE), "n"(NUM));
  if constexpr (NUM_REGS == 4)
  asm volatile("tcgen05.st.sync.aligned%5.x%6.b32 [%0], {%1, %2, %3, %4};"
              :: "r"(addr), "r"(tmp[0]), "r"(tmp[1]), "r"(tmp[2]), "r"(tmp[3]), "C"(SHAPE), "n"(NUM));
  if constexpr (NUM_REGS == 8)
  asm volatile("tcgen05.st.sync.aligned%9.x%10.b32 [%0], "
              "{%1, %2, %3, %4, %5, %6, %7, %8};"
              :: "r"(addr), "r"(tmp[0]), "r"(tmp[1]), "r"(tmp[2]), "r"(tmp[3]), "r"(tmp[4]), "r"(tmp[5]), "r"(tmp[6]), "r"(tmp[7]), "C"(SHAPE), "n"(NUM));
  if constexpr (NUM_REGS == 16)
  asm volatile("tcgen05.st.sync.aligned%17.x%18.b32 [%0], "
              "{%1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16};"
              :: "r"(addr),
                 "r"(tmp[ 0]), "r"(tmp[ 1]), "r"(tmp[ 2]), "r"(tmp[ 3]), "r"(tmp[ 4]), "r"(tmp[ 5]), "r"(tmp[ 6]), "r"(tmp[ 7]),
                 "r"(tmp[ 8]), "r"(tmp[ 9]), "r"(tmp[10]), "r"(tmp[11]), "r"(tmp[12]), "r"(tmp[13]), "r"(tmp[14]), "r"(tmp[15]),
                 "C"(SHAPE), "n"(NUM));
  if constexpr (NUM_REGS == 32)
  asm volatile("tcgen05.st.sync.aligned%33.x%34.b32 [%0], "
              "{%1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16, "
              "%17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, %28, %29, %30, %31, %32};"
              :: "r"(addr),
                 "r"(tmp[ 0]), "r"(tmp[ 1]), "r"(tmp[ 2]), "r"(tmp[ 3]), "r"(tmp[ 4]), "r"(tmp[ 5]), "r"(tmp[ 6]), "r"(tmp[ 7]),
                 "r"(tmp[ 8]), "r"(tmp[ 9]), "r"(tmp[10]), "r"(tmp[11]), "r"(tmp[12]), "r"(tmp[13]), "r"(tmp[14]), "r"(tmp[15]),
                 "r"(tmp[16]), "r"(tmp[17]), "r"(tmp[18]), "r"(tmp[19]), "r"(tmp[20]), "r"(tmp[21]), "r"(tmp[22]), "r"(tmp[23]),
                 "r"(tmp[24]), "r"(tmp[25]), "r"(tmp[26]), "r"(tmp[27]), "r"(tmp[28]), "r"(tmp[29]), "r"(tmp[30]), "r"(tmp[31]),
                 "C"(SHAPE), "n"(NUM));
}

// Convenience wrappers
PTX_DEVICE inline void tcgen05_ld_32x32b(float *tmp, int row, int col, int num) {
  if (num == 1)  tcgen05_ld<1,  SHAPE::_32x32b, 1>(tmp, row, col);
  if (num == 2)  tcgen05_ld<2,  SHAPE::_32x32b, 2>(tmp, row, col);
  if (num == 4)  tcgen05_ld<4,  SHAPE::_32x32b, 4>(tmp, row, col);
  if (num == 8)  tcgen05_ld<8,  SHAPE::_32x32b, 8>(tmp, row, col);
  if (num == 16) tcgen05_ld<16, SHAPE::_32x32b, 16>(tmp, row, col);
  if (num == 32) tcgen05_ld<32, SHAPE::_32x32b, 32>(tmp, row, col);
}

PTX_DEVICE inline void tcgen05_ld_16x128b(float *tmp, int row, int col, int num) {
  if (num == 1)  tcgen05_ld<1,  SHAPE::_16x128b, 1>(tmp, row, col);
  if (num == 2)  tcgen05_ld<2,  SHAPE::_16x128b, 2>(tmp, row, col);
  if (num == 4)  tcgen05_ld<4,  SHAPE::_16x128b, 4>(tmp, row, col);
  if (num == 8)  tcgen05_ld<8,  SHAPE::_16x128b, 8>(tmp, row, col);
  if (num == 16) tcgen05_ld<16, SHAPE::_16x128b, 16>(tmp, row, col);
  if (num == 32) tcgen05_ld<32, SHAPE::_16x128b, 32>(tmp, row, col);
}

PTX_DEVICE inline void tcgen05_ld_16x256b(float *tmp, int row, int col, int num) {
  if (num == 1)  tcgen05_ld<1,  SHAPE::_16x256b, 1>(tmp, row, col);
  if (num == 2)  tcgen05_ld<2,  SHAPE::_16x256b, 2>(tmp, row, col);
  if (num == 4)  tcgen05_ld<4,  SHAPE::_16x256b, 4>(tmp, row, col);
  if (num == 8)  tcgen05_ld<8,  SHAPE::_16x256b, 8>(tmp, row, col);
  if (num == 16) tcgen05_ld<16, SHAPE::_16x256b, 16>(tmp, row, col);
  if (num == 32) tcgen05_ld<32, SHAPE::_16x256b, 32>(tmp, row, col);
}


// ----- ptx_tcgen05_mma.cuh -----
#pragma once

#include "ptx_common.cuh"

// tcgen05.mma wrappers (CTA group 1 only for now)

struct COLLECTOR_USAGE {
  static constexpr char NONE[]      = "";
  static constexpr char A_FILL[]    = ".collector::a::fill";
  static constexpr char A_USE[]     = ".collector::a::use";
  static constexpr char A_LASTUSE[] = ".collector::a::lastuse";
  static constexpr char A_DISCARD[] = ".collector::a::discard";
};

// Block-scaled NVFP4 MMA (smem A/B descriptors, tmem scales)
template <int CTA_GROUP = 1, const char *collector_usage = COLLECTOR_USAGE::NONE>
PTX_DEVICE inline void tcgen05_mma_mxf4nvf4_block16(
  int d_tmem,
  uint64_t a_desc,
  uint64_t b_desc,
  uint32_t idesc,
  int scale_A_tmem,
  int scale_B_tmem,
  int enable_input_d
) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile(
    "{\n\t"
    ".reg .pred p;\n\t"
    "setp.ne.b32 p, %6, 0;\n\t"
    "tcgen05.mma.cta_group::1.kind::mxf4nvf4.block_scale.block16%7 [%0], %1, %2, %3, [%4], [%5], p;\n\t"
    "}"
    :: "r"(d_tmem), "l"(a_desc), "l"(b_desc), "r"(idesc),
       "r"(scale_A_tmem), "r"(scale_B_tmem), "r"(enable_input_d),
       "C"(collector_usage)
  );
}

// F16/BF16 MMA, SS (A/B in smem desc), C in tmem
PTX_DEVICE inline void tcgen05_mma_f16_ss(
  uint32_t tmem_c,
  uint64_t desc_a,
  uint64_t desc_b,
  uint32_t idesc,
  int accumulate
) {
  PTX_ELECT_ONE();
  uint32_t mask[4] = {0, 0, 0, 0};
  asm volatile(
    "{\n\t"
    ".reg .pred p;\n\t"
    "setp.ne.b32 p, %4, 0;\n\t"
    "tcgen05.mma.cta_group::1.kind::f16 [%0], %1, %2, %3, {%5, %6, %7, %8}, p;\n\t"
    "}"
    :: "r"(tmem_c), "l"(desc_a), "l"(desc_b), "r"(idesc), "r"(accumulate),
       "r"(mask[0]), "r"(mask[1]), "r"(mask[2]), "r"(mask[3])
  );
}

// F16/BF16 MMA, TS (A in tmem, B in smem desc), C in tmem
PTX_DEVICE inline void tcgen05_mma_f16_ts(
  uint32_t tmem_c,
  uint32_t tmem_a,
  uint64_t desc_b,
  uint32_t idesc,
  int accumulate
) {
  PTX_ELECT_ONE();
  uint32_t mask[4] = {0, 0, 0, 0};
  asm volatile(
    "{\n\t"
    ".reg .pred p;\n\t"
    "setp.ne.b32 p, %4, 0;\n\t"
    "tcgen05.mma.cta_group::1.kind::f16 [%0], [%1], %2, %3, {%5, %6, %7, %8}, p;\n\t"
    "}"
    :: "r"(tmem_c), "r"(tmem_a), "l"(desc_b), "r"(idesc), "r"(accumulate),
       "r"(mask[0]), "r"(mask[1]), "r"(mask[2]), "r"(mask[3])
  );
}

// F16/BF16 MMA, WS (warp-specialized), C in tmem
PTX_DEVICE inline void tcgen05_mma_ws_f16_ts(
  uint32_t tmem_c,
  uint32_t tmem_a,
  uint64_t desc_b,
  uint32_t idesc,
  int accumulate
) {
  PTX_ELECT_ONE();
  asm volatile(
    "{\n\t"
    ".reg .pred p;\n\t"
    "setp.ne.b32 p, %4, 0;\n\t"
    "tcgen05.mma.ws.cta_group::1.kind::f16 [%0], [%1], %2, %3, p, 0;\n\t"
    "}"
    :: "r"(tmem_c), "r"(tmem_a), "l"(desc_b), "r"(idesc), "r"(accumulate)
  );
}


// ----- ptx_tcgen05_sync.cuh -----
#pragma once

#include "ptx_common.cuh"

// tcgen05 commit, wait, fence wrappers (CTA group 1 only for now)

template <int CTA_GROUP = 1>
PTX_DEVICE inline void tcgen05_commit(int mbar_addr) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile("tcgen05.commit.cta_group::1.mbarrier::arrive::one.shared::cluster.b64 [%0];"
              :: "r"(mbar_addr) : "memory");
}

template <int CTA_GROUP = 1>
PTX_DEVICE inline void tcgen05_commit_mcast(int mbar_addr, uint16_t cta_mask) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile("tcgen05.commit.cta_group::1.mbarrier::arrive::one.shared::cluster.multicast::cluster.b64 [%0], %1;"
              :: "r"(mbar_addr), "h"(cta_mask) : "memory");
}

PTX_DEVICE inline void tcgen05_wait_ld() {
  asm volatile("tcgen05.wait::ld.sync.aligned;" ::: "memory");
}

PTX_DEVICE inline void tcgen05_wait_st() {
  asm volatile("tcgen05.wait::st.sync.aligned;" ::: "memory");
}

PTX_DEVICE inline void tcgen05_fence_before_thread_sync() {
  asm volatile("tcgen05.fence::before_thread_sync;" ::: "memory");
}

PTX_DEVICE inline void tcgen05_fence_after_thread_sync() {
  asm volatile("tcgen05.fence::after_thread_sync;" ::: "memory");
}


// ----- ptx_tma.cuh -----
#pragma once

#include "ptx_common.cuh"

// TMA bulk tensor loads (CTA group 1 only for now)

template <int CTA_GROUP = 1>
PTX_DEVICE inline void tma_1d_gmem2smem(int dst, const void *tmap_ptr, int x, int mbar_addr, uint64_t cache_policy) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile("cp.async.bulk.tensor.1d.shared::cta.global.mbarrier::complete_tx::bytes.cta_group::1.L2::cache_hint "
              "[%0], [%1, {%2}], [%3], %4;"
              :: "r"(dst), "l"(tmap_ptr), "r"(x), "r"(mbar_addr), "l"(cache_policy)
              : "memory");
}

template <int CTA_GROUP = 1>
PTX_DEVICE inline void tma_2d_gmem2smem(int dst, const void *tmap_ptr, int x, int y, int mbar_addr, uint64_t cache_policy) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile("cp.async.bulk.tensor.2d.shared::cta.global.mbarrier::complete_tx::bytes.cta_group::1.L2::cache_hint "
              "[%0], [%1, {%2, %3}], [%4], %5;"
              :: "r"(dst), "l"(tmap_ptr), "r"(x), "r"(y), "r"(mbar_addr), "l"(cache_policy)
              : "memory");
}

template <int CTA_GROUP = 1>
PTX_DEVICE inline void tma_3d_gmem2smem(int dst, const void *tmap_ptr, int x, int y, int z, int mbar_addr, uint64_t cache_policy) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile("cp.async.bulk.tensor.3d.shared::cta.global.mbarrier::complete_tx::bytes.cta_group::1.L2::cache_hint "
              "[%0], [%1, {%2, %3, %4}], [%5], %6;"
              :: "r"(dst), "l"(tmap_ptr), "r"(x), "r"(y), "r"(z), "r"(mbar_addr), "l"(cache_policy)
              : "memory");
}

template <int CTA_GROUP = 1>
PTX_DEVICE inline void tma_1d_gmem2smem_mcast(int dst, const void *tmap_ptr, int x, int mbar_addr, int16_t cta_mask, uint64_t cache_policy) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile("cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.cta_group::1.L2::cache_hint "
              "[%0], [%1, {%2}], [%3], %4, %5;"
              :: "r"(dst), "l"(tmap_ptr), "r"(x), "r"(mbar_addr), "h"(cta_mask), "l"(cache_policy)
              : "memory");
}

template <int CTA_GROUP = 1>
PTX_DEVICE inline void tma_2d_gmem2smem_mcast(int dst, const void *tmap_ptr, int x, int y, int mbar_addr, int16_t cta_mask, uint64_t cache_policy) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile("cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.cta_group::1.L2::cache_hint "
              "[%0], [%1, {%2, %3}], [%4], %5, %6;"
              :: "r"(dst), "l"(tmap_ptr), "r"(x), "r"(y), "r"(mbar_addr), "h"(cta_mask), "l"(cache_policy)
              : "memory");
}

template <int CTA_GROUP = 1>
PTX_DEVICE inline void tma_3d_gmem2smem_mcast(int dst, const void *tmap_ptr, int x, int y, int z, int mbar_addr, int16_t cta_mask, uint64_t cache_policy) {
  static_assert(CTA_GROUP == 1, "Only CTA_GROUP=1 supported for now");
  PTX_ELECT_ONE();
  asm volatile("cp.async.bulk.tensor.3d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.cta_group::1.L2::cache_hint "
              "[%0], [%1, {%2, %3, %4}], [%5], %6, %7;"
              :: "r"(dst), "l"(tmap_ptr), "r"(x), "r"(y), "r"(z), "r"(mbar_addr), "h"(cta_mask), "l"(cache_policy)
              : "memory");
}


// ----- kernel.cu -----
// AUTO-GENERATED by compile.py

extern "C" __global__ void kernel_entry() {
  // TODO: emit DSL ops here (no raw PTX).
}
'''

# Placeholder bindings.py
# TODO: replace with actual load_inline + bindings.


def compile_kernel():
    return None


def custom_kernel(data):
    raise NotImplementedError("kernel not implemented yet")
