"**Modal ‚ùå failure**

Running on:
* GPU: `NVIDIA B200`
* CPU: `x86_64`
* Device count: `1`
* Runtime: `CUDA`
* Platform: `Linux-4.4.0-x86_64-with-glibc2.39`
* Torch: `2.9.1+cu129`
* Hostname: `modal`


## Program stdout:
```
[1/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=dsa_topk_indexer_ext -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.12/dist-packages/torch/include -isystem /usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -gencode=arch=compute_100a,code=sm_100a --threads=4 --relocatable-device-code=false -std=c++17 -c /root/.cache/torch_extensions/py312_cu129/dsa_topk_indexer_ext/cuda.cu -o cuda.cuda.o 
[2/2] c++ main.o cuda.cuda.o -shared -lcuda -L/usr/local/lib/python3.12/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o dsa_topk_indexer_ext.so
--- stderr ---
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
                    ^^^^^^^^^^^^^^^^^^^
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/common/eval_base.py", line 581, in _make_benchmark_runner
    return runner.run_single_benchmark(test, recheck, max_repeats, max_time_ns)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/common/eval_base.py", line 404, in run_single_benchmark
    good, message = check_implementation(check_copy, output)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/test_kernels/dsa_index_2048/reference.py", line 118, in check_implementation
    expected = ref_kernel(data)
               ^^^^^^^^^^^^^^^^
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/test_kernels/dsa_index_2048/reference.py", line 114, in ref_kernel
    return _run_reference(q_index_fp8, k_index_cache_fp8, weights, seq_lens, block_table)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/test_kernels/dsa_index_2048/reference.py", line 61, in _run_reference
    seq_len = int(seq_lens[b].item())
                  ^^^^^^^^^^^^^^^^^^
torch.AcceleratorError: CUDA error: unspecified launch failure
Search for `cudaErrorLaunchFailure' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/test_kernels/dsa_index_2048/eval.py", line 43, in <module>
    sys.exit(main(DsaIndex2048EvalRunner()))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/common/eval_base.py", line 721, in main
    return run_benchmarking(logger, pool, tests, runner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/common/eval_base.py", line 630, in run_benchmarking
    pool.apply(_make_benchmark_runner, (runner, tests[0], False, 100, 10e7))
  File "/usr/lib/python3.12/multiprocessing/pool.py", line 360, in apply
    return self.apply_async(func, args, kwds).get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/pool.py", line 774, in get
    raise self._value
torch.AcceleratorError: CUDA error: unspecified launch failure
Search for `cudaErrorLaunchFailure' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
```"