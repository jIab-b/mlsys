tcgen05 for dummies — LLM-friendly TXT (with image-to-text placeholders)
Source page: https://gau-nernst.github.io/tcgen05/
Date on page: Dec 21, 2025
Author/site: gau-nernst's blog (Thien Tran)

NOTE ON FIDELITY
- This file is a structured plain-text transcription built from the page’s rendered text extraction.
- Images are represented in "IMAGE-AS-TEXT" blocks: (1) the image title/alt, (2) the on-page caption, and
  (3) a concise ASCII/diagrammatic description that preserves the *information* the image conveys as used in the article.

======================================================================
TITLE
======================================================================
tcgen05 for dummies

tcgen05 is the set of PTX instructions to program Tensor Cores on the latest NVIDIA Blackwell GPUs (sm100, not to be confused with consumer Blackwell sm120).
At the time of writing, the author couldn’t find a Blackwell tutorial in plain CUDA C++ with PTX, even though such exist for:
- Ampere: alexarmbr’s and spatters’
- Hopper: Pranjal’s (cudaforfun)

Goal: document the process of learning tcgen05 and reaching ~98% of CuBLAS speed on M=N=K=4096.

All B200 work was done on Modal (cloud), used to run short-lived functions for kernel iteration.
Code for this article:
https://github.com/gau-nernst/learn-cuda/tree/3b90ac9b/02e_matmul_sm100/

======================================================================
TABLE OF CONTENTS (as on page)
======================================================================
1. Recap on writing a performant matmul kernel
2. Basic tcgen05 kernel
   2.1 TMA and mbarrier for dummies
   2.2 Acquire-Release semantics
   2.3 Decipher tcgen05
3. 128-byte global load and swizzling
4. Pipelining
5. Warp specialization
6. 2-SM MMA
7. Persistent kernel (with static scheduling)
8. Closing remarks

======================================================================
1) Recap on writing a performant matmul kernel
======================================================================
Matrix multiplication between A (MxK) and B (KxN) produces C (MxN).
Each element of C is a dot product between a row of A and a column of B.

Naive matmul pseudocode:

    def matmul(A: Tensor, B: Tensor, C: Tensor, M: int, N: int, K: int):
        for m in range(M):
            for n in range(N):
                # initialize accumulator
                acc = 0

                # doing dot product along K
                for k in range(K):
                    acc += A[m, k] * B[k, n]

                # store result
                C[m, n] = acc

Most real matmuls use tiling (mini-matmuls accumulated along K):

    def tiled_matmul(A: Tensor, B: Tensor, C: Tensor, M: int, N: int, K: int):
        BLOCK_M = 128
        BLOCK_N = 128
        BLOCK_K = 64
        for m in range(0, M, BLOCK_M):
            for n in range(0, N, BLOCK_N):
                # initialize accumulator
                acc = torch.zeros(BLOCK_M, BLOCK_N)

                # doing mini matmuls along K
                for k in range(0, K, BLOCK_K):
                    # select tiles of A and B
                    A_tile = A[m:m+BLOCK_M, k:k+BLOCK_K]  # shape [BLOCK_M, BLOCK_K]
                    B_tile = B[k:k+BLOCK_K, n:n+BLOCK_N]  # shape [BLOCK_K, BLOCK_N]
                    acc += mini_matmul(A_tile, B_tile)    # shape [BLOCK_M, BLOCK_N]

                # store result
                C[m : m + BLOCK_M, n : n + BLOCK_N] = acc

IMAGE-AS-TEXT
[Image title/alt] Illustration of naive matmul and tiled matmul
[Caption] Illustration of naive matmul (left) and tiled matmul (right).
[Diagrammatic text]
- Left (naive): element-wise dot products for each (m,n): traverse K for every output scalar.
- Right (tiled): output is partitioned into BLOCK_M x BLOCK_N tiles. For each output tile, iterate over K in BLOCK_K chunks:
    - load A_tile (BLOCK_M x BLOCK_K)
    - load B_tile (BLOCK_K x BLOCK_N)
    - accumulate acc (BLOCK_M x BLOCK_N)
This highlights reuse of loaded tiles and alignment with Tensor Core tile shapes.

Commentary: A "dumb reason" tiled is better: Tensor Cores are a mini-matmul engine, so you naturally tile the problem to Tensor Core shapes. In practice there are multiple tiling levels to match hardware.

High-level GPU structure:
- m and n loops parallelized across threadblocks: each threadblock computes BLOCK_M x BLOCK_N of C.
- Each threadblock iterates along K: loads A/B tiles and does MMA.
Each generation uses different PTX for load+compute:

Generation | Load                          | Compute
-----------|-------------------------------|-------------------------
Ampere     | cp.async                      | mma
Hopper     | cp.async.bulk.tensor (TMA)    | wgmma.mma_async
Blackwell  | cp.async.bulk.tensor (TMA)    | tcgen05.mma

======================================================================
2) Basic tcgen05 kernel
======================================================================
2.1) TMA and mbarrier for dummies
--------------------------------
Tensor Memory Accelerator (TMA) (exists since Hopper):
- Issues memory loads with minimal register usage and address calculations.
- Old cp.async: max 16-byte load per thread.
- TMA: arbitrary sizes, using only 1 thread.

In PTX:
- cp.async.bulk (1D tile)
- cp.async.bulk.tensor (1D to 5D tile)

To use TMA, create a Tensor Map (CUtensorMap) on the host (CUDA Driver API).
Kernel argument types use:
- const __grid_constant__ CUtensorMap A_tmap
- const __grid_constant__ CUtensorMap B_tmap

Skeleton (as on page):

    #include <cudaTypedefs.h>  // required for CUtensorMap
    #include <cuda_bf16.h>

    constexpr int BLOCK_M = 64;
    constexpr int BLOCK_N = 64;
    constexpr int BLOCK_K = 64;

    constexpr int TB_SIZE = 128;

    __global__
    void kernel(
      const __grid_constant__ CUtensorMap A_tmap,
      const __grid_constant__ CUtensorMap B_tmap,
      nv_bfloat16 *C_ptr,
      int M, int N, int K
    ) {
      ...  // covered later
    }

    // forward declaration
    void init_2D_tmap(
      CUtensorMap *tmap,
      const nv_bfloat16 *ptr,
      uint64_t global_height, uint64_t global_width,
      uint32_t shared_height, uint32_t shared_width
    );

    void launch(
      const nv_bfloat16 *A_ptr,
      const nv_bfloat16 *B_ptr,
            nv_bfloat16 *C_ptr,
      int M, int N, int K
    ) {
      // prepare tensor map objects on the host
      CUtensorMap A_tmap, B_tmap;
      init_2D_tmap(&A_tmap, A_ptr, M, K, BLOCK_M, BLOCK_K);
      init_2D_tmap(&B_tmap, B_ptr, N, K, BLOCK_N, BLOCK_K);

      const dim3 grid(N / BLOCK_N, M / BLOCK_M);

      // 1 A tile [BLOCK_M, BLOCK_K] and 1 B tile [BLOCK_N, BLOCK_K]
      const int smem_size = (BLOCK_M + BLOCK_N) * BLOCK_K * sizeof(nv_bfloat16);

      kernel<<<grid, TB_SIZE, smem_size>>>(A_tmap, B_tmap, C_ptr, M, N, K);
    }

Assumption: both A and B are K-major (contiguous in K), aligning with PyTorch nn.Linear usage:
- Inputs: (batch_size, in_features) contiguous
- Weights: (out_features, in_features) contiguous
- Y = X @ W.T

init_2D_tmap() wraps cuTensorMapEncodeTiled() to define global & shared layouts.

IMAGE-AS-TEXT
[Image title/alt] Tensormap
[Caption] Parameters involved in a Tensormap object. globalDim, globalStrides, and boxDim are encoded inside the Tensormap object, while tensorCoords are supplied at runtime to cp.async.bulk.tensor to select the 2D tile.
[Diagrammatic text]
TensorMap encodes:
- globalDim: sizes of each dimension (order is reversed vs natural)
- globalStrides: byte strides (for dims 2..rank)
- boxDim: tile shape to copy
At runtime you supply tensorCoords = (offsets) selecting which tile, and destination shared pointer.

Example code snippet:

    void init_2D_tmap(
      CUtensorMap *tmap,
      const nv_bfloat16 *ptr,
      uint64_t global_height, uint64_t global_width,
      uint32_t shared_height, uint32_t shared_width
    ) {
      constexpr uint32_t rank = 2;

      // ordering of dims and strides is reverse of the natural PyTorch's order.
      // 1st dim is the fastest changing dim.
      uint64_t globalDim[rank] = {global_width, global_height};

      // global strides are in bytes.
      // additionally, the 1st dim is assumed to have stride of 1-element width.
      uint64_t globalStrides[rank-1] = {global_width * sizeof(nv_bfloat16)};

      // shape in shared memory
      uint32_t boxDim[rank]         = {shared_width, shared_height};
      uint32_t elementStrides[rank] = {1, 1};

      auto err = cuTensorMapEncodeTiled(
        tmap,
        CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_BFLOAT16,
        rank,
        (void *)ptr,
        globalDim,
        globalStrides,
        boxDim,
        elementStrides,
        // you can ignore the rest for now
        CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,
        CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE,
        CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,
        CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE
      );
      // check the returned error code
    }

TMA operates in the "async proxy". Mental model used in the article:
- Treat TMA as a separate device relative to CUDA cores.
- When reading data written by TMA, or writing data for TMA to read, follow PTX memory consistency model and required synchronizations.

High-level kernel flow idea (initial):
- For each K tile:
  - Issue TMA to bring A/B tiles into shared
  - Wait for TMA completion via mbarrier
  - Issue tcgen05.mma

mbarrier usage (phase parity):
- mbarrier has phases 0,1,2,... but code typically uses parity (0/1) and flips each iteration.

mbarrier_wait is essentially spinning on the barrier (Cutlass reference is in the article):

    __device__ inline
    void mbarrier_wait(int mbar_addr, int phase) {
      uint32_t ticks = 0x989680;  // optional
      asm volatile(
        "{
	"
        ".reg .pred P1;
	"
        "LAB_WAIT:
	"
        "mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 P1, [%0], %1, %2;
	"
        "@P1 bra.uni DONE;
	"
        "bra.uni LAB_WAIT;
	"
        "DONE:
	"
        "}"
        :: "r"(mbar_addr), "r"(phase), "r"(ticks)
      );
    }

(There are further details in the original post on initialization, acquire-release, and correct pairing.)

----------------------------------------------------------------------
Tensor memory (Blackwell)
----------------------------------------------------------------------
To hold MMA results, Blackwell introduces Tensor memory (tmem):
- Capacity: 128 x 512 elements, each 32-bit (good for FP32 / INT32 accum).
- Allocation: tcgen05.alloc in units of columns (all 128 rows always allocated).
- Deallocation: tcgen05.dealloc must be done before kernel exit.
- Access:
  - tcgen05.ld : tmem -> registers (epilogue)
  - tcgen05.st : registers -> tmem
  - tcgen05.cp : shared -> tmem

Allocation example fragment:

    #pragma nv_diag_suppress static_var_with_dynamic_init
    __shared__ uint64_t mbars[1];
    __shared__ int tmem_addr[1];  // tmem address is 32-bit

    if (tid == 0) {
      // lane0 of warp0 initializes mbarriers (1 thread)
      asm volatile("mbarrier.init.shared::cta.b64 [%0], %1;" :: "r"(mbar_addr), "r"(1));
      asm volatile("fence.mbarrier_init.release.cluster;");
    } else if (warp_id == 1) {
      // one full warp allocates tmem
      // tcgen05.alloc returns tmem address in shared memory
      const int addr = static_cast<int>(__cvta_generic_to_shared(tmem_addr));
      asm volatile("tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%0], %1;"
                   :: "r"(addr), "r"(BLOCK_N));
    }

(See full code: matmul_v1.cu in the linked repo.)

----------------------------------------------------------------------
Deciphering tcgen05 shared memory descriptors (context)
----------------------------------------------------------------------
The post spends time "deciphering" how to set shared memory descriptors and interpret LBO/SBO and core matrices.

IMAGE-AS-TEXT
[Image title/alt] smem descriptor v0
[Caption] My initial understanding of LBO and SBO. Each smaller tile is a core matrix within the larger threadblock tile in shared memory.
[Diagrammatic text]
- Think of the threadblock tile in SMEM as a big 2D array.
- It is partitioned into "core matrices" (8 rows x 2 tiles x 16B granularity) that align with tcgen/wgmma operand fetch.
- LBO/SBO index which core matrix (and which 8x2 sub-tiles) to address.

(Exact bitfields and final correct interpretation are detailed in the original post; see PTX doc references in the post.)

----------------------------------------------------------------------
Epilogue: tcgen05.ld and Layout D
----------------------------------------------------------------------
For epilogue, the post uses tcgen05.ld.

IMAGE-AS-TEXT
[Image title/alt] tcgen05 Data path Layout D
[Caption] tcgen05 Data path Layout D. Source: NVIDIA PTX doc.
[Diagrammatic text]
- For MMA_M=128 and .cta_group::1, Layout D partitions tmem access by warp:
  - each warp has partial/striped access to the tmem tile.
- This implies minimum 4 warps are required to read out a full BLOCK_M x BLOCK_N tile.

IMAGE-AS-TEXT
[Image title/alt] tcgen05 matrix fragment .32x32b
[Caption] tcgen05 Matrix fragment for .32x32b. Source: NVIDIA PTX doc.
[Diagrammatic text]
- A per-warp fragment is 32 x 32 (bytes?) with 32-bit accum elements.
- .x8 multiplier: each thread loads 8 consecutive FP32 accum values; they can be cast/packed into BF16 and stored as 16 bytes.

Deallocation reminder:
- Ensure all threads finish reading from tmem before dealloc (e.g., __syncthreads()).
- Then dealloc (warp0 in sample):

    __syncthreads();
    if (warp_id == 0)
      asm volatile("tcgen05.dealloc.cta_group::1.sync.aligned.b32 %0, %1;" :: "r"(taddr), "r"(BLOCK_N));

Full code reference: matmul_v1.cu in repo.

Minor note:
- elect.sync can be used instead of tid==0 for selecting issuing thread (seen in DeepGEMM). The post notes it and uses it as a good idea.

Benchmark (M=N=K=4096):
- CuBLAS (PyTorch 2.9.1 + CUDA 13): 1506.74 TFLOPS
- v1a (basic tcgen05 + 2D 16B TMA): 254.62 TFLOPS
- v1b (3D 16B TMA): 252.81 TFLOPS

Concept summary table (as on page):
- mbarrier: init + semantics to sync TMA and tcgen05.mma
- Tensor memory: alloc/dealloc + address encode/select
- TMA: encode tensor map on host; cp.async.bulk.tensor on device; sync with mbarrier
- MMA: prepare input layouts + smem descriptors; tcgen05.mma; sync with mbarrier
- Epilogue: tcgen05.ld to read out tensor memory

======================================================================
3) 128-byte global load and swizzling
======================================================================
The post moves from 16B to 128B swizzling (largest supported by TMA).

Swizzling purpose (general): avoid shared-memory bank conflicts by permuting data across banks (often XOR column indices with row indices).
Swizzling in TMA is in 16B units; e.g. SWIZZLE_128B means 8x16B units permuted according to row index.

IMAGE-AS-TEXT
[Image title/alt] Swizzling patterns in TMA
[Caption] (On-page: visualizes how 128B chunks are permuted; with a note that the visualization isn’t quite correct due to row-size constraints.)
[Diagrammatic text]
- Consider a row of 128B = 8 blocks of 16B: [b0 b1 b2 b3 b4 b5 b6 b7].
- For different rows r, the order of these 16B blocks is permuted as a function of r.
- Goal: make typical access patterns spread across all 32 banks.

Implementation note in the post:
- Kernel code does not compute swizzling explicitly.
- Instead:
  - Set CU_TENSOR_MAP_SWIZZLE_128B in tensor map encode.
  - Set bits 61-63 to 2 in the SMEM descriptor.
As long as TMA and tcgen05.mma agree on swizzle, code stays correct even if hardware details differ.

Observation about speedup:
- Swizzling is often for bank conflicts, but the first kernel may already have no bank conflicts because each 8x16B tile spans exactly 32 banks.
- A hypothesized reason: better global load behavior when the innermost TMA dimension is larger (128B).

Updated tensor-map parameter sketch (diff shown on page):
- 2D TMA rank=2:
  - globalDim: {64, M} (was {8, M})
  - globalStrides: {K * sizeof(bf16)} unchanged
  - boxDim: {64, BLOCK_M} (was {8, BLOCK_M})
- 3D TMA rank=3:
  - globalDim: {64, M, K/64} (was {8, M, K/8})
  - globalStrides: {K*sizeof(bf16), 128} (was {K*sizeof(bf16), 16})
  - boxDim: {64, BLOCK_M, BLOCK_K/64} (was {8, BLOCK_M, BLOCK_K/8})

======================================================================
4) Pipelining
======================================================================
Pipelining structure (as shown on page; details in matmul_v3.cu):
- Separate load() and compute() lambdas.
- Prefetch NUM_STAGES ahead.
- In main loop:
  - load(next)
  - wait for current load stage
  - compute(current)
  - wait for compute stage so buffer can be reused
To wait for specific TMA stages: use 1 mbarrier per TMA stage.

Benchmark:
- CuBLAS: 1506.74 TFLOPS
- v2b (3D 128B TMA): 695.43 TFLOPS
- v3 (pipelining): 939.61 TFLOPS
(~35% speedup over v2b)

======================================================================
5) Warp specialization
======================================================================
Idea (Cutlass-style):
- Only 1 thread is needed to issue TMA and MMA.
- Dedicate 1 warp to TMA, 1 warp to MMA, to avoid branching (warp_id==0) inside a unified loop.

Pseudo-structure:

    const int num_iters = K / BLOCK_K;
    if (warp_id == 0 && elect_sync()) {
      // TMA warp
      for (int iter_k = 0; iter_k < num_iters; iter_k++)
        ...
    }
    else if (warp_id == 1 && elect_sync()) {
      // MMA warp
      for (int iter_k = 0; iter_k < num_iters; iter_k++)
        ...
    }

On Blackwell/Hopper, tensor cores operate asynchronously w.r.t. threads.
So multiple tcgen05.mma can be issued without waiting for completion, but TMA must not overwrite a stage buffer still in use by MMA.

IMAGE-AS-TEXT
[Image title/alt] Warp specialization pipelining
[Caption] 4-stage pipelining in Ampere (top) and Blackwell with warp specialization (bottom).
[Diagrammatic text]
- Ampere: synchronous-ish staging between load and mma within a single warp/warpgroup.
- Blackwell warp specialization: two independent warps, each progressing their own loop, coordinated by barriers:
  - TMA warp produces SMEM stages.
  - MMA warp consumes SMEM stages and produces TMEM accum.
- Use mbarrier arrays per stage for each direction (TMA<->MMA).

Implementation sketch:
- Allocate shared mbars: NUM_STAGES for TMA, NUM_STAGES for MMA.
- Init with count=1 (only 1 issuing thread per warp), fence.mbarrier_init.release.cluster.

Important detail:
- At start, TMA should NOT wait for MMA because buffers are initially free.
- Handle by flipping phase used by TMA when waiting for MMA completion.

Correctness note:
- __syncthreads() does not guarantee MMA finished; only that instructions were issued.
- To wait for last MMA iteration, use an mbarrier (either find which stage or use a dedicated extra mbarrier and tcgen05.commit to it).

Benchmark:
- v3 (pipelining): 939.61 TFLOPS
- v4 (warp specialization): 1208.83 TFLOPS
(~29% speedup)

======================================================================
6) 2-SM MMA
======================================================================
Blackwell feature: use tensor cores across 2 threadblocks (CTA group of 2) to compute an MMA tile cooperatively.

IMAGE-AS-TEXT
[Image title/alt] 2SM MMA
[Caption] Data organization in 2-SM MMA. A0, B0, C0 stay on CTA0, while A1, B1, C1 stay on CTA1.
[Diagrammatic text]
- Two CTAs form a cooperative group.
- Each CTA holds its own half/partition:
  - CTA0: A0, B0, C0
  - CTA1: A1, B1, C1
- tcgen05.mma and commits can be done with .cta_group::2 and multicast where needed.

Commit example diff shown:
- Use tcgen05.commit.cta_group::2 ... multicast with cta_mask = 0b11 (two CTAs).

PTX doc clarification quoted in the post:
- "The mbarrier signal is multicast to the same offset as mbar in the shared memory of each destination CTA."

Benchmark:
- v5 (2-SM MMA): 1302.29 TFLOPS

Cluster barrier note:
- Cluster-scope equivalent of __syncthreads():
  - barrier.cluster.arrive.release.aligned
  - barrier.cluster.wait.acquire.aligned
Used after mbarrier and tmem init, and before tmem dealloc.
(The author notes a race condition risk if omitted.)

======================================================================
7) Persistent kernel (with static scheduling)
======================================================================
Motivation:
- Epilogue and initial setup of new threadblocks take significant time, leaving tensor cores idle.
Persistent approach:
- Launch exactly 148 threadblocks (number of SMs on Modal B200); each handles multiple output tiles sequentially.

Benefits:
1) One-time per-SM setup for whole kernel duration (instead of per output tile).
2) Overlap epilogue with TMA/MMA:
   - epilogue warps do tmem->regs, dtype cast, regs->gmem
   - TMA/MMA warps start next output tile.

Synchronization model extended:
- Two producer-consumer pairs coordinated via mbarrier:
  A) TMA <-> MMA (shared memory buffers)
  B) Mainloop <-> Epilogue (tensor memory buffers)

High-level structure (as on page; condensed):

- warp 0: TMA warp, iterates output tiles, issues TMA along K, signals TMA done.
- warp 1 (CTA0 only): MMA warp, waits for epilogue (tmem buffer free), runs mainloop along K, signals MMA done, signals mainloop done.
- warps 2..5: epilogue warps, wait for mainloop done, run epilogue, signal epilogue done.

Total warps: 6 (1 TMA, 1 MMA, 4 epilogue), because need >=4 epilogue warps to access all tmem.

IMAGE-AS-TEXT
[Image title/alt] Profile v6
[Caption] Intra-kernel profile of kernel v6. Source: profile_v5.json.gz (linked from the post).
[Diagrammatic text]
- Timeline shows overlap between epilogue and the next tile’s TMA/MMA.
- Overlap not perfect in trace (profiler overhead suspected), but idle time is reduced.

Result:
- v6 (persistent w/ static scheduling): 1475.93 TFLOPS
- This is ~98% of CuBLAS on the chosen shape.

======================================================================
8) Closing remarks — Iteration summary
======================================================================
Kernel name | TFLOPS
------------|-------
CuBLAS (PyTorch 2.9.1 + CUDA 13) | 1506.74
v1a (basic tcgen05 + 2D 16B TMA) | 254.62
v1b (3D 16B TMA) | 252.81
v2a (2D 128B TMA) | 681.20
v2b (3D 128B TMA) | 695.43
v3 (pipelining) | 939.61
v4 (warp specialization) | 1208.83
v5 (2-SM MMA) | 1302.29
v6 (persistent w/ static scheduling) | 1475.93

======================================================================
LINKS REFERENCED ON PAGE (important ones)
======================================================================
- tcgen05 in NVIDIA docs (PTX): https://docs.nvidia.com/
- PTX doc (general): https://docs.nvidia.com/
- learn-cuda code for this post:
  https://github.com/gau-nernst/learn-cuda/tree/3b90ac9b/02e_matmul_sm100/
- matmul_v1.cu, matmul_v3.cu, matmul_v4.cu, matmul_v6.cu: linked from the post within that repo.
- DeepGEMM (referenced for patterns): https://github.com/deepseek-ai/DeepGEMM

END OF FILE
