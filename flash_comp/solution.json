{
  "name": "dsa-topk-ref-v1",
  "definition": "dsa_topk_indexer_fp8_h64_d128_topk2048_ps64",
  "author": "team-name",
  "spec": {
    "language": "cuda",
    "target_hardware": [
      "cuda"
    ],
    "entry_point": "binding.py::kernel",
    "dependencies": [],
    "destination_passing_style": true,
    "binding": null
  },
  "sources": [
    {
      "path": "binding.py",
      "content": "import os\nimport torch\nfrom pathlib import Path\nfrom torch.utils.cpp_extension import load_inline\n\ntry:\n    from tvm.ffi import register_func\nexcept Exception:\n    def register_func(_name):\n        def deco(fn):\n            return fn\n        return deco\n\n\n_module = None\n_op = None\n\nos.environ.setdefault(\"TVM_FFI_CUDA_ARCH_LIST\", \"10.0a\")\n\n\ndef _read_cuda_source() -> str:\n    here = Path(__file__).resolve().parent\n    for candidate in (\"dsa_index.cu\", \"kernel.cu\"):\n        path = here / candidate\n        if path.exists():\n            return path.read_text()\n    raise FileNotFoundError(f\"No CUDA source found in {here} (expected dsa_index.cu or kernel.cu)\")\n\n\ndef _load_extension():\n    global _module, _op\n    if _op is not None:\n        return _op\n\n    cuda_src = _read_cuda_source()\n    _module = load_inline(\n        name=\"dsa_topk_indexer_ext\",\n        cpp_sources=\"\",\n        cuda_sources=cuda_src,\n        verbose=True,\n        is_python_module=False,\n        no_implicit_headers=True,\n        extra_cuda_cflags=[\n            \"-O3\",\n            \"--use_fast_math\",\n            \"-gencode=arch=compute_100a,code=sm_100a\",\n            \"--expt-relaxed-constexpr\",\n            \"--relocatable-device-code=false\",\n        ],\n        extra_ldflags=[\"-lcuda\"],\n    )\n    _op = torch.ops.index_bindings.dsa_topk_indexer_launch\n    return _op\n\n\ndef compile_kernel() -> None:\n    _load_extension()\n\n\ndef dsa_topk_indexer(\n    q_index_fp8: torch.Tensor,\n    k_index_cache_fp8: torch.Tensor,\n    weights: torch.Tensor,\n    seq_lens: torch.Tensor,\n    block_table: torch.Tensor,\n    topk_indices: torch.Tensor,\n) -> torch.Tensor:\n    op = _load_extension()\n    op(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        topk_indices,\n    )\n    return topk_indices\n\n\n@register_func(\"flashinfer.kernel\")\ndef kernel(\n    q_index_fp8,\n    k_index_cache_fp8,\n    weights,\n    seq_lens,\n    block_table,\n    topk_indices,\n):\n    dsa_topk_indexer(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        topk_indices,\n    )\n    return topk_indices\n"
    },
    {
      "path": "dsa_index.py",
      "content": "import os\nimport torch\nfrom pathlib import Path\nfrom torch.utils.cpp_extension import load_inline\n\ntry:\n    from tvm.ffi import register_func\nexcept Exception:\n    def register_func(_name):\n        def deco(fn):\n            return fn\n        return deco\n\n\n_module = None\n_op = None\n\nos.environ.setdefault(\"TVM_FFI_CUDA_ARCH_LIST\", \"10.0a\")\n\n\ndef _read_cuda_source() -> str:\n    here = Path(__file__).resolve().parent\n    for candidate in (\"dsa_index.cu\", \"kernel.cu\"):\n        path = here / candidate\n        if path.exists():\n            return path.read_text()\n    raise FileNotFoundError(f\"No CUDA source found in {here} (expected dsa_index.cu or kernel.cu)\")\n\n\ndef _load_extension():\n    global _module, _op\n    if _op is not None:\n        return _op\n\n    cuda_src = _read_cuda_source()\n    _module = load_inline(\n        name=\"dsa_topk_indexer_ext\",\n        cpp_sources=\"\",\n        cuda_sources=cuda_src,\n        verbose=True,\n        is_python_module=False,\n        no_implicit_headers=True,\n        extra_cuda_cflags=[\n            \"-O3\",\n            \"--use_fast_math\",\n            \"-gencode=arch=compute_100a,code=sm_100a\",\n            \"--expt-relaxed-constexpr\",\n            \"--relocatable-device-code=false\",\n        ],\n        extra_ldflags=[\"-lcuda\"],\n    )\n    _op = torch.ops.index_bindings.dsa_topk_indexer_launch\n    return _op\n\n\ndef compile_kernel() -> None:\n    _load_extension()\n\n\ndef dsa_topk_indexer(\n    q_index_fp8: torch.Tensor,\n    k_index_cache_fp8: torch.Tensor,\n    weights: torch.Tensor,\n    seq_lens: torch.Tensor,\n    block_table: torch.Tensor,\n    topk_indices: torch.Tensor,\n) -> torch.Tensor:\n    op = _load_extension()\n    op(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        topk_indices,\n    )\n    return topk_indices\n\n\n@register_func(\"flashinfer.kernel\")\ndef kernel(\n    q_index_fp8,\n    k_index_cache_fp8,\n    weights,\n    seq_lens,\n    block_table,\n    topk_indices,\n):\n    dsa_topk_indexer(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        topk_indices,\n    )\n    return topk_indices\n"
    },
    {
      "path": "kernel.cu",
      "content": "#include <tvm/ffi/container/tensor.h>\n#include <tvm/ffi/error.h>\n#include <tvm/ffi/extra/c_env_api.h>\n#include <tvm/ffi/function.h>\n#include <cuda.h>\n#include <cuda_fp8.h>\n#include <cuda_runtime.h>\n\n#include <cstddef>\n#include <climits>\n#include <cfloat>\n#include <cstdint>\n\n// ---------------------------------------------------------------------------------\n// Constants\n// ---------------------------------------------------------------------------------\nconstexpr int kNumHeads = 64;\nconstexpr int kHeadDim = 128;\nconstexpr int kPageSize = 64;\n\nconstexpr int kPayloadBytesPerToken = 128;\nconstexpr int kScaleBytesPerToken = 4;\nconstexpr int kRowBytes = 132;\nconstexpr int kPageBytes = kPageSize * kRowBytes;      // 8448\nconstexpr int kPackedFp8Bytes = kPageSize * kHeadDim;  // 8192\n\nconstexpr int kStageTokens = 64;\nconstexpr int kNumStages = 10;\n\nconstexpr int kProducerWarp = 0;\nconstexpr int kMmaWarp = 1;\nconstexpr int kEpilogueWarpBase = 2;\nconstexpr int kNumEpilogueWarps = 2;\nconstexpr int kNumWarps = 2 + kNumEpilogueWarps;\nconstexpr int kThreadsPerBlock = kNumWarps * 32;\n\nconstexpr int kMmaK = 32;\nconstexpr int kMmaIters = kHeadDim / kMmaK;  // 4\nconstexpr int kDesiredDynamicSmemBytes = 228 * 1024;\n\n// ---------------------------------------------------------------------------------\n// Device helpers\n// ---------------------------------------------------------------------------------\n__device__ inline uint32_t lane_id() {\n    uint32_t lane;\n    asm volatile(\"mov.u32 %0, %laneid;\" : \"=r\"(lane));\n    return lane;\n}\n\n__device__ inline uint32_t cvta_to_shared_u32(const void* p) {\n    return static_cast<uint32_t>(__cvta_generic_to_shared(p));\n}\n\n__device__ inline bool elect_lane0() {\n    return lane_id() == 0;\n}\n\n__device__ inline constexpr uint64_t desc_encode(uint64_t x) {\n    return (x & 0x3'FFFFULL) >> 4ULL;\n}\n\n__device__ inline uint64_t make_desc_kmajor_128b(int smem_addr) {\n    const int sbo = 8 * 128;\n    return desc_encode(static_cast<uint64_t>(smem_addr)) |\n           (desc_encode(static_cast<uint64_t>(sbo)) << 32ULL) |\n           (1ULL << 46ULL) |\n           (2ULL << 61ULL);\n}\n\ntemplate <typename T>\n__device__ inline T* smem_alloc(unsigned char*& ptr, int n) {\n    uintptr_t p = reinterpret_cast<uintptr_t>(ptr);\n    constexpr uintptr_t kAlign = alignof(T);\n    p = (p + kAlign - 1) & ~(kAlign - 1);\n    T* out = reinterpret_cast<T*>(p);\n    ptr = reinterpret_cast<unsigned char*>(out + n);\n    return out;\n}\n\n__device__ inline void mbarrier_init(int mbar_addr, int count) {\n    asm volatile(\"mbarrier.init.shared::cta.b64 [%0], %1;\" :: \"r\"(mbar_addr), \"r\"(count));\n}\n\n__device__ inline void mbarrier_arrive_expect_tx(int mbar_addr, int size_bytes) {\n    asm volatile(\"mbarrier.arrive.expect_tx.release.cta.shared::cta.b64 _, [%0], %1;\"\n                 :: \"r\"(mbar_addr), \"r\"(size_bytes)\n                 : \"memory\");\n}\n\n__device__ inline void mbarrier_arrive(int mbar_addr) {\n    asm volatile(\"mbarrier.arrive.release.cta.shared::cta.b64 _, [%0];\"\n                 :: \"r\"(mbar_addr)\n                 : \"memory\");\n}\n\n__device__ inline void mbarrier_wait_parity(int mbar_addr, int phase) {\n    const uint32_t ticks = 0x989680;\n    asm volatile(\n        \"{\\n\\t\"\n        \".reg .pred p;\\n\\t\"\n        \"L_WAIT_%=:\\n\\t\"\n        \"mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 p, [%0], %1, %2;\\n\\t\"\n        \"@p bra.uni L_DONE_%=;\\n\\t\"\n        \"bra.uni L_WAIT_%=;\\n\\t\"\n        \"L_DONE_%=:\\n\\t\"\n        \"}\"\n        :: \"r\"(mbar_addr), \"r\"(phase), \"r\"(ticks)\n        : \"memory\");\n}\n\n__device__ inline void tma_3d_gmem2smem(\n    int dst_smem_addr,\n    const void* tmap_ptr,\n    int x,\n    int y,\n    int z,\n    int mbar_addr,\n    uint64_t cache_policy\n) {\n    if (!elect_lane0()) {\n        return;\n    }\n    asm volatile(\n        \"cp.async.bulk.tensor.3d.shared::cta.global.mbarrier::complete_tx::bytes.cta_group::1.L2::cache_hint \"\n        \"[%0], [%1, {%2, %3, %4}], [%5], %6;\"\n        :: \"r\"(dst_smem_addr), \"l\"(tmap_ptr), \"r\"(x), \"r\"(y), \"r\"(z), \"r\"(mbar_addr), \"l\"(cache_policy)\n        : \"memory\");\n}\n\n__device__ inline void tcgen05_alloc(int smem_addr, int num_cols) {\n    asm volatile(\"tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%0], %1;\"\n                 :: \"r\"(smem_addr), \"r\"(num_cols));\n}\n\n__device__ inline void tcgen05_dealloc(int base_tmem, int num_cols) {\n    asm volatile(\"tcgen05.dealloc.cta_group::1.sync.aligned.b32 %0, %1;\"\n                 :: \"r\"(base_tmem), \"r\"(num_cols));\n}\n\n__device__ inline void tcgen05_mma_f8f6f4(\n    uint32_t tmem_d,\n    uint64_t desc_a,\n    uint64_t desc_b,\n    uint32_t idesc,\n    int accumulate\n) {\n    uint32_t mask[4] = {0, 0, 0, 0};\n    asm volatile(\n        \"{\\n\\t\"\n        \".reg .pred p;\\n\\t\"\n        \"setp.ne.b32 p, %4, 0;\\n\\t\"\n        \"tcgen05.mma.cta_group::1.kind::f8f6f4 [%0], %1, %2, %3, {%5, %6, %7, %8}, p;\\n\\t\"\n        \"}\"\n        :: \"r\"(tmem_d), \"l\"(desc_a), \"l\"(desc_b), \"r\"(idesc), \"r\"(accumulate),\n           \"r\"(mask[0]), \"r\"(mask[1]), \"r\"(mask[2]), \"r\"(mask[3]));\n}\n\n__device__ inline void tcgen05_commit(int mbar_addr) {\n    if (!elect_lane0()) {\n        return;\n    }\n    asm volatile(\"tcgen05.commit.cta_group::1.mbarrier::arrive::one.shared::cluster.b64 [%0];\"\n                 :: \"r\"(mbar_addr)\n                 : \"memory\");\n}\n\n__device__ inline void tcgen05_wait_ld() {\n    asm volatile(\"tcgen05.wait::ld.sync.aligned;\" ::: \"memory\");\n}\n\n__device__ inline void tcgen05_fence_after_thread_sync() {\n    asm volatile(\"tcgen05.fence::after_thread_sync;\" ::: \"memory\");\n}\n\n__device__ inline float tcgen05_ld_32x32b_1(int row, int col) {\n    float val;\n    int addr = (row << 16) | col;\n    asm volatile(\"tcgen05.ld.sync.aligned.32x32b.x1.b32 {%0}, [%1];\"\n                 : \"=f\"(val) : \"r\"(addr));\n    return val;\n}\n\n__device__ inline uint32_t read_u32_le(const uint8_t* p) {\n    return static_cast<uint32_t>(p[0]) |\n           (static_cast<uint32_t>(p[1]) << 8) |\n           (static_cast<uint32_t>(p[2]) << 16) |\n           (static_cast<uint32_t>(p[3]) << 24);\n}\n\n__device__ inline void insert_topk_desc(float score, int idx, float* scores, int* ids, int& count, int k) {\n    if (k <= 0 || idx < 0) {\n        return;\n    }\n\n    if (count < k) {\n        int pos = count;\n        while (pos > 0 && score > scores[pos - 1]) {\n            scores[pos] = scores[pos - 1];\n            ids[pos] = ids[pos - 1];\n            --pos;\n        }\n        scores[pos] = score;\n        ids[pos] = idx;\n        ++count;\n        return;\n    }\n\n    if (score <= scores[k - 1]) {\n        return;\n    }\n\n    int pos = k - 1;\n    while (pos > 0 && score > scores[pos - 1]) {\n        scores[pos] = scores[pos - 1];\n        ids[pos] = ids[pos - 1];\n        --pos;\n    }\n    scores[pos] = score;\n    ids[pos] = idx;\n}\n\n// ---------------------------------------------------------------------------------\n// Kernel: 3-stage warp-specialized pipeline\n// ---------------------------------------------------------------------------------\n__global__ void dsa_topk_indexer_kernel(\n    const uint8_t* q_index_bytes,   // [B,64,128], FP8 E4M3\n    const int8_t* k_index_cache,    // [num_pages,64,1,132], deep_gemm packed\n    const float* weights,           // [B,64]\n    const int* seq_lens,            // [B]\n    const int* block_table,         // [B,max_num_pages]\n    const void* k_fp8_tmap,         // payload tensor map\n    int* topk_indices,              // [B,topk]\n    int batch_size,\n    int num_pages,\n    int max_num_pages,\n    int topk\n) {\n    const int b = blockIdx.x;\n    const int tid = threadIdx.x;\n    const int warp_id = tid >> 5;\n    const int lane = tid & 31;\n\n    if (b >= batch_size || warp_id >= kNumWarps) {\n        return;\n    }\n\n    const int* block_table_b = block_table + static_cast<int64_t>(b) * max_num_pages;\n    const float* weights_b = weights + static_cast<int64_t>(b) * kNumHeads;\n    int* out_b = topk_indices + static_cast<int64_t>(b) * topk;\n\n    int seq_len = seq_lens[b];\n    if (seq_len < 0) seq_len = 0;\n    const int max_seq_by_pages = max_num_pages * kPageSize;\n    if (seq_len > max_seq_by_pages) seq_len = max_seq_by_pages;\n\n    for (int i = tid; i < topk; i += blockDim.x) {\n        out_b[i] = -1;\n    }\n    __syncthreads();\n\n    if (seq_len == 0 || topk <= 0) {\n        return;\n    }\n\n    const int actual_topk = (topk < seq_len) ? topk : seq_len;\n\n    extern __shared__ unsigned char smem_raw[];\n    unsigned char* smem_ptr = smem_raw;\n\n    // Per-stage K data.\n    uint8_t* k_stage_payload = smem_alloc<uint8_t>(smem_ptr, kNumStages * kStageTokens * kPayloadBytesPerToken);\n    float* k_stage_scale = smem_alloc<float>(smem_ptr, kNumStages * kStageTokens);\n\n    // Q + weights.\n    uint8_t* q_stage = smem_alloc<uint8_t>(smem_ptr, kNumHeads * kHeadDim);\n    float* w_stage = smem_alloc<float>(smem_ptr, kNumHeads);\n\n    // Stage metadata.\n    int* stage_page_idx = smem_alloc<int>(smem_ptr, kNumStages);\n    int* stage_valid_tokens = smem_alloc<int>(smem_ptr, kNumStages);\n\n    // Per-stage phase counters.\n    int* tma_phase = smem_alloc<int>(smem_ptr, kNumStages);\n    int* mma_phase = smem_alloc<int>(smem_ptr, kNumStages);\n    int* epi_phase = smem_alloc<int>(smem_ptr, kNumStages);\n\n    // Three mbar arrays (tma_done, mma_done, epi_done).\n    uint64_t* tma_mbar = smem_alloc<uint64_t>(smem_ptr, kNumStages);\n    uint64_t* mma_mbar = smem_alloc<uint64_t>(smem_ptr, kNumStages);\n    uint64_t* epi_mbar = smem_alloc<uint64_t>(smem_ptr, kNumStages);\n\n    // Per-stage epilogue candidates (requested per-stage top-k buffer).\n    float* stage_tile_scores = smem_alloc<float>(smem_ptr, kNumStages * kStageTokens);\n    int* stage_tile_ids = smem_alloc<int>(smem_ptr, kNumStages * kStageTokens);\n\n    // Global rolling top-k.\n    float* topk_scores = smem_alloc<float>(smem_ptr, topk);\n    int* topk_ids = smem_alloc<int>(smem_ptr, topk);\n    int* topk_count_ptr = smem_alloc<int>(smem_ptr, 1);\n\n    // TMEM scratch.\n    int* tmem_addr_scratch = smem_alloc<int>(smem_ptr, 1);\n\n    // Init barriers and phases.\n    if (warp_id == kProducerWarp && lane < kNumStages) {\n        mbarrier_init(cvta_to_shared_u32(&tma_mbar[lane]), 1);\n        mbarrier_init(cvta_to_shared_u32(&mma_mbar[lane]), 1);\n        mbarrier_init(cvta_to_shared_u32(&epi_mbar[lane]), 1);\n        tma_phase[lane] = 0;\n        mma_phase[lane] = 0;\n        epi_phase[lane] = 0;\n    }\n\n    // Stage Q + weights.\n    for (int idx = tid; idx < kNumHeads * kHeadDim; idx += blockDim.x) {\n        const int64_t q_off = static_cast<int64_t>(b) * kNumHeads * kHeadDim + idx;\n        q_stage[idx] = q_index_bytes[q_off];\n    }\n    for (int idx = tid; idx < kNumHeads; idx += blockDim.x) {\n        w_stage[idx] = weights_b[idx];\n    }\n\n    // Init global rolling top-k.\n    for (int i = tid; i < topk; i += blockDim.x) {\n        topk_scores[i] = -FLT_MAX;\n        topk_ids[i] = -1;\n    }\n    if (tid == 0) {\n        topk_count_ptr[0] = 0;\n        tmem_addr_scratch[0] = 0;\n    }\n    __syncthreads();\n\n    if (warp_id == kMmaWarp) {\n        tcgen05_alloc(cvta_to_shared_u32(tmem_addr_scratch), kStageTokens);\n    }\n    __syncthreads();\n\n    constexpr uint32_t kIdesc = (0U << 7U)    // atype = E4M3\n                              | (0U << 10U)   // btype = E4M3\n                              | (1U << 4U)    // dtype = F32\n                              | ((uint32_t)(kStageTokens >> 3U) << 17U)\n                              | ((uint32_t)(kNumHeads >> 4U) << 24U);\n\n    const int num_tiles = (seq_len + kStageTokens - 1) / kStageTokens;\n\n    // Pipeline schedule:\n    // iter: producer(tile=iter), mma(tile=iter-1), epilogue(tile=iter-2).\n    for (int iter = 0; iter < num_tiles + 2; ++iter) {\n        // ---------------- Producer warp ----------------\n        if (warp_id == kProducerWarp) {\n            const int tile_id = iter;\n            if (tile_id < num_tiles) {\n                const int stage = tile_id % kNumStages;\n\n                // Stage reuse requires prior epilogue completion on the same stage slot.\n                if (tile_id >= kNumStages && lane == 0) {\n                    mbarrier_wait_parity(cvta_to_shared_u32(&epi_mbar[stage]), epi_phase[stage]);\n                    epi_phase[stage] ^= 1;\n                }\n                __syncwarp();\n\n                const int tile_seq_start = tile_id * kStageTokens;\n                const int remain = seq_len - tile_seq_start;\n                int valid_tokens = (remain > 0) ? ((remain < kStageTokens) ? remain : kStageTokens) : 0;\n\n                int page_idx = -1;\n                if (tile_id >= 0 && tile_id < max_num_pages) {\n                    page_idx = block_table_b[tile_id];\n                }\n                if (!(page_idx >= 0 && page_idx < num_pages)) {\n                    valid_tokens = 0;\n                }\n\n                if (lane == 0) {\n                    stage_page_idx[stage] = page_idx;\n                    stage_valid_tokens[stage] = valid_tokens;\n                }\n                __syncwarp();\n\n                float* stage_scale = k_stage_scale + stage * kStageTokens;\n                for (int tok = lane; tok < valid_tokens; tok += 32) {\n                    const uint8_t* page_ptr = reinterpret_cast<const uint8_t*>(k_index_cache) +\n                                              static_cast<int64_t>(page_idx) * kPageBytes;\n                    const int scale_off = kPackedFp8Bytes + tok * kScaleBytesPerToken;\n                    stage_scale[tok] = __uint_as_float(read_u32_le(page_ptr + scale_off));\n                }\n                for (int tok = lane + valid_tokens; tok < kStageTokens; tok += 32) {\n                    stage_scale[tok] = 0.0f;\n                }\n                __syncwarp();\n\n                // Payload uses strict TMA path; invalid stages send explicit arrive signal.\n                if (lane == 0) {\n                    if (valid_tokens > 0) {\n                        const int dst = cvta_to_shared_u32(\n                            k_stage_payload + stage * kStageTokens * kPayloadBytesPerToken);\n                        const int mbar_addr = cvta_to_shared_u32(&tma_mbar[stage]);\n                        mbarrier_arrive_expect_tx(mbar_addr, kStageTokens * kPayloadBytesPerToken);\n                        tma_3d_gmem2smem(dst, k_fp8_tmap, 0, 0, page_idx, mbar_addr, 0ULL);\n                    } else {\n                        mbarrier_arrive(cvta_to_shared_u32(&tma_mbar[stage]));\n                    }\n                }\n            }\n        }\n\n        // ---------------- MMA warp ----------------\n        if (warp_id == kMmaWarp) {\n            const int tile_id = iter - 1;\n            if (tile_id >= 0 && tile_id < num_tiles) {\n                const int stage = tile_id % kNumStages;\n\n                if (lane == 0) {\n                    mbarrier_wait_parity(cvta_to_shared_u32(&tma_mbar[stage]), tma_phase[stage]);\n                    tma_phase[stage] ^= 1;\n                }\n                __syncwarp();\n\n                const int valid_tokens = stage_valid_tokens[stage];\n                if (valid_tokens > 0 && lane == 0) {\n                    const int q_addr = cvta_to_shared_u32(q_stage);\n                    const int k_addr = cvta_to_shared_u32(\n                        k_stage_payload + stage * kStageTokens * kPayloadBytesPerToken);\n\n                    uint64_t a_desc = make_desc_kmajor_128b(q_addr);\n                    uint64_t b_desc = make_desc_kmajor_128b(k_addr);\n\n                    for (int ki = 0; ki < kMmaIters; ++ki) {\n                        tcgen05_mma_f8f6f4(0, a_desc, b_desc, kIdesc, (ki > 0));\n                        a_desc += (kMmaK >> 4);\n                        b_desc += (kMmaK >> 4);\n                    }\n\n                    tcgen05_commit(cvta_to_shared_u32(&mma_mbar[stage]));\n                } else if (lane == 0) {\n                    mbarrier_arrive(cvta_to_shared_u32(&mma_mbar[stage]));\n                }\n            }\n        }\n\n        // ---------------- Epilogue warps ----------------\n        if (warp_id >= kEpilogueWarpBase && warp_id < kEpilogueWarpBase + kNumEpilogueWarps) {\n            const int epi_warp = warp_id - kEpilogueWarpBase;\n            const int tile_id = iter - 2;\n\n            if (tile_id >= 0 && tile_id < num_tiles) {\n                const int stage = tile_id % kNumStages;\n\n                if (epi_warp == 0 && lane == 0) {\n                    mbarrier_wait_parity(cvta_to_shared_u32(&mma_mbar[stage]), mma_phase[stage]);\n                    mma_phase[stage] ^= 1;\n                }\n\n                // Epilogue warps synchronize before/after TMEM reads and candidate writes.\n                asm volatile(\"bar.sync 1, 64;\" ::: \"memory\");\n\n                tcgen05_fence_after_thread_sync();\n\n                const int valid_tokens = stage_valid_tokens[stage];\n                const int page_idx = stage_page_idx[stage];\n                const float* stage_scale = k_stage_scale + stage * kStageTokens;\n                float* cand_scores = stage_tile_scores + stage * kStageTokens;\n                int* cand_ids = stage_tile_ids + stage * kStageTokens;\n\n                for (int t = epi_warp; t < valid_tokens; t += kNumEpilogueWarps) {\n                    float d0 = tcgen05_ld_32x32b_1(0, t);\n                    float d1 = tcgen05_ld_32x32b_1(1, t);\n                    tcgen05_wait_ld();\n\n                    const float scale = stage_scale[t];\n                    const float v0 = d0 * scale;\n                    const float v1 = d1 * scale;\n\n                    const float w0 = w_stage[lane];\n                    const float w1 = w_stage[lane + 32];\n\n                    float my_sum = (v0 > 0.0f ? (v0 * w0) : 0.0f) +\n                                   (v1 > 0.0f ? (v1 * w1) : 0.0f);\n\n                    for (int off = 16; off > 0; off >>= 1) {\n                        my_sum += __shfl_down_sync(0xFFFFFFFF, my_sum, off);\n                    }\n\n                    if (lane == 0) {\n                        cand_scores[t] = my_sum;\n                        cand_ids[t] = page_idx * kPageSize + t;\n                    }\n                }\n\n                asm volatile(\"bar.sync 1, 64;\" ::: \"memory\");\n\n                if (epi_warp == 0 && lane == 0) {\n                    int topk_count = topk_count_ptr[0];\n                    for (int t = 0; t < valid_tokens; ++t) {\n                        insert_topk_desc(cand_scores[t], cand_ids[t], topk_scores, topk_ids, topk_count, actual_topk);\n                    }\n                    topk_count_ptr[0] = topk_count;\n                    mbarrier_arrive(cvta_to_shared_u32(&epi_mbar[stage]));\n                }\n            }\n        }\n    }\n\n    __syncthreads();\n\n    if (warp_id == kEpilogueWarpBase) {\n        const int topk_count = topk_count_ptr[0];\n        for (int i = lane; i < actual_topk; i += 32) {\n            out_b[i] = (i < topk_count) ? topk_ids[i] : -1;\n        }\n    }\n\n    __syncthreads();\n    if (warp_id == kMmaWarp) {\n        tcgen05_dealloc(0, kStageTokens);\n    }\n}\n\n// ---------------------------------------------------------------------------------\n// Host-side tensor-map encode helpers\n// ---------------------------------------------------------------------------------\nstatic void* g_k_fp8_tmap_dev = nullptr;\n\nstatic inline size_t align_up_size(size_t v, size_t a) {\n    return (v + a - 1) & ~(a - 1);\n}\n\nstatic size_t required_dynamic_smem_bytes(int topk) {\n    size_t off = 0;\n\n    off = align_up_size(off, alignof(uint8_t));\n    off += static_cast<size_t>(kNumStages) * kStageTokens * kPayloadBytesPerToken;\n\n    off = align_up_size(off, alignof(float));\n    off += static_cast<size_t>(kNumStages) * kStageTokens * sizeof(float);\n\n    off = align_up_size(off, alignof(uint8_t));\n    off += static_cast<size_t>(kNumHeads) * kHeadDim;\n\n    off = align_up_size(off, alignof(float));\n    off += static_cast<size_t>(kNumHeads) * sizeof(float);\n\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * sizeof(int);  // stage_page_idx\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * sizeof(int);  // stage_valid_tokens\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * sizeof(int);  // tma_phase\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * sizeof(int);  // mma_phase\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * sizeof(int);  // epi_phase\n\n    off = align_up_size(off, alignof(uint64_t));\n    off += static_cast<size_t>(kNumStages) * sizeof(uint64_t);  // tma_mbar\n    off = align_up_size(off, alignof(uint64_t));\n    off += static_cast<size_t>(kNumStages) * sizeof(uint64_t);  // mma_mbar\n    off = align_up_size(off, alignof(uint64_t));\n    off += static_cast<size_t>(kNumStages) * sizeof(uint64_t);  // epi_mbar\n\n    off = align_up_size(off, alignof(float));\n    off += static_cast<size_t>(kNumStages) * kStageTokens * sizeof(float);  // stage_tile_scores\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * kStageTokens * sizeof(int);    // stage_tile_ids\n\n    off = align_up_size(off, alignof(float));\n    off += static_cast<size_t>(topk) * sizeof(float);  // topk_scores\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(topk) * sizeof(int);    // topk_ids\n    off = align_up_size(off, alignof(int));\n    off += sizeof(int);                                 // topk_count_ptr\n    off = align_up_size(off, alignof(int));\n    off += sizeof(int);                                 // tmem_addr_scratch\n\n    return off;\n}\n\nstatic inline bool is_dtype(DLDataType dtype, uint8_t code, uint8_t bits, uint16_t lanes = 1) {\n    return dtype.code == code && dtype.bits == bits && dtype.lanes == lanes;\n}\n\nstatic inline int dtype_nbytes(DLDataType dtype) {\n    return static_cast<int>((static_cast<int>(dtype.bits) * static_cast<int>(dtype.lanes) + 7) / 8);\n}\n\nstatic void ensure_k_fp8_tmap_on_device(const int8_t* k_index_cache_ptr, int num_pages) {\n    if (g_k_fp8_tmap_dev == nullptr) {\n        cudaError_t alloc_st = cudaMalloc(&g_k_fp8_tmap_dev, sizeof(CUtensorMap));\n        if (alloc_st != cudaSuccess) {\n            TVM_FFI_THROW(RuntimeError)\n                << \"cudaMalloc(CUtensorMap) failed: \" << cudaGetErrorString(alloc_st);\n        }\n    }\n\n    CUtensorMap k_fp8_tmap{};\n    CUresult init_st = cuInit(0);\n    if (init_st != CUDA_SUCCESS) {\n        TVM_FFI_THROW(RuntimeError) << \"cuInit failed with error code \" << static_cast<int>(init_st);\n    }\n\n    constexpr int rank = 3;\n    const uint64_t globalDim[rank] = {\n        static_cast<uint64_t>(kPayloadBytesPerToken),\n        static_cast<uint64_t>(kPageSize),\n        static_cast<uint64_t>(num_pages),\n    };\n    const uint64_t globalStrides[rank - 1] = {\n        static_cast<uint64_t>(kPayloadBytesPerToken),\n        static_cast<uint64_t>(kPageBytes),\n    };\n    const unsigned int boxDim[rank] = {\n        static_cast<unsigned int>(kPayloadBytesPerToken),\n        static_cast<unsigned int>(kPageSize),\n        1U,\n    };\n    const unsigned int elementStrides[rank] = {1U, 1U, 1U};\n\n    const CUresult st = cuTensorMapEncodeTiled(\n        &k_fp8_tmap,\n        CU_TENSOR_MAP_DATA_TYPE_UINT8,\n        static_cast<cuuint32_t>(rank),\n        const_cast<int8_t*>(k_index_cache_ptr),\n        reinterpret_cast<const cuuint64_t*>(globalDim),\n        reinterpret_cast<const cuuint64_t*>(globalStrides),\n        reinterpret_cast<const cuuint32_t*>(boxDim),\n        reinterpret_cast<const cuuint32_t*>(elementStrides),\n        CU_TENSOR_MAP_INTERLEAVE_NONE,\n        CU_TENSOR_MAP_SWIZZLE_NONE,\n        CU_TENSOR_MAP_L2_PROMOTION_L2_128B,\n        CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n    );\n    if (st != CUDA_SUCCESS) {\n        TVM_FFI_THROW(RuntimeError)\n            << \"cuTensorMapEncodeTiled failed with error code \" << static_cast<int>(st);\n    }\n\n    cudaError_t cp_st = cudaMemcpy(g_k_fp8_tmap_dev, &k_fp8_tmap, sizeof(CUtensorMap), cudaMemcpyHostToDevice);\n    if (cp_st != cudaSuccess) {\n        TVM_FFI_THROW(RuntimeError)\n            << \"cudaMemcpy(CUtensorMap) failed: \" << cudaGetErrorString(cp_st);\n    }\n}\n\n// ---------------------------------------------------------------------------------\n// Launch entry\n// ---------------------------------------------------------------------------------\nvoid dsa_topk_indexer_launch(\n    tvm::ffi::TensorView q_index_fp8,\n    tvm::ffi::TensorView k_index_cache_fp8,\n    tvm::ffi::TensorView weights,\n    tvm::ffi::TensorView seq_lens,\n    tvm::ffi::TensorView block_table,\n    tvm::ffi::TensorView topk_indices\n) {\n    if (q_index_fp8.device().device_type != kDLCUDA) {\n        TVM_FFI_THROW(ValueError) << \"q_index_fp8 must be CUDA\";\n    }\n    if (k_index_cache_fp8.device().device_type != kDLCUDA) {\n        TVM_FFI_THROW(ValueError) << \"k_index_cache_fp8 must be CUDA\";\n    }\n    if (weights.device().device_type != kDLCUDA) {\n        TVM_FFI_THROW(ValueError) << \"weights must be CUDA\";\n    }\n    if (seq_lens.device().device_type != kDLCUDA) {\n        TVM_FFI_THROW(ValueError) << \"seq_lens must be CUDA\";\n    }\n    if (block_table.device().device_type != kDLCUDA) {\n        TVM_FFI_THROW(ValueError) << \"block_table must be CUDA\";\n    }\n    if (topk_indices.device().device_type != kDLCUDA) {\n        TVM_FFI_THROW(ValueError) << \"topk_indices must be CUDA\";\n    }\n\n    TVM_FFI_ICHECK(q_index_fp8.IsContiguous()) << \"q_index_fp8 must be contiguous\";\n    TVM_FFI_ICHECK(k_index_cache_fp8.IsContiguous()) << \"k_index_cache_fp8 must be contiguous\";\n    TVM_FFI_ICHECK(weights.IsContiguous()) << \"weights must be contiguous\";\n    TVM_FFI_ICHECK(seq_lens.IsContiguous()) << \"seq_lens must be contiguous\";\n    TVM_FFI_ICHECK(block_table.IsContiguous()) << \"block_table must be contiguous\";\n    TVM_FFI_ICHECK(topk_indices.IsContiguous()) << \"topk_indices must be contiguous\";\n\n    const DLDataType q_dtype = q_index_fp8.dtype();\n    const DLDataType k_dtype = k_index_cache_fp8.dtype();\n    const DLDataType w_dtype = weights.dtype();\n    const DLDataType s_dtype = seq_lens.dtype();\n    const DLDataType bt_dtype = block_table.dtype();\n    const DLDataType out_dtype = topk_indices.dtype();\n\n    const int q_nbytes = dtype_nbytes(q_dtype);\n    if (q_nbytes != 1 && q_nbytes != 2) {\n        TVM_FFI_THROW(TypeError)\n            << \"q_index_fp8 must have 1-byte (fp8-bytes) or 2-byte (fp16) elements\";\n    }\n    if (!is_dtype(k_dtype, kDLInt, 8)) {\n        TVM_FFI_THROW(TypeError) << \"k_index_cache_fp8 must be int8\";\n    }\n    if (!is_dtype(w_dtype, kDLFloat, 32)) {\n        TVM_FFI_THROW(TypeError) << \"weights must be float32\";\n    }\n    if (!is_dtype(s_dtype, kDLInt, 32)) {\n        TVM_FFI_THROW(TypeError) << \"seq_lens must be int32\";\n    }\n    if (!is_dtype(bt_dtype, kDLInt, 32)) {\n        TVM_FFI_THROW(TypeError) << \"block_table must be int32\";\n    }\n    if (!is_dtype(out_dtype, kDLInt, 32)) {\n        TVM_FFI_THROW(TypeError) << \"topk_indices must be int32\";\n    }\n\n    TVM_FFI_ICHECK_EQ(q_index_fp8.ndim(), 3) << \"q_index_fp8 must be [B,64,128]\";\n    TVM_FFI_ICHECK_EQ(k_index_cache_fp8.ndim(), 4) << \"k_index_cache_fp8 must be [num_pages,64,1,132]\";\n    TVM_FFI_ICHECK_EQ(weights.ndim(), 2) << \"weights must be [B,64]\";\n    TVM_FFI_ICHECK_EQ(seq_lens.ndim(), 1) << \"seq_lens must be [B]\";\n    TVM_FFI_ICHECK_EQ(block_table.ndim(), 2) << \"block_table must be [B,max_num_pages]\";\n    TVM_FFI_ICHECK_EQ(topk_indices.ndim(), 2) << \"topk_indices must be [B,topk]\";\n\n    TVM_FFI_ICHECK(q_index_fp8.size(1) == kNumHeads && q_index_fp8.size(2) == kHeadDim)\n        << \"q_index_fp8 shape mismatch; expected [B,64,128]\";\n    TVM_FFI_ICHECK(k_index_cache_fp8.size(1) == kNumHeads &&\n                   k_index_cache_fp8.size(2) == 1 &&\n                   k_index_cache_fp8.size(3) == kRowBytes)\n        << \"k_index_cache_fp8 shape mismatch; expected [num_pages,64,1,132]\";\n    TVM_FFI_ICHECK_EQ(weights.size(1), kNumHeads) << \"weights shape mismatch; expected [B,64]\";\n\n    const int batch_size = static_cast<int>(q_index_fp8.size(0));\n    const int num_pages = static_cast<int>(k_index_cache_fp8.size(0));\n    const int max_num_pages = static_cast<int>(block_table.size(1));\n    const int topk = static_cast<int>(topk_indices.size(1));\n\n    TVM_FFI_ICHECK_GT(num_pages, 0) << \"num_pages must be > 0\";\n    TVM_FFI_ICHECK_GT(max_num_pages, 0) << \"max_num_pages must be > 0\";\n\n    TVM_FFI_ICHECK_EQ(weights.size(0), batch_size) << \"weights batch mismatch\";\n    TVM_FFI_ICHECK_EQ(seq_lens.size(0), batch_size) << \"seq_lens batch mismatch\";\n    TVM_FFI_ICHECK_EQ(block_table.size(0), batch_size) << \"block_table batch mismatch\";\n    TVM_FFI_ICHECK_EQ(topk_indices.size(0), batch_size) << \"topk_indices batch mismatch\";\n\n    if (batch_size == 0 || topk == 0) {\n        return;\n    }\n\n    ensure_k_fp8_tmap_on_device(reinterpret_cast<const int8_t*>(k_index_cache_fp8.data_ptr()), num_pages);\n\n    int device = -1;\n    cudaError_t device_st = cudaGetDevice(&device);\n    if (device_st != cudaSuccess) {\n        TVM_FFI_THROW(RuntimeError) << \"cudaGetDevice failed: \" << cudaGetErrorString(device_st);\n    }\n\n    int max_optin_smem = 0;\n    cudaError_t attr_st = cudaDeviceGetAttribute(&max_optin_smem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    if (attr_st != cudaSuccess) {\n        TVM_FFI_THROW(RuntimeError)\n            << \"cudaDeviceGetAttribute failed: \" << cudaGetErrorString(attr_st);\n    }\n    const size_t smem_need = required_dynamic_smem_bytes(topk);\n    TVM_FFI_ICHECK(smem_need <= static_cast<size_t>(INT_MAX)) << \"required smem does not fit int\";\n\n    int dynamic_smem_bytes = kDesiredDynamicSmemBytes;\n    const int smem_need_i = static_cast<int>(smem_need);\n    if (dynamic_smem_bytes < smem_need_i) dynamic_smem_bytes = smem_need_i;\n    if (max_optin_smem > 0 && dynamic_smem_bytes > max_optin_smem) dynamic_smem_bytes = max_optin_smem;\n    TVM_FFI_ICHECK(dynamic_smem_bytes >= smem_need_i)\n        << \"insufficient dynamic shared memory: need \" << smem_need_i << \", have \" << dynamic_smem_bytes;\n\n    cudaError_t max_smem_st = cudaFuncSetAttribute(\n        dsa_topk_indexer_kernel,\n        cudaFuncAttributeMaxDynamicSharedMemorySize,\n        dynamic_smem_bytes\n    );\n    if (max_smem_st != cudaSuccess) {\n        TVM_FFI_THROW(RuntimeError)\n            << \"cudaFuncSetAttribute(MaxDynamicSharedMemorySize) failed: \"\n            << cudaGetErrorString(max_smem_st);\n    }\n    cudaError_t carveout_st = cudaFuncSetAttribute(\n        dsa_topk_indexer_kernel,\n        cudaFuncAttributePreferredSharedMemoryCarveout,\n        100\n    );\n    if (carveout_st != cudaSuccess) {\n        TVM_FFI_THROW(RuntimeError)\n            << \"cudaFuncSetAttribute(PreferredSharedMemoryCarveout) failed: \"\n            << cudaGetErrorString(carveout_st);\n    }\n\n    DLDevice dev = q_index_fp8.device();\n    cudaStream_t stream = static_cast<cudaStream_t>(TVMFFIEnvGetStream(dev.device_type, dev.device_id));\n\n    const int blocks = batch_size;\n    dsa_topk_indexer_kernel<<<blocks, kThreadsPerBlock, dynamic_smem_bytes, stream>>>(\n        reinterpret_cast<const uint8_t*>(q_index_fp8.data_ptr()),\n        reinterpret_cast<const int8_t*>(k_index_cache_fp8.data_ptr()),\n        reinterpret_cast<const float*>(weights.data_ptr()),\n        reinterpret_cast<const int*>(seq_lens.data_ptr()),\n        reinterpret_cast<const int*>(block_table.data_ptr()),\n        g_k_fp8_tmap_dev,\n        reinterpret_cast<int*>(topk_indices.data_ptr()),\n        batch_size,\n        num_pages,\n        max_num_pages,\n        topk\n    );\n    cudaError_t launch_st = cudaGetLastError();\n    if (launch_st != cudaSuccess) {\n        TVM_FFI_THROW(RuntimeError) << \"kernel launch failed: \" << cudaGetErrorString(launch_st);\n    }\n}\n\nvoid kernel(\n    tvm::ffi::TensorView q_index_fp8,\n    tvm::ffi::TensorView k_index_cache_fp8,\n    tvm::ffi::TensorView weights,\n    tvm::ffi::TensorView seq_lens,\n    tvm::ffi::TensorView block_table,\n    tvm::ffi::TensorView topk_indices\n) {\n    dsa_topk_indexer_launch(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        topk_indices\n    );\n}\n\nTVM_FFI_DLL_EXPORT_TYPED_FUNC(kernel, kernel);\n"
    }
  ],
  "description": ""
}