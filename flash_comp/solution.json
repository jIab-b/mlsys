{
  "name": "dsa-topk-ref-v1",
  "definition": "dsa_topk_indexer_fp8_h64_d128_topk2048_ps64",
  "author": "team-name",
  "spec": {
    "language": "python",
    "target_hardware": [
      "cuda"
    ],
    "entry_point": "binding.py::kernel",
    "dependencies": [],
    "destination_passing_style": true,
    "binding": null
  },
  "sources": [
    {
      "path": "binding.py",
      "content": "import torch\nfrom pathlib import Path\nfrom torch.utils.cpp_extension import load_inline\n\n\n_module = None\n\n\ndef _read_cuda_source() -> str:\n    here = Path(__file__).resolve().parent\n    for candidate in (\"dsa_index.cu\", \"kernel.cu\"):\n        path = here / candidate\n        if path.exists():\n            return path.read_text()\n    raise FileNotFoundError(f\"No CUDA source found in {here} (expected dsa_index.cu or kernel.cu)\")\n\n\n_CPP_DECL_SRC = \"\"\"\n#include <torch/extension.h>\nvoid dsa_topk_indexer_launch(\n    torch::Tensor q_index_fp8,\n    torch::Tensor k_index_cache_fp8,\n    torch::Tensor weights,\n    torch::Tensor seq_lens,\n    torch::Tensor block_table,\n    torch::Tensor topk_indices);\n\"\"\"\n\n\ndef _get_module():\n    global _module\n    if _module is None:\n        _module = load_inline(\n            name=\"dsa_topk_indexer_ext\",\n            cpp_sources=_CPP_DECL_SRC,\n            cuda_sources=_read_cuda_source(),\n            functions=[\"dsa_topk_indexer_launch\"],\n            verbose=True,\n            extra_cuda_cflags=[\n                \"-O0\",\n                \"-gencode=arch=compute_100a,code=sm_100a\",\n                \"--threads=4\",\n            ],\n            extra_ldflags=[\"-lcuda\"],\n        )\n    return _module\n\n\ndef compile_kernel() -> None:\n    _get_module()\n\n\ndef dsa_topk_indexer(\n    q_index_fp8: torch.Tensor,\n    k_index_cache_fp8: torch.Tensor,\n    weights: torch.Tensor,\n    seq_lens: torch.Tensor,\n    block_table: torch.Tensor,\n    topk_indices: torch.Tensor,\n) -> torch.Tensor:\n    mod = _get_module()\n    mod.dsa_topk_indexer_launch(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        topk_indices,\n    )\n    return topk_indices\n\n\ndef kernel(\n    q_index_fp8,\n    k_index_cache_fp8,\n    weights,\n    seq_lens,\n    block_table,\n    topk_indices,\n):\n    return dsa_topk_indexer(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        topk_indices,\n    )\n"
    },
    {
      "path": "dsa_index.py",
      "content": "import torch\nfrom pathlib import Path\nfrom torch.utils.cpp_extension import load_inline\n\n\n_module = None\n\n\ndef _read_cuda_source() -> str:\n    here = Path(__file__).resolve().parent\n    for candidate in (\"dsa_index.cu\", \"kernel.cu\"):\n        path = here / candidate\n        if path.exists():\n            return path.read_text()\n    raise FileNotFoundError(f\"No CUDA source found in {here} (expected dsa_index.cu or kernel.cu)\")\n\n\n_CPP_DECL_SRC = \"\"\"\n#include <torch/extension.h>\nvoid dsa_topk_indexer_launch(\n    torch::Tensor q_index_fp8,\n    torch::Tensor k_index_cache_fp8,\n    torch::Tensor weights,\n    torch::Tensor seq_lens,\n    torch::Tensor block_table,\n    torch::Tensor topk_indices);\n\"\"\"\n\n\ndef _get_module():\n    global _module\n    if _module is None:\n        _module = load_inline(\n            name=\"dsa_topk_indexer_ext\",\n            cpp_sources=_CPP_DECL_SRC,\n            cuda_sources=_read_cuda_source(),\n            functions=[\"dsa_topk_indexer_launch\"],\n            verbose=True,\n            extra_cuda_cflags=[\n                \"-O0\",\n                \"-gencode=arch=compute_100a,code=sm_100a\",\n                \"--threads=4\",\n            ],\n            extra_ldflags=[\"-lcuda\"],\n        )\n    return _module\n\n\ndef compile_kernel() -> None:\n    _get_module()\n\n\ndef dsa_topk_indexer(\n    q_index_fp8: torch.Tensor,\n    k_index_cache_fp8: torch.Tensor,\n    weights: torch.Tensor,\n    seq_lens: torch.Tensor,\n    block_table: torch.Tensor,\n    topk_indices: torch.Tensor,\n) -> torch.Tensor:\n    mod = _get_module()\n    mod.dsa_topk_indexer_launch(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        topk_indices,\n    )\n    return topk_indices\n\n\ndef kernel(\n    q_index_fp8,\n    k_index_cache_fp8,\n    weights,\n    seq_lens,\n    block_table,\n    topk_indices,\n):\n    return dsa_topk_indexer(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        topk_indices,\n    )\n"
    },
    {
      "path": "kernel.cu",
      "content": "#include <ATen/cuda/CUDAContext.h>\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_fp8.h>\n#include <cuda_runtime.h>\n\n#include <cstddef>\n#include <climits>\n#include <cfloat>\n#include <cstdint>\n\n// ---------------------------------------------------------------------------------\n// Constants\n// ---------------------------------------------------------------------------------\nconstexpr int kNumHeads = 64;\nconstexpr int kHeadDim = 128;\nconstexpr int kPageSize = 64;\n\nconstexpr int kPayloadBytesPerToken = 128;\nconstexpr int kScaleBytesPerToken = 4;\nconstexpr int kRowBytes = 132;\nconstexpr int kPageBytes = kPageSize * kRowBytes;      // 8448\nconstexpr int kPackedFp8Bytes = kPageSize * kHeadDim;  // 8192\n\nconstexpr int kStageTokens = 64;\nconstexpr int kNumStages = 10;\n\nconstexpr int kProducerWarp = 0;\nconstexpr int kMmaWarp = 1;\nconstexpr int kEpilogueWarpBase = 2;\nconstexpr int kNumEpilogueWarps = 2;\nconstexpr int kNumWarps = 2 + kNumEpilogueWarps;\nconstexpr int kThreadsPerBlock = kNumWarps * 32;\n\nconstexpr int kMmaK = 32;\nconstexpr int kMmaIters = kHeadDim / kMmaK;  // 4\nconstexpr int kDesiredDynamicSmemBytes = 228 * 1024;\n\n\n// ---------------------------------------------------------------------------------\n// Device helpers\n// ---------------------------------------------------------------------------------\n__device__ inline uint32_t lane_id() {\n    uint32_t lane;\n    asm volatile(\"mov.u32 %0, %laneid;\" : \"=r\"(lane));\n    return lane;\n}\n\n__device__ inline uint32_t cvta_to_shared_u32(const void* p) {\n    return static_cast<uint32_t>(__cvta_generic_to_shared(p));\n}\n\n__device__ inline bool elect_lane0() {\n    return lane_id() == 0;\n}\n\n__device__ inline constexpr uint64_t desc_encode(uint64_t x) {\n    return (x & 0x3'FFFFULL) >> 4ULL;\n}\n\n__device__ inline uint64_t make_desc_kmajor_128b(int smem_addr) {\n    const int sbo = 8 * 128;\n    return desc_encode(static_cast<uint64_t>(smem_addr)) |\n           (desc_encode(static_cast<uint64_t>(sbo)) << 32ULL) |\n           (1ULL << 46ULL) |\n           (2ULL << 61ULL);\n}\n\ntemplate <typename T>\n__device__ inline T* smem_alloc(unsigned char*& ptr, int n) {\n    uintptr_t p = reinterpret_cast<uintptr_t>(ptr);\n    constexpr uintptr_t kAlign = alignof(T);\n    p = (p + kAlign - 1) & ~(kAlign - 1);\n    T* out = reinterpret_cast<T*>(p);\n    ptr = reinterpret_cast<unsigned char*>(out + n);\n    return out;\n}\n\n__device__ inline void mbarrier_init(int mbar_addr, int count) {\n    asm volatile(\"mbarrier.init.shared::cta.b64 [%0], %1;\" :: \"r\"(mbar_addr), \"r\"(count));\n}\n\n__device__ inline void mbarrier_arrive_expect_tx(int mbar_addr, int size_bytes) {\n    asm volatile(\"mbarrier.arrive.expect_tx.release.cta.shared::cta.b64 _, [%0], %1;\"\n                 :: \"r\"(mbar_addr), \"r\"(size_bytes)\n                 : \"memory\");\n}\n\n__device__ inline void mbarrier_arrive(int mbar_addr) {\n    asm volatile(\"mbarrier.arrive.release.cta.shared::cta.b64 _, [%0];\"\n                 :: \"r\"(mbar_addr)\n                 : \"memory\");\n}\n\n__device__ inline void mbarrier_wait_parity(int mbar_addr, int phase) {\n    const uint32_t ticks = 0x989680;\n    asm volatile(\n        \"{\\n\\t\"\n        \".reg .pred p;\\n\\t\"\n        \"L_WAIT_%=:\\n\\t\"\n        \"mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 p, [%0], %1, %2;\\n\\t\"\n        \"@p bra.uni L_DONE_%=;\\n\\t\"\n        \"bra.uni L_WAIT_%=;\\n\\t\"\n        \"L_DONE_%=:\\n\\t\"\n        \"}\"\n        :: \"r\"(mbar_addr), \"r\"(phase), \"r\"(ticks)\n        : \"memory\");\n}\n\n__device__ inline void tma_3d_gmem2smem(\n    int dst_smem_addr,\n    const void* tmap_ptr,\n    int x,\n    int y,\n    int z,\n    int mbar_addr,\n    uint64_t cache_policy\n) {\n    asm volatile(\n        \"cp.async.bulk.tensor.3d.shared::cta.global.mbarrier::complete_tx::bytes.cta_group::1.L2::cache_hint \"\n        \"[%0], [%1, {%2, %3, %4}], [%5], %6;\"\n        :: \"r\"(dst_smem_addr), \"l\"(tmap_ptr), \"r\"(x), \"r\"(y), \"r\"(z), \"r\"(mbar_addr), \"l\"(cache_policy)\n        : \"memory\");\n}\n\n__device__ inline void tma_2d_gmem2smem(\n    int dst_smem_addr,\n    const void* tmap_ptr,\n    int x,\n    int y,\n    int mbar_addr,\n    uint64_t cache_policy\n) {\n    asm volatile(\n        \"cp.async.bulk.tensor.2d.shared::cta.global.mbarrier::complete_tx::bytes.cta_group::1.L2::cache_hint \"\n        \"[%0], [%1, {%2, %3}], [%4], %5;\"\n        :: \"r\"(dst_smem_addr), \"l\"(tmap_ptr), \"r\"(x), \"r\"(y), \"r\"(mbar_addr), \"l\"(cache_policy)\n        : \"memory\");\n}\n\n__device__ inline void tcgen05_alloc(int smem_addr, int num_cols) {\n    asm volatile(\"tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%0], %1;\"\n                 :: \"r\"(smem_addr), \"r\"(num_cols));\n}\n\n__device__ inline void tcgen05_dealloc(int base_tmem, int num_cols) {\n    asm volatile(\"tcgen05.dealloc.cta_group::1.sync.aligned.b32 %0, %1;\"\n                 :: \"r\"(base_tmem), \"r\"(num_cols));\n}\n\n__device__ inline void tcgen05_mma_f8f6f4(\n    uint32_t tmem_d,\n    uint64_t desc_a,\n    uint64_t desc_b,\n    uint32_t idesc,\n    int accumulate\n) {\n    uint32_t mask[4] = {0, 0, 0, 0};\n    asm volatile(\n        \"{\\n\\t\"\n        \".reg .pred p;\\n\\t\"\n        \"setp.ne.b32 p, %4, 0;\\n\\t\"\n        \"tcgen05.mma.cta_group::1.kind::f8f6f4 [%0], %1, %2, %3, {%5, %6, %7, %8}, p;\\n\\t\"\n        \"}\"\n        :: \"r\"(tmem_d), \"l\"(desc_a), \"l\"(desc_b), \"r\"(idesc), \"r\"(accumulate),\n           \"r\"(mask[0]), \"r\"(mask[1]), \"r\"(mask[2]), \"r\"(mask[3]));\n}\n\n__device__ inline void tcgen05_commit(int mbar_addr) {\n    if (!elect_lane0()) {\n        return;\n    }\n    asm volatile(\"tcgen05.commit.cta_group::1.mbarrier::arrive::one.shared::cluster.b64 [%0];\"\n                 :: \"r\"(mbar_addr)\n                 : \"memory\");\n}\n\n__device__ inline void tcgen05_wait_ld() {\n    asm volatile(\"tcgen05.wait::ld.sync.aligned;\" ::: \"memory\");\n}\n\n__device__ inline void tcgen05_fence_after_thread_sync() {\n    asm volatile(\"tcgen05.fence::after_thread_sync;\" ::: \"memory\");\n}\n\n__device__ inline float tcgen05_ld_32x32b_1(int row, int col) {\n    float val;\n    int addr = (row << 16) | col;\n    asm volatile(\"tcgen05.ld.sync.aligned.32x32b.x1.b32 {%0}, [%1];\"\n                 : \"=f\"(val) : \"r\"(addr));\n    return val;\n}\n\n__device__ inline uint32_t read_u32_le(const uint8_t* p) {\n    return static_cast<uint32_t>(p[0]) |\n           (static_cast<uint32_t>(p[1]) << 8) |\n           (static_cast<uint32_t>(p[2]) << 16) |\n           (static_cast<uint32_t>(p[3]) << 24);\n}\n\n__device__ inline void insert_topk_desc(float score, int idx, float* scores, int* ids, int& count, int k) {\n    if (k <= 0 || idx < 0) {\n        return;\n    }\n\n    if (count < k) {\n        int pos = count;\n        while (pos > 0 && score > scores[pos - 1]) {\n            scores[pos] = scores[pos - 1];\n            ids[pos] = ids[pos - 1];\n            --pos;\n        }\n        scores[pos] = score;\n        ids[pos] = idx;\n        ++count;\n        return;\n    }\n\n    if (score <= scores[k - 1]) {\n        return;\n    }\n\n    int pos = k - 1;\n    while (pos > 0 && score > scores[pos - 1]) {\n        scores[pos] = scores[pos - 1];\n        ids[pos] = ids[pos - 1];\n        --pos;\n    }\n    scores[pos] = score;\n    ids[pos] = idx;\n}\n\n// ---------------------------------------------------------------------------------\n// Kernel: 3-stage warp-specialized pipeline\n// ---------------------------------------------------------------------------------\n__global__ void dsa_topk_indexer_kernel(\n    const uint8_t* q_index_bytes,   // [B,64,128], FP8 E4M3\n    const int8_t* k_index_cache,    // [num_pages,64,1,132], deep_gemm packed\n    const float* weights,           // [B,64]\n    const int* seq_lens,            // [B]\n    const int* block_table,         // [B,max_num_pages]\n    const void* k_fp8_tmap,         // payload tensor map\n    const void* k_scale_tmap,       // scale tensor map\n    int* topk_indices,              // [B,topk]\n    int batch_size,\n    int num_pages,\n    int max_num_pages,\n    int topk\n) {\n    const int b = blockIdx.x;\n    const int tid = threadIdx.x;\n    const int warp_id = tid >> 5;\n    const int lane = tid & 31;\n\n    if (b >= batch_size || warp_id >= kNumWarps) {\n        return;\n    }\n\n    const int* block_table_b = block_table + static_cast<int64_t>(b) * max_num_pages;\n    const float* weights_b = weights + static_cast<int64_t>(b) * kNumHeads;\n    int* out_b = topk_indices + static_cast<int64_t>(b) * topk;\n\n    int seq_len = seq_lens[b];\n    if (seq_len < 0) seq_len = 0;\n    const int max_seq_by_pages = max_num_pages * kPageSize;\n    if (seq_len > max_seq_by_pages) seq_len = max_seq_by_pages;\n\n    for (int i = tid; i < topk; i += blockDim.x) {\n        out_b[i] = -1;\n    }\n    __syncthreads();\n\n    if (seq_len == 0 || topk <= 0) {\n        return;\n    }\n\n    const int actual_topk = (topk < seq_len) ? topk : seq_len;\n\n    extern __shared__ __align__(1024) unsigned char smem_raw[];\n    unsigned char* smem_ptr = smem_raw;\n\n    // Per-stage K data.\n    uint8_t* k_stage_payload = smem_alloc<uint8_t>(smem_ptr, kNumStages * kStageTokens * kPayloadBytesPerToken);\n    float* k_stage_scale = smem_alloc<float>(smem_ptr, kNumStages * kStageTokens);\n\n    // Q + weights.\n    uint8_t* q_stage = smem_alloc<uint8_t>(smem_ptr, kNumHeads * kHeadDim);\n    float* w_stage = smem_alloc<float>(smem_ptr, kNumHeads);\n\n    // Stage metadata.\n    int* stage_page_idx = smem_alloc<int>(smem_ptr, kNumStages);\n    int* stage_valid_tokens = smem_alloc<int>(smem_ptr, kNumStages);\n\n    // Per-stage phase counters.\n    int* tma_phase = smem_alloc<int>(smem_ptr, kNumStages);\n    int* mma_phase = smem_alloc<int>(smem_ptr, kNumStages);\n    int* epi_phase = smem_alloc<int>(smem_ptr, kNumStages);\n\n    // Three mbar arrays (tma_done, mma_done, epi_done).\n    uint64_t* tma_mbar = smem_alloc<uint64_t>(smem_ptr, kNumStages);\n    uint64_t* mma_mbar = smem_alloc<uint64_t>(smem_ptr, kNumStages);\n    uint64_t* epi_mbar = smem_alloc<uint64_t>(smem_ptr, kNumStages);\n\n    // Per-stage epilogue candidates (requested per-stage top-k buffer).\n    float* stage_tile_scores = smem_alloc<float>(smem_ptr, kNumStages * kStageTokens);\n    int* stage_tile_ids = smem_alloc<int>(smem_ptr, kNumStages * kStageTokens);\n\n    // Global rolling top-k.\n    float* topk_scores = smem_alloc<float>(smem_ptr, topk);\n    int* topk_ids = smem_alloc<int>(smem_ptr, topk);\n    int* topk_count_ptr = smem_alloc<int>(smem_ptr, 1);\n\n    // TMEM scratch.\n    int* tmem_addr_scratch = smem_alloc<int>(smem_ptr, 1);\n\n    // Init barriers and phases.\n    if (warp_id == kProducerWarp && lane < kNumStages) {\n        mbarrier_init(cvta_to_shared_u32(&tma_mbar[lane]), 1);\n        mbarrier_init(cvta_to_shared_u32(&mma_mbar[lane]), 1);\n        mbarrier_init(cvta_to_shared_u32(&epi_mbar[lane]), 1);\n        tma_phase[lane] = 0;\n        mma_phase[lane] = 0;\n        epi_phase[lane] = 0;\n    }\n    if (warp_id == kProducerWarp && lane == 0) {\n        asm volatile(\"fence.mbarrier_init.release.cluster;\");\n    }\n    __syncthreads();\n\n\n    // Stage Q + weights.\n    for (int idx = tid; idx < kNumHeads * kHeadDim; idx += blockDim.x) {\n        const int64_t q_off = static_cast<int64_t>(b) * kNumHeads * kHeadDim + idx;\n        q_stage[idx] = q_index_bytes[q_off];\n    }\n    for (int idx = tid; idx < kNumHeads; idx += blockDim.x) {\n        w_stage[idx] = weights_b[idx];\n    }\n\n    // Init global rolling top-k.\n    for (int i = tid; i < topk; i += blockDim.x) {\n        topk_scores[i] = -FLT_MAX;\n        topk_ids[i] = -1;\n    }\n    if (tid == 0) {\n        topk_count_ptr[0] = 0;\n        tmem_addr_scratch[0] = 0;\n    }\n    __syncthreads();\n\n    if (warp_id == kMmaWarp) {\n        tcgen05_alloc(cvta_to_shared_u32(tmem_addr_scratch), kStageTokens);\n    }\n    __syncthreads();\n\n    constexpr uint32_t kIdesc = (0U << 7U)    // atype = E4M3\n                              | (0U << 10U)   // btype = E4M3\n                              | (1U << 4U)    // dtype = F32\n                              | ((uint32_t)(kStageTokens >> 3U) << 17U)\n                              | ((uint32_t)(kNumHeads >> 4U) << 24U);\n\n    const int num_tiles = (seq_len + kStageTokens - 1) / kStageTokens;\n\n    // Pipeline schedule:\n    // iter: producer(tile=iter), mma(tile=iter-1), epilogue(tile=iter-2).\n    for (int iter = 0; iter < num_tiles + 2; ++iter) {\n        // ---------------- Producer warp ----------------\n        if (warp_id == kProducerWarp) {\n            const int tile_id = iter;\n            if (tile_id < num_tiles) {\n                const int stage = tile_id % kNumStages;\n\n                // Stage reuse requires prior epilogue completion on the same stage slot.\n                if (tile_id >= kNumStages && lane == 0) {\n                    mbarrier_wait_parity(cvta_to_shared_u32(&epi_mbar[stage]), epi_phase[stage]);\n                    epi_phase[stage] ^= 1;\n                }\n                __syncwarp();\n\n                const int tile_seq_start = tile_id * kStageTokens;\n                const int remain = seq_len - tile_seq_start;\n                int valid_tokens = (remain > 0) ? ((remain < kStageTokens) ? remain : kStageTokens) : 0;\n\n                int page_idx = -1;\n                if (tile_id >= 0 && tile_id < max_num_pages) {\n                    page_idx = block_table_b[tile_id];\n                }\n                if (!(page_idx >= 0 && page_idx < num_pages)) {\n                    valid_tokens = 0;\n                }\n\n                if (lane == 0) {\n                    stage_page_idx[stage] = page_idx;\n                    stage_valid_tokens[stage] = valid_tokens;\n                }\n                __syncwarp();\n\n                // Zero out scale padding for invalid tokens.\n                float* stage_scale = k_stage_scale + stage * kStageTokens;\n                for (int tok = lane + valid_tokens; tok < kStageTokens; tok += 32) {\n                    stage_scale[tok] = 0.0f;\n                }\n                __syncwarp();\n\n                // TMA both payload and scales to the same mbarrier.\n                if (lane == 0) {\n                    if (valid_tokens > 0) {\n                        const int payload_dst = cvta_to_shared_u32(\n                            k_stage_payload + stage * kStageTokens * kPayloadBytesPerToken);\n                        const int scale_dst = cvta_to_shared_u32(stage_scale);\n                        const int mbar_addr = cvta_to_shared_u32(&tma_mbar[stage]);\n                        constexpr int payload_bytes = kStageTokens * kPayloadBytesPerToken;\n                        constexpr int scale_bytes = kStageTokens * kScaleBytesPerToken;\n\n                        mbarrier_arrive_expect_tx(mbar_addr, payload_bytes + scale_bytes);\n                        tma_3d_gmem2smem(payload_dst, k_fp8_tmap, 0, 0, page_idx, mbar_addr, 0ULL);\n                        tma_2d_gmem2smem(scale_dst, k_scale_tmap, 0, page_idx, mbar_addr, 0ULL);\n                    } else {\n                        mbarrier_arrive(cvta_to_shared_u32(&tma_mbar[stage]));\n                    }\n                }\n            }\n        }\n\n        // ---------------- MMA warp ----------------\n        if (warp_id == kMmaWarp) {\n            const int tile_id = iter - 1;\n            if (tile_id >= 0 && tile_id < num_tiles) {\n                const int stage = tile_id % kNumStages;\n\n                if (lane == 0) {\n                    mbarrier_wait_parity(cvta_to_shared_u32(&tma_mbar[stage]), tma_phase[stage]);\n                    tma_phase[stage] ^= 1;\n                }\n                __syncwarp();\n\n                const int valid_tokens = stage_valid_tokens[stage];\n                if (valid_tokens > 0 && lane == 0) {\n                    const int q_addr = cvta_to_shared_u32(q_stage);\n                    const int k_addr = cvta_to_shared_u32(\n                        k_stage_payload + stage * kStageTokens * kPayloadBytesPerToken);\n\n                    uint64_t a_desc = make_desc_kmajor_128b(q_addr);\n                    uint64_t b_desc = make_desc_kmajor_128b(k_addr);\n\n                    for (int ki = 0; ki < kMmaIters; ++ki) {\n                        tcgen05_mma_f8f6f4(0, a_desc, b_desc, kIdesc, (ki > 0));\n                        a_desc += (kMmaK >> 4);\n                        b_desc += (kMmaK >> 4);\n                    }\n\n                    tcgen05_commit(cvta_to_shared_u32(&mma_mbar[stage]));\n                } else if (lane == 0) {\n                    mbarrier_arrive(cvta_to_shared_u32(&mma_mbar[stage]));\n                }\n            }\n        }\n\n        // ---------------- Epilogue warps ----------------\n        if (warp_id >= kEpilogueWarpBase && warp_id < kEpilogueWarpBase + kNumEpilogueWarps) {\n            const int epi_warp = warp_id - kEpilogueWarpBase;\n            const int tile_id = iter - 2;\n\n            if (tile_id >= 0 && tile_id < num_tiles) {\n                const int stage = tile_id % kNumStages;\n\n                if (epi_warp == 0 && lane == 0) {\n                    mbarrier_wait_parity(cvta_to_shared_u32(&mma_mbar[stage]), mma_phase[stage]);\n                    mma_phase[stage] ^= 1;\n                }\n\n                // Epilogue warps synchronize before/after TMEM reads and candidate writes.\n                asm volatile(\"bar.sync 1, 64;\" ::: \"memory\");\n\n                tcgen05_fence_after_thread_sync();\n\n                const int valid_tokens = stage_valid_tokens[stage];\n                const int page_idx = stage_page_idx[stage];\n                const float* stage_scale = k_stage_scale + stage * kStageTokens;\n                float* cand_scores = stage_tile_scores + stage * kStageTokens;\n                int* cand_ids = stage_tile_ids + stage * kStageTokens;\n\n                // epi_warp 0 reads TMEM rows 0-31 (heads 0..31)\n                // epi_warp 1 reads TMEM rows 32-63 (heads 32..63)\n                const int tmem_row_base = epi_warp * 32;\n\n                for (int t = 0; t < valid_tokens; ++t) {\n                    // Each warp loads its 32 heads for token t\n                    float d = tcgen05_ld_32x32b_1(tmem_row_base, t);\n                    tcgen05_wait_ld();\n\n                    const float scale = stage_scale[t];\n                    const float v = d * scale;\n\n                    // weight for this lane's head\n                    const float w = w_stage[tmem_row_base + lane];\n\n                    // ReLU + weighted\n                    float my_sum = (v > 0.0f) ? (v * w) : 0.0f;\n\n                    // Warp reduction: partial sum over 32 heads\n                    for (int off = 16; off > 0; off >>= 1) {\n                        my_sum += __shfl_down_sync(0xFFFFFFFF, my_sum, off);\n                    }\n\n                    // epi_warp 1 writes partial sum to smem for epi_warp 0 to read\n                    if (epi_warp == 1 && lane == 0) {\n                        cand_scores[t] = my_sum;\n                    }\n                    asm volatile(\"bar.sync 1, 64;\" ::: \"memory\");\n\n                    if (epi_warp == 0 && lane == 0) {\n                        float total = my_sum + cand_scores[t];\n                        cand_scores[t] = total;\n                        cand_ids[t] = page_idx * kPageSize + t;\n                    }\n\n                    asm volatile(\"bar.sync 1, 64;\" ::: \"memory\");\n                }\n\n                if (epi_warp == 0 && lane == 0) {\n                    int topk_count = topk_count_ptr[0];\n\n                    for (int t = 0; t < valid_tokens; ++t) {\n                        insert_topk_desc(cand_scores[t], cand_ids[t], topk_scores, topk_ids, topk_count, actual_topk);\n                    }\n                    topk_count_ptr[0] = topk_count;\n\n                    mbarrier_arrive(cvta_to_shared_u32(&epi_mbar[stage]));\n                }\n            }\n        }\n    }\n\n    __syncthreads();\n\n    if (warp_id == kEpilogueWarpBase) {\n        const int topk_count = topk_count_ptr[0];\n        for (int i = lane; i < actual_topk; i += 32) {\n            out_b[i] = (i < topk_count) ? topk_ids[i] : -1;\n        }\n    }\n\n    __syncthreads();\n    if (warp_id == kMmaWarp) {\n        tcgen05_dealloc(0, kStageTokens);\n    }\n\n}\n\n// ---------------------------------------------------------------------------------\n// Host-side tensor-map encode helpers\n// ---------------------------------------------------------------------------------\nstatic void* g_k_fp8_tmap_dev = nullptr;\nstatic void* g_k_scale_tmap_dev = nullptr;\n\nstatic inline size_t align_up_size(size_t v, size_t a) {\n    return (v + a - 1) & ~(a - 1);\n}\n\nstatic size_t required_dynamic_smem_bytes(int topk) {\n    size_t off = 0;\n\n    off = align_up_size(off, alignof(uint8_t));\n    off += static_cast<size_t>(kNumStages) * kStageTokens * kPayloadBytesPerToken;\n\n    off = align_up_size(off, alignof(float));\n    off += static_cast<size_t>(kNumStages) * kStageTokens * sizeof(float);\n\n    off = align_up_size(off, alignof(uint8_t));\n    off += static_cast<size_t>(kNumHeads) * kHeadDim;\n\n    off = align_up_size(off, alignof(float));\n    off += static_cast<size_t>(kNumHeads) * sizeof(float);\n\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * sizeof(int);  // stage_page_idx\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * sizeof(int);  // stage_valid_tokens\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * sizeof(int);  // tma_phase\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * sizeof(int);  // mma_phase\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * sizeof(int);  // epi_phase\n\n    off = align_up_size(off, alignof(uint64_t));\n    off += static_cast<size_t>(kNumStages) * sizeof(uint64_t);  // tma_mbar\n    off = align_up_size(off, alignof(uint64_t));\n    off += static_cast<size_t>(kNumStages) * sizeof(uint64_t);  // mma_mbar\n    off = align_up_size(off, alignof(uint64_t));\n    off += static_cast<size_t>(kNumStages) * sizeof(uint64_t);  // epi_mbar\n\n    off = align_up_size(off, alignof(float));\n    off += static_cast<size_t>(kNumStages) * kStageTokens * sizeof(float);  // stage_tile_scores\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(kNumStages) * kStageTokens * sizeof(int);    // stage_tile_ids\n\n    off = align_up_size(off, alignof(float));\n    off += static_cast<size_t>(topk) * sizeof(float);  // topk_scores\n    off = align_up_size(off, alignof(int));\n    off += static_cast<size_t>(topk) * sizeof(int);    // topk_ids\n    off = align_up_size(off, alignof(int));\n    off += sizeof(int);                                 // topk_count_ptr\n    off = align_up_size(off, alignof(int));\n    off += sizeof(int);                                 // tmem_addr_scratch\n\n    return off;\n}\n\nstatic void ensure_k_fp8_tmap_on_device(const int8_t* k_index_cache_ptr, int num_pages) {\n    if (g_k_fp8_tmap_dev == nullptr) {\n        cudaError_t alloc_st = cudaMalloc(&g_k_fp8_tmap_dev, sizeof(CUtensorMap));\n        TORCH_CHECK(alloc_st == cudaSuccess, \"cudaMalloc(CUtensorMap) failed\");\n    }\n\n    CUtensorMap k_fp8_tmap{};\n    cuInit(0);\n\n    constexpr cuuint32_t rank = 3;\n    const cuuint64_t globalDim[rank] = {\n        static_cast<cuuint64_t>(kPayloadBytesPerToken),\n        static_cast<cuuint64_t>(kPageSize),\n        static_cast<cuuint64_t>(num_pages),\n    };\n    const cuuint64_t globalStrides[rank - 1] = {\n        static_cast<cuuint64_t>(kPayloadBytesPerToken),\n        static_cast<cuuint64_t>(kPageBytes),\n    };\n    const cuuint32_t boxDim[rank] = {\n        static_cast<cuuint32_t>(kPayloadBytesPerToken),\n        static_cast<cuuint32_t>(kPageSize),\n        1U,\n    };\n    const cuuint32_t elementStrides[rank] = {1U, 1U, 1U};\n\n    const CUresult st = cuTensorMapEncodeTiled(\n        &k_fp8_tmap,\n        CU_TENSOR_MAP_DATA_TYPE_UINT8,\n        rank,\n        const_cast<int8_t*>(k_index_cache_ptr),\n        globalDim,\n        globalStrides,\n        boxDim,\n        elementStrides,\n        CU_TENSOR_MAP_INTERLEAVE_NONE,\n        CU_TENSOR_MAP_SWIZZLE_128B,\n        CU_TENSOR_MAP_L2_PROMOTION_L2_128B,\n        CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n    );\n    TORCH_CHECK(st == CUDA_SUCCESS, \"cuTensorMapEncodeTiled failed for payload\");\n\n    cudaError_t cp_st = cudaMemcpy(g_k_fp8_tmap_dev, &k_fp8_tmap, sizeof(CUtensorMap), cudaMemcpyHostToDevice);\n    TORCH_CHECK(cp_st == cudaSuccess, \"cudaMemcpy(payload CUtensorMap) failed\");\n\n    // Scale tensor map: 2D over the scale region within each page.\n    if (g_k_scale_tmap_dev == nullptr) {\n        cudaError_t alloc_st2 = cudaMalloc(&g_k_scale_tmap_dev, sizeof(CUtensorMap));\n        TORCH_CHECK(alloc_st2 == cudaSuccess, \"cudaMalloc(scale CUtensorMap) failed\");\n    }\n\n    CUtensorMap k_scale_tmap{};\n    {\n        constexpr cuuint32_t srank = 2;\n        const uint8_t* scale_base = reinterpret_cast<const uint8_t*>(k_index_cache_ptr) + kPackedFp8Bytes;\n        const cuuint64_t sglobalDim[srank] = {\n            static_cast<cuuint64_t>(kPageSize * kScaleBytesPerToken),  // 256\n            static_cast<cuuint64_t>(num_pages),\n        };\n        const cuuint64_t sglobalStrides[srank - 1] = {\n            static_cast<cuuint64_t>(kPageBytes),  // stride between pages = 8448\n        };\n        const cuuint32_t sboxDim[srank] = {\n            static_cast<cuuint32_t>(kPageSize * kScaleBytesPerToken),  // 256\n            1U,\n        };\n        const cuuint32_t selementStrides[srank] = {1U, 1U};\n\n        const CUresult st2 = cuTensorMapEncodeTiled(\n            &k_scale_tmap,\n            CU_TENSOR_MAP_DATA_TYPE_UINT8,\n            srank,\n            const_cast<uint8_t*>(scale_base),\n            sglobalDim,\n            sglobalStrides,\n            sboxDim,\n            selementStrides,\n            CU_TENSOR_MAP_INTERLEAVE_NONE,\n            CU_TENSOR_MAP_SWIZZLE_NONE,\n            CU_TENSOR_MAP_L2_PROMOTION_L2_128B,\n            CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n        );\n        TORCH_CHECK(st2 == CUDA_SUCCESS, \"cuTensorMapEncodeTiled failed for scales\");\n    }\n\n    cudaError_t cp_st2 = cudaMemcpy(g_k_scale_tmap_dev, &k_scale_tmap, sizeof(CUtensorMap), cudaMemcpyHostToDevice);\n    TORCH_CHECK(cp_st2 == cudaSuccess, \"cudaMemcpy(scale CUtensorMap) failed\");\n}\n\n// ---------------------------------------------------------------------------------\n// Launch entry\n// ---------------------------------------------------------------------------------\nvoid dsa_topk_indexer_launch(\n    torch::Tensor q_index_fp8,\n    torch::Tensor k_index_cache_fp8,\n    torch::Tensor weights,\n    torch::Tensor seq_lens,\n    torch::Tensor block_table,\n    torch::Tensor topk_indices\n) {\n    const int batch_size = static_cast<int>(q_index_fp8.size(0));\n    const int num_pages = static_cast<int>(k_index_cache_fp8.size(0));\n    const int max_num_pages = static_cast<int>(block_table.size(1));\n    const int topk = static_cast<int>(topk_indices.size(1));\n    if (batch_size == 0 || topk == 0) {\n        return;\n    }\n\n    ensure_k_fp8_tmap_on_device(reinterpret_cast<const int8_t*>(k_index_cache_fp8.data_ptr()), num_pages);\n\n    int device = -1;\n    cudaGetDevice(&device);\n\n    int max_optin_smem = 0;\n    cudaDeviceGetAttribute(&max_optin_smem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    const size_t smem_need = required_dynamic_smem_bytes(topk);\n    TORCH_CHECK(smem_need <= static_cast<size_t>(INT_MAX), \"required smem does not fit int\");\n    int dynamic_smem_bytes = kDesiredDynamicSmemBytes;\n    const int smem_need_i = static_cast<int>(smem_need);\n    if (dynamic_smem_bytes < smem_need_i) dynamic_smem_bytes = smem_need_i;\n    if (max_optin_smem > 0 && dynamic_smem_bytes > max_optin_smem) dynamic_smem_bytes = max_optin_smem;\n    TORCH_CHECK(dynamic_smem_bytes >= smem_need_i,\n                \"insufficient dynamic shared memory: need \", smem_need_i, \", have \", dynamic_smem_bytes);\n    cudaFuncSetAttribute(\n        dsa_topk_indexer_kernel,\n        cudaFuncAttributeMaxDynamicSharedMemorySize,\n        dynamic_smem_bytes\n    );\n    cudaFuncSetAttribute(\n        dsa_topk_indexer_kernel,\n        cudaFuncAttributePreferredSharedMemoryCarveout,\n        100\n    );\n\n    const int blocks = batch_size;\n    dsa_topk_indexer_kernel<<<blocks, kThreadsPerBlock, dynamic_smem_bytes>>>(\n        reinterpret_cast<const uint8_t*>(q_index_fp8.data_ptr()),\n        reinterpret_cast<const int8_t*>(k_index_cache_fp8.data_ptr()),\n        reinterpret_cast<const float*>(weights.data_ptr()),\n        reinterpret_cast<const int*>(seq_lens.data_ptr()),\n        reinterpret_cast<const int*>(block_table.data_ptr()),\n        g_k_fp8_tmap_dev,\n        g_k_scale_tmap_dev,\n        reinterpret_cast<int*>(topk_indices.data_ptr()),\n        batch_size,\n        num_pages,\n        max_num_pages,\n        topk\n    );\n    cudaError_t launch_st = cudaGetLastError();\n    TORCH_CHECK(launch_st == cudaSuccess, \"kernel launch failed: \", cudaGetErrorString(launch_st));\n\n    cudaError_t sync_st = cudaDeviceSynchronize();\n    if (sync_st != cudaSuccess) {\n        TORCH_CHECK(false, \"kernel sync failed: \", cudaGetErrorString(sync_st));\n    }\n}\n"
    }
  ],
  "description": ""
}