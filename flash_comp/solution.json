{
  "name": "my-team-solution-v1",
  "definition": "dsa_topk_indexer_fp8_h64_d128_topk2048_ps64",
  "author": "team-name",
  "spec": {
    "language": "cuda",
    "target_hardware": [
      "cuda"
    ],
    "entry_point": "binding.py::kernel",
    "dependencies": [],
    "destination_passing_style": true,
    "binding": null
  },
  "sources": [
    {
      "path": "2.py",
      "content": "#!POPCORN leaderboard modal_nvfp4_dual_gemm\n#!POPCORN gpu B200\n\nimport torch\nfrom task import input_t, output_t\nfrom torch.utils.cpp_extension import load_inline\n\nCUDA_SRC = r\"\"\"\n#include <cudaTypedefs.h>\n#include <cuda_fp16.h>\n\n#include <torch/library.h>\n#include <ATen/core/Tensor.h>\n\nconstexpr int WARP_SIZE = 32;\n\nconstexpr int MMA_K = 64;  // 32 bytes\n\n// https://github.com/NVIDIA/cutlass/blob/v4.3.2/include/cute/arch/copy_sm90_desc.hpp#L193-L197\nconstexpr uint64_t EVICT_NORMAL = 0x1000000000000000;\nconstexpr uint64_t EVICT_FIRST = 0x12F0000000000000;\nconstexpr uint64_t EVICT_LAST = 0x14F0000000000000;\n\n__device__ inline\nconstexpr uint64_t desc_encode(uint64_t x) { return (x & 0x3'FFFFULL) >> 4ULL; };\n\n// https://github.com/NVIDIA/cutlass/blob/v4.2.1/include/cute/arch/cluster_sm90.hpp#L180\n__device__\nuint32_t elect_sync() {\n  uint32_t pred = 0;\n  asm volatile(\n    \"{\\n\\t\"\n    \".reg .pred %%px;\\n\\t\"\n    \"elect.sync _|%%px, %1;\\n\\t\"\n    \"@%%px mov.s32 %0, 1;\\n\\t\"\n    \"}\"\n    : \"+r\"(pred)\n    : \"r\"(0xFFFFFFFF)\n  );\n  return pred;\n}\n\n__device__ inline\nvoid mbarrier_init(int mbar_addr, int count) {\n  asm volatile(\"mbarrier.init.shared::cta.b64 [%0], %1;\" :: \"r\"(mbar_addr), \"r\"(count));\n}\n\n// NOTE: using .shared::cluster\n__device__ inline\nvoid mbarrier_arrive_expect_tx(int mbar_addr, int size) {\n  asm volatile(\"mbarrier.arrive.expect_tx.release.cta.shared::cluster.b64 _, [%0], %1;\" :: \"r\"(mbar_addr), \"r\"(size) : \"memory\");\n}\n\n// https://github.com/NVIDIA/cutlass/blob/v4.2.1/include/cutlass/arch/barrier.h#L408\n__device__\nvoid mbarrier_wait(int mbar_addr, int phase) {\n  uint32_t ticks = 0x989680;  // this is optional\n  asm volatile(\n    \"{\\n\\t\"\n    \".reg .pred P1;\\n\\t\"\n    \"LAB_WAIT:\\n\\t\"\n    \"mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 P1, [%0], %1, %2;\\n\\t\"\n    \"@!P1 bra.uni LAB_WAIT;\\n\\t\"\n    \"}\"\n    :: \"r\"(mbar_addr), \"r\"(phase), \"r\"(ticks)\n  );\n}\n\n__device__ inline\nvoid tma_prefetch(const void *src, int size, uint64_t cache_policy) {\n  asm volatile(\"cp.async.bulk.prefetch.L2.global.L2::cache_hint [%0], %1, %2;\"\n              :: \"l\"(src), \"r\"(size), \"l\"(cache_policy) : \"memory\");\n}\n\n__device__ inline\nvoid tma_1d_prefetch(const void *tmap_ptr, int x, uint64_t cache_policy) {\n  asm volatile(\"cp.async.bulk.prefetch.tensor.1d.L2.global.L2::cache_hint [%0, {%1}], %2;\"\n              :: \"l\"(tmap_ptr), \"r\"(x), \"l\"(cache_policy) : \"memory\");\n}\n\n__device__ inline\nvoid tma_2d_prefetch(const void *tmap_ptr, int x, int y, uint64_t cache_policy) {\n  asm volatile(\"cp.async.bulk.prefetch.tensor.2d.L2.global.L2::cache_hint [%0, {%1, %2}], %3;\"\n              :: \"l\"(tmap_ptr), \"r\"(x), \"r\"(y), \"l\"(cache_policy) : \"memory\");\n}\n\n__device__ inline\nvoid tma_3d_prefetch(const void *tmap_ptr, int x, int y, int z, uint64_t cache_policy) {\n  asm volatile(\"cp.async.bulk.prefetch.tensor.3d.L2.global.L2::cache_hint [%0, {%1, %2, %3}], %4;\"\n              :: \"l\"(tmap_ptr), \"r\"(x), \"r\"(y), \"r\"(z), \"l\"(cache_policy) : \"memory\");\n}\n\n__device__ inline\nvoid tma_gmem2smem(int dst, const void *src, int size, int mbar_addr, uint64_t cache_policy) {\n  asm volatile(\"cp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes.L2::cache_hint [%0], [%1], %2, [%3], %4;\"\n              :: \"r\"(dst), \"l\"(src), \"r\"(size), \"r\"(mbar_addr), \"l\"(cache_policy));\n}\n\ntemplate <int CTA_GROUP = 1>\n__device__ inline\nvoid tma_1d_gmem2smem(int dst, const void *tmap_ptr, int x, int mbar_addr, uint64_t cache_policy) {\n  asm volatile(\"cp.async.bulk.tensor.1d.shared::cta.global.mbarrier::complete_tx::bytes.cta_group::%5.L2::cache_hint \"\n              \"[%0], [%1, {%2}], [%3], %4;\"\n              :: \"r\"(dst), \"l\"(tmap_ptr), \"r\"(x), \"r\"(mbar_addr), \"l\"(cache_policy), \"n\"(CTA_GROUP)\n              : \"memory\");\n}\n\ntemplate <int CTA_GROUP = 1>\n__device__ inline\nvoid tma_1d_gmem2smem_mcast(int dst, const void *tmap_ptr, int x, int mbar_addr, int16_t cta_mask, uint64_t cache_policy) {\n  asm volatile(\"cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.cta_group::%6.L2::cache_hint \"\n              \"[%0], [%1, {%2}], [%3], %4, %5;\"\n              :: \"r\"(dst), \"l\"(tmap_ptr), \"r\"(x), \"r\"(mbar_addr), \"h\"(cta_mask), \"l\"(cache_policy), \"n\"(CTA_GROUP)\n              : \"memory\");\n}\n\ntemplate <int CTA_GROUP = 1>\n__device__ inline\nvoid tma_2d_gmem2smem(int dst, const void *tmap_ptr, int x, int y, int mbar_addr, uint64_t cache_policy) {\n  asm volatile(\"cp.async.bulk.tensor.2d.shared::cta.global.mbarrier::complete_tx::bytes.cta_group::%6.L2::cache_hint \"\n              \"[%0], [%1, {%2, %3}], [%4], %5;\"\n              :: \"r\"(dst), \"l\"(tmap_ptr), \"r\"(x), \"r\"(y), \"r\"(mbar_addr), \"l\"(cache_policy), \"n\"(CTA_GROUP)\n              : \"memory\");\n}\n\ntemplate <int CTA_GROUP = 1>\n__device__ inline\nvoid tma_2d_gmem2smem_mcast(int dst, const void *tmap_ptr, int x, int y, int mbar_addr, int16_t cta_mask, uint64_t cache_policy) {\n  asm volatile(\"cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.cta_group::%7.L2::cache_hint \"\n              \"[%0], [%1, {%2, %3}], [%4], %5, %6;\"\n              :: \"r\"(dst), \"l\"(tmap_ptr), \"r\"(x), \"r\"(y), \"r\"(mbar_addr), \"h\"(cta_mask), \"l\"(cache_policy), \"n\"(CTA_GROUP)\n              : \"memory\");\n}\n\ntemplate <int CTA_GROUP = 1>\n__device__ inline\nvoid tma_3d_gmem2smem(int dst, const void *tmap_ptr, int x, int y, int z, int mbar_addr, uint64_t cache_policy) {\n  asm volatile(\"cp.async.bulk.tensor.3d.shared::cta.global.mbarrier::complete_tx::bytes.cta_group::%7.L2::cache_hint \"\n              \"[%0], [%1, {%2, %3, %4}], [%5], %6;\"\n              :: \"r\"(dst), \"l\"(tmap_ptr), \"r\"(x), \"r\"(y), \"r\"(z), \"r\"(mbar_addr), \"l\"(cache_policy), \"n\"(CTA_GROUP)\n              : \"memory\");\n}\n\ntemplate <int CTA_GROUP = 1>\n__device__ inline\nvoid tma_3d_gmem2smem_mcast(int dst, const void *tmap_ptr, int x, int y, int z, int mbar_addr, int16_t cta_mask, uint64_t cache_policy) {\n  asm volatile(\"cp.async.bulk.tensor.3d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.cta_group::%8.L2::cache_hint \"\n              \"[%0], [%1, {%2, %3, %4}], [%5], %6, %7;\"\n              :: \"r\"(dst), \"l\"(tmap_ptr), \"r\"(x), \"r\"(y), \"r\"(z), \"r\"(mbar_addr), \"h\"(cta_mask), \"l\"(cache_policy), \"n\"(CTA_GROUP)\n              : \"memory\");\n}\n\ntemplate <int CTA_GROUP = 1>\n__device__ inline\nvoid tcgen05_cp_nvfp4(int taddr, uint64_t s_desc) {\n  // .32x128b corresponds to (32, 16) 8-bit scale -> 1 MMA for nvfp4.\n  // .warpx4 duplicates data across 32-lane groups.\n  asm volatile(\"tcgen05.cp.cta_group::%2.32x128b.warpx4 [%0], %1;\" :: \"r\"(taddr), \"l\"(s_desc), \"n\"(CTA_GROUP));\n}\n\ntemplate <int CTA_GROUP = 1>\n__device__ inline\nvoid tcgen05_commit(int mbar_addr) {\n  asm volatile(\"tcgen05.commit.cta_group::%1.mbarrier::arrive::one.shared::cluster.b64 [%0];\"\n              :: \"r\"(mbar_addr), \"n\"(CTA_GROUP) : \"memory\");\n}\n\ntemplate <int CTA_GROUP = 1>\n__device__ inline\nvoid tcgen05_commit_mcast(int mbar_addr, uint16_t cta_mask) {\n  asm volatile(\"tcgen05.commit.cta_group::%2.mbarrier::arrive::one.shared::cluster.multicast::cluster.b64 [%0], %1;\"\n              :: \"r\"(mbar_addr), \"h\"(cta_mask), \"n\"(CTA_GROUP) : \"memory\");\n}\n\nstruct COLLECTOR_USAGE {\n  static constexpr char NONE[]      = \"\";\n  static constexpr char A_FILL[]    = \".collector::a::fill\";\n  static constexpr char A_USE[]     = \".collector::a::use\";\n  static constexpr char A_LASTUSE[] = \".collector::a::lastuse\";\n  static constexpr char A_DISCARD[] = \".collector::a::discard\";\n};\n\ntemplate <int CTA_GROUP = 1, const char *collector_usage = COLLECTOR_USAGE::NONE>\n__device__ inline\nvoid tcgen05_mma_nvfp4(\n  int d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t i_desc,\n  int scale_A_tmem,\n  int scale_B_tmem,\n  int enable_input_d\n) {\n  asm volatile(\n    \"{\\n\\t\"\n    \".reg .pred p;\\n\\t\"  // predicate register enable-input-d\n    \"setp.ne.b32 p, %6, 0;\\n\\t\"\n    \"tcgen05.mma.cta_group::%7.kind::mxf4nvf4.block_scale.block16%8 [%0], %1, %2, %3, [%4], [%5], p;\\n\\t\"\n    \"}\"\n    :: \"r\"(d_tmem), \"l\"(a_desc), \"l\"(b_desc), \"r\"(i_desc),\n       \"r\"(scale_A_tmem), \"r\"(scale_B_tmem), \"r\"(enable_input_d),\n       \"n\"(CTA_GROUP), \"C\"(collector_usage)\n  );\n}\n\n// see https://docs.nvidia.com/cuda/inline-ptx-assembly/index.html\nstruct SHAPE {\n  static constexpr char _32x32b[]  = \".32x32b\";   // 32x1 tile for each warp\n  static constexpr char _16x128b[] = \".16x128b\";  // 16x4 tile\n  static constexpr char _16x256b[] = \".16x256b\";  // 16x8 tile\n};\n\ntemplate <int NUM_REGS, const char *SHAPE, int NUM>\n__device__ inline\nvoid tcgen05_ld(float *tmp, int row, int col) {\n  int addr = (row << 16) | col;\n\n  if constexpr (NUM_REGS == 1)\n  asm volatile(\"tcgen05.ld.sync.aligned%3.x%4.b32 {%0}, [%1];\"\n              : \"=f\"(tmp[0]) : \"r\"(addr), \"C\"(SHAPE), \"n\"(NUM));\n  if constexpr (NUM_REGS == 2)\n  asm volatile(\"tcgen05.ld.sync.aligned%3.x%4.b32 {%0, %1}, [%2];\"\n              : \"=f\"(tmp[0]), \"=f\"(tmp[1]) : \"r\"(addr), \"C\"(SHAPE), \"n\"(NUM));\n  if constexpr (NUM_REGS == 4)\n  asm volatile(\"tcgen05.ld.sync.aligned%5.x%6.b32 \"\n              \"{%0, %1, %2, %3}, [%4];\"\n              : \"=f\"(tmp[0]), \"=f\"(tmp[1]), \"=f\"(tmp[2]), \"=f\"(tmp[3])\n              : \"r\"(addr), \"C\"(SHAPE), \"n\"(NUM));\n  if constexpr (NUM_REGS == 8)\n  asm volatile(\"tcgen05.ld.sync.aligned%9.x%10.b32 \"\n              \"{ %0,  %1,  %2,  %3,  %4,  %5,  %6,  %7}, [%8];\"\n              : \"=f\"(tmp[0]), \"=f\"(tmp[1]), \"=f\"(tmp[2]), \"=f\"(tmp[3]), \"=f\"(tmp[4]), \"=f\"(tmp[5]), \"=f\"(tmp[6]), \"=f\"(tmp[7])\n              : \"r\"(addr), \"C\"(SHAPE), \"n\"(NUM));\n  if constexpr (NUM_REGS == 16)\n  asm volatile(\"tcgen05.ld.sync.aligned%17.x%18.b32 \"\n              \"{ %0,  %1,  %2,  %3,  %4,  %5,  %6,  %7, \"\n              \"  %8,  %9, %10, %11, %12, %13, %14, %15}, [%16];\"\n              : \"=f\"(tmp[ 0]), \"=f\"(tmp[ 1]), \"=f\"(tmp[ 2]), \"=f\"(tmp[ 3]), \"=f\"(tmp[ 4]), \"=f\"(tmp[ 5]), \"=f\"(tmp[ 6]), \"=f\"(tmp[ 7]),\n                \"=f\"(tmp[ 8]), \"=f\"(tmp[ 9]), \"=f\"(tmp[10]), \"=f\"(tmp[11]), \"=f\"(tmp[12]), \"=f\"(tmp[13]), \"=f\"(tmp[14]), \"=f\"(tmp[15])\n              : \"r\"(addr), \"C\"(SHAPE), \"n\"(NUM));\n  if constexpr (NUM_REGS == 32)\n  asm volatile(\"tcgen05.ld.sync.aligned%33.x%34.b32 \"\n              \"{ %0,  %1,  %2,  %3,  %4,  %5,  %6,  %7, \"\n              \"  %8,  %9, %10, %11, %12, %13, %14, %15, \"\n              \" %16, %17, %18, %19, %20, %21, %22, %23, \"\n              \" %24, %25, %26, %27, %28, %29, %30, %31}, [%32];\"\n              : \"=f\"(tmp[ 0]), \"=f\"(tmp[ 1]), \"=f\"(tmp[ 2]), \"=f\"(tmp[ 3]), \"=f\"(tmp[ 4]), \"=f\"(tmp[ 5]), \"=f\"(tmp[ 6]), \"=f\"(tmp[ 7]),\n                \"=f\"(tmp[ 8]), \"=f\"(tmp[ 9]), \"=f\"(tmp[10]), \"=f\"(tmp[11]), \"=f\"(tmp[12]), \"=f\"(tmp[13]), \"=f\"(tmp[14]), \"=f\"(tmp[15]),\n                \"=f\"(tmp[16]), \"=f\"(tmp[17]), \"=f\"(tmp[18]), \"=f\"(tmp[19]), \"=f\"(tmp[20]), \"=f\"(tmp[21]), \"=f\"(tmp[22]), \"=f\"(tmp[23]),\n                \"=f\"(tmp[24]), \"=f\"(tmp[25]), \"=f\"(tmp[26]), \"=f\"(tmp[27]), \"=f\"(tmp[28]), \"=f\"(tmp[29]), \"=f\"(tmp[30]), \"=f\"(tmp[31])\n              : \"r\"(addr), \"C\"(SHAPE), \"n\"(NUM));\n  if constexpr (NUM_REGS == 64)\n  asm volatile(\"tcgen05.ld.sync.aligned%65.x%66.b32 \"\n              \"{ %0,  %1,  %2,  %3,  %4,  %5,  %6,  %7, \"\n              \"  %8,  %9, %10, %11, %12, %13, %14, %15, \"\n              \" %16, %17, %18, %19, %20, %21, %22, %23, \"\n              \" %24, %25, %26, %27, %28, %29, %30, %31, \"\n              \" %32, %33, %34, %35, %36, %37, %38, %39, \"\n              \" %40, %41, %42, %43, %44, %45, %46, %47, \"\n              \" %48, %49, %50, %51, %52, %53, %54, %55, \"\n              \" %56, %57, %58, %59, %60, %61, %62, %63}, [%64];\"\n              : \"=f\"(tmp[ 0]), \"=f\"(tmp[ 1]), \"=f\"(tmp[ 2]), \"=f\"(tmp[ 3]), \"=f\"(tmp[ 4]), \"=f\"(tmp[ 5]), \"=f\"(tmp[ 6]), \"=f\"(tmp[ 7]),\n                \"=f\"(tmp[ 8]), \"=f\"(tmp[ 9]), \"=f\"(tmp[10]), \"=f\"(tmp[11]), \"=f\"(tmp[12]), \"=f\"(tmp[13]), \"=f\"(tmp[14]), \"=f\"(tmp[15]),\n                \"=f\"(tmp[16]), \"=f\"(tmp[17]), \"=f\"(tmp[18]), \"=f\"(tmp[19]), \"=f\"(tmp[20]), \"=f\"(tmp[21]), \"=f\"(tmp[22]), \"=f\"(tmp[23]),\n                \"=f\"(tmp[24]), \"=f\"(tmp[25]), \"=f\"(tmp[26]), \"=f\"(tmp[27]), \"=f\"(tmp[28]), \"=f\"(tmp[29]), \"=f\"(tmp[30]), \"=f\"(tmp[31]),\n                \"=f\"(tmp[32]), \"=f\"(tmp[33]), \"=f\"(tmp[34]), \"=f\"(tmp[35]), \"=f\"(tmp[36]), \"=f\"(tmp[37]), \"=f\"(tmp[38]), \"=f\"(tmp[39]),\n                \"=f\"(tmp[40]), \"=f\"(tmp[41]), \"=f\"(tmp[42]), \"=f\"(tmp[43]), \"=f\"(tmp[44]), \"=f\"(tmp[45]), \"=f\"(tmp[46]), \"=f\"(tmp[47]),\n                \"=f\"(tmp[48]), \"=f\"(tmp[49]), \"=f\"(tmp[50]), \"=f\"(tmp[51]), \"=f\"(tmp[52]), \"=f\"(tmp[53]), \"=f\"(tmp[54]), \"=f\"(tmp[55]),\n                \"=f\"(tmp[56]), \"=f\"(tmp[57]), \"=f\"(tmp[58]), \"=f\"(tmp[59]), \"=f\"(tmp[60]), \"=f\"(tmp[61]), \"=f\"(tmp[62]), \"=f\"(tmp[63])\n              : \"r\"(addr), \"C\"(SHAPE), \"n\"(NUM));\n  if constexpr (NUM_REGS == 128)\n  asm volatile(\"tcgen05.ld.sync.aligned%129.x%130.b32 \"\n              \"{ %0,  %1,  %2,  %3,  %4,  %5,  %6,  %7, \"\n              \"  %8,  %9, %10, %11, %12, %13, %14, %15, \"\n              \" %16, %17, %18, %19, %20, %21, %22, %23, \"\n              \" %24, %25, %26, %27, %28, %29, %30, %31, \"\n              \" %32, %33, %34, %35, %36, %37, %38, %39, \"\n              \" %40, %41, %42, %43, %44, %45, %46, %47, \"\n              \" %48, %49, %50, %51, %52, %53, %54, %55, \"\n              \" %56, %57, %58, %59, %60, %61, %62, %63, \"\n              \" %64, %65, %66, %67, %68, %69, %70, %71, \"\n              \" %72, %73, %74, %75, %76, %77, %78, %79, \"\n              \" %80, %81, %82, %83, %84, %85, %86, %87, \"\n              \" %88, %89, %90, %91, %92, %93, %94, %95, \"\n              \" %96, %97, %98, %99,%100,%101,%102,%103, \"\n              \"%104,%105,%106,%107,%108,%109,%110,%111, \"\n              \"%112,%113,%114,%115,%116,%117,%118,%119, \"\n              \"%120,%121,%122,%123,%124,%125,%126,%127}, [%128];\"\n              : \"=f\"(tmp[ 0]), \"=f\"(tmp[ 1]), \"=f\"(tmp[ 2]), \"=f\"(tmp[ 3]), \"=f\"(tmp[ 4]), \"=f\"(tmp[ 5]), \"=f\"(tmp[ 6]), \"=f\"(tmp[ 7]),\n                \"=f\"(tmp[ 8]), \"=f\"(tmp[ 9]), \"=f\"(tmp[10]), \"=f\"(tmp[11]), \"=f\"(tmp[12]), \"=f\"(tmp[13]), \"=f\"(tmp[14]), \"=f\"(tmp[15]),\n                \"=f\"(tmp[16]), \"=f\"(tmp[17]), \"=f\"(tmp[18]), \"=f\"(tmp[19]), \"=f\"(tmp[20]), \"=f\"(tmp[21]), \"=f\"(tmp[22]), \"=f\"(tmp[23]),\n                \"=f\"(tmp[24]), \"=f\"(tmp[25]), \"=f\"(tmp[26]), \"=f\"(tmp[27]), \"=f\"(tmp[28]), \"=f\"(tmp[29]), \"=f\"(tmp[30]), \"=f\"(tmp[31]),\n                \"=f\"(tmp[32]), \"=f\"(tmp[33]), \"=f\"(tmp[34]), \"=f\"(tmp[35]), \"=f\"(tmp[36]), \"=f\"(tmp[37]), \"=f\"(tmp[38]), \"=f\"(tmp[39]),\n                \"=f\"(tmp[40]), \"=f\"(tmp[41]), \"=f\"(tmp[42]), \"=f\"(tmp[43]), \"=f\"(tmp[44]), \"=f\"(tmp[45]), \"=f\"(tmp[46]), \"=f\"(tmp[47]),\n                \"=f\"(tmp[48]), \"=f\"(tmp[49]), \"=f\"(tmp[50]), \"=f\"(tmp[51]), \"=f\"(tmp[52]), \"=f\"(tmp[53]), \"=f\"(tmp[54]), \"=f\"(tmp[55]),\n                \"=f\"(tmp[56]), \"=f\"(tmp[57]), \"=f\"(tmp[58]), \"=f\"(tmp[59]), \"=f\"(tmp[60]), \"=f\"(tmp[61]), \"=f\"(tmp[62]), \"=f\"(tmp[63]),\n                \"=f\"(tmp[64]), \"=f\"(tmp[65]), \"=f\"(tmp[66]), \"=f\"(tmp[67]), \"=f\"(tmp[68]), \"=f\"(tmp[69]), \"=f\"(tmp[70]), \"=f\"(tmp[71]),\n                \"=f\"(tmp[72]), \"=f\"(tmp[73]), \"=f\"(tmp[74]), \"=f\"(tmp[75]), \"=f\"(tmp[76]), \"=f\"(tmp[77]), \"=f\"(tmp[78]), \"=f\"(tmp[79]),\n                \"=f\"(tmp[80]), \"=f\"(tmp[81]), \"=f\"(tmp[82]), \"=f\"(tmp[83]), \"=f\"(tmp[84]), \"=f\"(tmp[85]), \"=f\"(tmp[86]), \"=f\"(tmp[87]),\n                \"=f\"(tmp[88]), \"=f\"(tmp[89]), \"=f\"(tmp[90]), \"=f\"(tmp[91]), \"=f\"(tmp[92]), \"=f\"(tmp[93]), \"=f\"(tmp[94]), \"=f\"(tmp[95]),\n                \"=f\"(tmp[96]), \"=f\"(tmp[97]), \"=f\"(tmp[98]), \"=f\"(tmp[99]), \"=f\"(tmp[100]),\"=f\"(tmp[101]),\"=f\"(tmp[102]),\"=f\"(tmp[103]),\n                \"=f\"(tmp[104]),\"=f\"(tmp[105]),\"=f\"(tmp[106]),\"=f\"(tmp[107]),\"=f\"(tmp[108]),\"=f\"(tmp[109]),\"=f\"(tmp[110]),\"=f\"(tmp[111]),\n                \"=f\"(tmp[112]),\"=f\"(tmp[113]),\"=f\"(tmp[114]),\"=f\"(tmp[115]),\"=f\"(tmp[116]),\"=f\"(tmp[117]),\"=f\"(tmp[118]),\"=f\"(tmp[119]),\n                \"=f\"(tmp[120]),\"=f\"(tmp[121]),\"=f\"(tmp[122]),\"=f\"(tmp[123]),\"=f\"(tmp[124]),\"=f\"(tmp[125]),\"=f\"(tmp[126]),\"=f\"(tmp[127])\n              : \"r\"(addr), \"C\"(SHAPE), \"n\"(NUM));\n}\n\ntemplate <int num>\n__device__ inline void\ntcgen05_ld_32x32b(float *tmp, int row, int col) {\n  // each 32x32b tile uses 1 register per thread\n  tcgen05_ld<num, SHAPE::_32x32b, num>(tmp, row, col);\n}\n\ntemplate <int num>\n__device__ inline\nvoid tcgen05_ld_16x128b(float *tmp, int row, int col) {\n  // each 16x128b tile uses 2 registers per thread\n  tcgen05_ld<num * 2, SHAPE::_16x128b, num>(tmp, row, col);\n}\n\ntemplate <int num>\n__device__ inline\nvoid tcgen05_ld_16x256b(float *tmp, int row, int col) {\n  // each 16x256b tile uses 4 registers per thread\n  tcgen05_ld<num * 4, SHAPE::_16x256b, num>(tmp, row, col);\n}\n\nconstexpr int BLOCK_M = 128;\nconstexpr int BLOCK_K = 256;\nconstexpr int NUM_WARPS = 6;\nconstexpr int TB_SIZE = NUM_WARPS * WARP_SIZE;\n\ntemplate <typename T>\n__device__ __inline__\nT warp_uniform(T x) { return __shfl_sync(0xFFFF'FFFF, x, 0); }\n\ntemplate <int K, int BLOCK_N, int NUM_STAGES, bool ONE_MMA>\n__global__\n__cluster_dims__(2, 1, 1)\n__launch_bounds__(TB_SIZE)\nvoid kernel_cutlass (\n  const __grid_constant__ CUtensorMap A_tmap,\n  const __grid_constant__ CUtensorMap B1_tmap,\n  const __grid_constant__ CUtensorMap B2_tmap,\n  const __grid_constant__ CUtensorMap SFA_tmap,\n  const __grid_constant__ CUtensorMap SFB1_tmap,\n  const __grid_constant__ CUtensorMap SFB2_tmap,\n  half *C_ptr,\n  int M, int N\n) {\n  const int tid = threadIdx.x;\n  const int bid_m = warp_uniform(blockIdx.x);\n  const int bid_n = warp_uniform(blockIdx.y);\n  const int cta_rank = bid_m % 2;\n\n  const int lane_id = tid % WARP_SIZE;\n  const int warp_id = warp_uniform(tid / WARP_SIZE);  // making warp uniform is important\n\n  const int off_m = bid_m * BLOCK_M;\n  const int off_n = bid_n * BLOCK_N;\n\n  // set up smem\n  extern __shared__ __align__(1024) char smem_ptr[];\n  const int smem = static_cast<int>(__cvta_generic_to_shared(smem_ptr));\n  constexpr int A_size = BLOCK_M * BLOCK_K / 2;\n  constexpr int B_size = BLOCK_N * BLOCK_K / 2;\n  constexpr int SF_size = 128 * BLOCK_K / 16;\n  constexpr int STAGE_SIZE = A_size + B_size + SF_size * 3;  // SFA + SFB1 + SFB2\n\n  // set up mbarriers and tmem\n  const int tma_mbar_addr = smem + NUM_STAGES * STAGE_SIZE;\n  const int mma_mbar_addr = tma_mbar_addr + NUM_STAGES * 8;\n  const int mainloop_mbar_addr = mma_mbar_addr + NUM_STAGES * 8;\n\n  constexpr uint64_t cache_A = EVICT_NORMAL;\n  constexpr uint64_t cache_B = EVICT_FIRST;\n\n  constexpr int bar_epilogue = 2;\n  constexpr int rest_k = K / 16 / 4;\n\n  if (warp_id == 0 && elect_sync()) {\n    // this is better than cp.async.bulk.prefetch.tensor\n    asm volatile(\"prefetch.tensormap [%0];\" :: \"l\"(&A_tmap) : \"memory\");\n    asm volatile(\"prefetch.tensormap [%0];\" :: \"l\"(&B1_tmap) : \"memory\");\n    asm volatile(\"prefetch.tensormap [%0];\" :: \"l\"(&B2_tmap) : \"memory\");\n    asm volatile(\"prefetch.tensormap [%0];\" :: \"l\"(&SFA_tmap) : \"memory\");\n    asm volatile(\"prefetch.tensormap [%0];\" :: \"l\"(&SFB1_tmap) : \"memory\");\n    asm volatile(\"prefetch.tensormap [%0];\" :: \"l\"(&SFB2_tmap) : \"memory\");\n\n    //tma_3d_prefetch(&A_tmap, 0, off_m, 0, cache_A);\n    //tma_3d_prefetch(&B1_tmap, 0, off_n, 0, cache_B);\n    //tma_3d_prefetch(&B2_tmap, 0, off_n, 0, cache_B);\n    tma_1d_prefetch(&SFA_tmap, bid_m * rest_k * 512 / 8, cache_A);\n    tma_1d_prefetch(&SFB1_tmap, (off_n / 128) * rest_k * 512 / 8, cache_B);\n    tma_1d_prefetch(&SFB2_tmap, (off_n / 128) * rest_k * 512 / 8, cache_B);\n  }\n  else if (warp_id == 1 && elect_sync()) {\n    // 1 thread init mbarrier\n    for (int i = 0; i < NUM_STAGES; i++) {\n      mbarrier_init(tma_mbar_addr + i * 8, 2);  // 2 CTAs report to CTA0 only\n      mbarrier_init(mma_mbar_addr + i * 8, 1);\n    }\n    mbarrier_init(mainloop_mbar_addr, 1);\n    asm volatile(\"fence.mbarrier_init.release.cluster;\");  // visible to async proxy\n  }\n\n  // mbarrier visible to all threads in cluster\n  // .relaxed since we already use .release for fence.mbarrier_init earlier.\n  asm volatile(\"barrier.cluster.arrive.relaxed.aligned;\");\n  asm volatile(\"barrier.cluster.wait.acquire.aligned;\");\n\n  constexpr int num_iters = K / BLOCK_K;\n\n  // warp-specialization\n  if (warp_id == NUM_WARPS - 2) {\n    // TMA warp\n    constexpr int16_t cta_mask = 3;\n\n    if (elect_sync()) {\n      int stage_id = 0;\n      int mma_phase = 1;\n\n      auto   B_tmap_ptr = cta_rank == 0 ?   &B1_tmap :   &B2_tmap;\n      auto SFB_tmap_ptr = cta_rank == 0 ? &SFB1_tmap : &SFB2_tmap;\n\n      const int b_off_n = off_n + cta_rank * (BLOCK_N / 2);\n\n      // don't unroll. this is important\n      #pragma unroll 1\n      for (int iter_k = 0; iter_k < num_iters; iter_k++) {\n        // select tma mbar and smem\n        const int mbar_addr = (tma_mbar_addr + stage_id * 8) & 0xFEFFFFFF;  // CTA0\n        const int A_smem = smem + stage_id * STAGE_SIZE;\n        const int B1_smem = A_smem + A_size;\n        const int B2_smem = B1_smem + B_size / 2;\n        const int SFA_smem = B1_smem + B_size;\n        const int SFB_smem = SFA_smem + (1 + cta_rank) * SF_size;\n\n        // wait MMA\n        mbarrier_wait(mma_mbar_addr + stage_id * 8, mma_phase);\n\n        // issue TMA\n        // issue order has a significant impact on benchmark.2 and benchmark.3\n        if constexpr (ONE_MMA) {\n          tma_3d_gmem2smem<2>(B1_smem, B_tmap_ptr, 0, off_n, iter_k, mbar_addr, cache_B);\n        } else {\n          tma_3d_gmem2smem<2>(B1_smem, &B1_tmap, 0, b_off_n, iter_k, mbar_addr, cache_B);\n          tma_3d_gmem2smem<2>(B2_smem, &B2_tmap, 0, b_off_n, iter_k, mbar_addr, cache_B);\n        }\n        tma_3d_gmem2smem<2>(A_smem, &A_tmap, 0, off_m, iter_k, mbar_addr, cache_A);\n\n        // divide by 8 because we use int64 as dtype for tensor map (to get around boxDim<=256 restriction).\n        const int off_sfb = (off_n / 128) * rest_k * 512 + iter_k * 2048;\n        const int off_sfa = bid_m * rest_k * 512 + iter_k * 2048;\n        tma_1d_gmem2smem_mcast<2>(SFB_smem, SFB_tmap_ptr, off_sfb / 8, mbar_addr, cta_mask, cache_B);\n        tma_1d_gmem2smem<2>(SFA_smem, &SFA_tmap, off_sfa / 8, mbar_addr, cache_A);\n\n        mbarrier_arrive_expect_tx(mbar_addr, STAGE_SIZE);  // signal TMA done\n        stage_id = (stage_id + 1) % NUM_STAGES;\n        if (stage_id == 0)\n          mma_phase ^= 1;\n\n        //constexpr int prefetch_dist = 1;\n        //if (iter_k + prefetch_dist < num_iters) {\n        //  tma_3d_prefetch(&B1_tmap, 0, b_off_n, iter_k + prefetch_dist, cache_B);\n        //  tma_3d_prefetch(&B2_tmap, 0, b_off_n, iter_k + prefetch_dist, cache_B);\n        //}\n      }\n    }\n  }\n  else if (warp_id == NUM_WARPS - 1) {\n    // MMA warp\n    // allocate tmem (both CTAs issue)\n    // must use a separate smem address, even though we don't care about the returned value\n    int addr = mainloop_mbar_addr + 8;\n    asm volatile(\"tcgen05.alloc.cta_group::2.sync.aligned.shared::cta.b32 [%0], %1;\" :: \"r\"(addr), \"r\"(BLOCK_N * 4));\n\n    // https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instruction-descriptor\n    constexpr uint32_t MMA_M = BLOCK_M * 2;  // CTA0 and CTA1\n    constexpr uint32_t MMA_N = ONE_MMA ? BLOCK_N * 2 : BLOCK_N;  // B1 and B2\n    constexpr uint32_t i_desc = (1U << 7U)   // atype=E2M1\n                              | (1U << 10U)  // btype=E2M1\n                              | (MMA_N >> 3U << 17U)\n                              | (MMA_M >> 7U << 27U)\n                              ;\n    constexpr int16_t cta_mask = 3;\n\n    // only CTA0 issues MMA\n    if (cta_rank == 0 && elect_sync()) {\n      int stage_id = 0;\n      int tma_phase = 0;\n\n      // only works for ONE_MMA=false\n      const int sfb_offset = BLOCK_N == 64 ? (bid_n % 2) * 2 : 0;\n\n      for (int iter_k = 0; iter_k < num_iters; iter_k++) {\n        // select smem\n        const int A_smem = smem + stage_id * STAGE_SIZE;\n        const int B1_smem = A_smem + A_size;\n        const int B2_smem = B1_smem + B_size / 2;  // only used when ONE_MMA=false. otherwise, B_smem is either all B1 or all B2.\n        const int SFA_smem = B1_smem + B_size;\n        const int SFB1_smem = SFA_smem + SF_size;\n        const int SFB2_smem = SFB1_smem + SF_size;\n\n        // set up shared memory descriptors\n        // https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-shared-memory-descriptor\n        // no swizzling\n        constexpr uint64_t SF_desc = (desc_encode(8 * 16) << 32ULL) | (1ULL << 46ULL);\n        uint64_t sfa_desc = SF_desc | (SFA_smem >> 4ULL);\n        uint64_t sfb1_desc = SF_desc | (SFB1_smem >> 4ULL);\n        uint64_t sfb2_desc = SF_desc | (SFB2_smem >> 4ULL);\n\n        // AB_desc: 128-byte swizzling. LBO is implied to be 1.\n        constexpr uint64_t AB_desc = (desc_encode(8 * 128) << 32ULL) | (1ULL << 46ULL) | (2ULL << 61ULL);\n        uint64_t a_desc = AB_desc | (A_smem >> 4ULL);\n        uint64_t b1_desc = AB_desc | (B1_smem >> 4ULL);\n        uint64_t b2_desc = AB_desc | (B2_smem >> 4ULL);\n\n        // tmem layout: | A@B1  | A@B2  | SFA | SFB |\n        //              |BLOCK_N|BLOCK_N| ... | ... |\n        // each MMA consumes:\n        // - (128, 64) of A -> (128, 4) of SFA -> reshaped as (32, 4', 4) -> 4 tmem columns\n        int scale_A_tmem = BLOCK_N * 2;\n        int scale_B1_tmem = BLOCK_N * 3;\n        int scale_B2_tmem = scale_B1_tmem + 4;\n\n        // wait TMA\n        mbarrier_wait(tma_mbar_addr + stage_id * 8, tma_phase);\n\n        // manually unroll the 1st iteration\n        // tcgen05.cp -> tcgen05.mma should be pipelined correctly per PTX doc\n        // https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-memory-consistency-model-pipelined-instructions\n        // scale layout is a bit cursed for 2-SM MMA\n        tcgen05_cp_nvfp4<2>(scale_A_tmem, sfa_desc);\n        tcgen05_cp_nvfp4<2>(scale_B1_tmem, sfb1_desc);\n        tcgen05_cp_nvfp4<2>(scale_B2_tmem, sfb2_desc);\n\n        if constexpr (ONE_MMA) {\n          tcgen05_mma_nvfp4<2>(0, a_desc, b1_desc, i_desc, scale_A_tmem, scale_B1_tmem, iter_k);\n        } else {\n          tcgen05_mma_nvfp4<2, COLLECTOR_USAGE::A_FILL   >(      0, a_desc, b1_desc, i_desc, scale_A_tmem, scale_B1_tmem + sfb_offset, iter_k);\n          tcgen05_mma_nvfp4<2, COLLECTOR_USAGE::A_LASTUSE>(BLOCK_N, a_desc, b2_desc, i_desc, scale_A_tmem, scale_B2_tmem + sfb_offset, iter_k);\n        }\n\n        for (int k = 1; k < BLOCK_K / MMA_K; k++) {\n          // go to the next 4 columns\n          scale_A_tmem += 4;\n          scale_B1_tmem += 8;\n          scale_B2_tmem += 8;\n\n          // go to the next 512-byte\n          sfa_desc += (512 >> 4);\n          sfb1_desc += (512 >> 4);\n          sfb2_desc += (512 >> 4);\n\n          // go to the next 32-byte\n          a_desc += (32 >> 4);\n          b1_desc += (32 >> 4);\n          b2_desc += (32 >> 4);\n\n          tcgen05_cp_nvfp4<2>(scale_A_tmem, sfa_desc);\n          tcgen05_cp_nvfp4<2>(scale_B1_tmem, sfb1_desc);\n          tcgen05_cp_nvfp4<2>(scale_B2_tmem, sfb2_desc);\n\n          if constexpr (ONE_MMA) {\n            tcgen05_mma_nvfp4<2>(0, a_desc, b1_desc, i_desc, scale_A_tmem, scale_B1_tmem, 1);\n          } else {\n            tcgen05_mma_nvfp4<2, COLLECTOR_USAGE::A_FILL   >(      0, a_desc, b1_desc, i_desc, scale_A_tmem, scale_B1_tmem + sfb_offset, 1);\n            tcgen05_mma_nvfp4<2, COLLECTOR_USAGE::A_LASTUSE>(BLOCK_N, a_desc, b2_desc, i_desc, scale_A_tmem, scale_B2_tmem + sfb_offset, 1);\n          }\n        }\n\n        tcgen05_commit_mcast<2>(mma_mbar_addr + stage_id * 8, cta_mask);  // signal MMA done\n        stage_id = (stage_id + 1) % NUM_STAGES;\n        if (stage_id == 0)\n          tma_phase ^= 1;\n      }\n\n      tcgen05_commit_mcast<2>(mainloop_mbar_addr, cta_mask);  // signal mainloop done\n    }\n  }\n  else {\n    // epilogue warps\n    // wait mainloop\n    if (warp_id == 0)\n      mbarrier_wait(mainloop_mbar_addr, 0);\n    asm volatile(\"bar.sync %0, %1;\" :: \"n\"(bar_epilogue), \"r\"(4 * WARP_SIZE) : \"memory\");\n    asm volatile(\"tcgen05.fence::after_thread_sync;\");\n\n    // multiplying y first is sometimes faster\n    auto act = [](float x, float y) { return y * x / (1.0f + __expf(-x)); };\n\n    auto epilogue_16x256b = [&]() {\n      // smaller width = less registers + some pipelining\n      constexpr int WIDTH = std::min(BLOCK_N, 8);\n\n      for (int m = 0; m < 32 / 16; m++)\n        for (int n = 0; n < BLOCK_N / WIDTH; n++) {\n          float b1[WIDTH / 2];\n          float b2[WIDTH / 2];\n\n          constexpr int num = WIDTH / 8;  // each 16x256b tile is 8-element wide\n          int row = cta_rank * BLOCK_M + warp_id * 32 + m * 16;\n          tcgen05_ld_16x256b<num>(b1, row, n * WIDTH);\n          tcgen05_ld_16x256b<num>(b2, row, n * WIDTH + BLOCK_N);\n          asm volatile(\"tcgen05.wait::ld.sync.aligned;\");\n\n          half2 out[WIDTH / 4];\n          for (int i = 0; i < WIDTH / 4; i++) {\n            float o0 = act(b1[i * 2 + 0], b2[i * 2 + 0]);\n            float o1 = act(b1[i * 2 + 1], b2[i * 2 + 1]);\n            out[i] = __float22half2_rn({o0, o1});\n          }\n\n          for (int i = 0; i < WIDTH / 8; i++) {\n            const int row = off_m + warp_id * 32 + m * 16 + lane_id / 4;\n            const int col = off_n + n * WIDTH + i * 8 + (lane_id % 4) * 2;\n\n            reinterpret_cast<half2 *>(C_ptr + (row + 0) * N + col)[0] = out[i * 2 + 0];\n            reinterpret_cast<half2 *>(C_ptr + (row + 8) * N + col)[0] = out[i * 2 + 1];\n          }\n        }\n    };\n\n    auto epilogue_32x32b = [&]() {\n      // smaller width = less registers + some pipelining\n      constexpr int WIDTH = std::min(BLOCK_N, 16);\n\n      for (int n = 0; n < BLOCK_N / WIDTH; n++) {\n        float b1[WIDTH];\n        float b2[WIDTH];\n\n        int row = cta_rank * BLOCK_M + warp_id * 32;\n        tcgen05_ld_32x32b<WIDTH>(b1, row, n * WIDTH);\n        tcgen05_ld_32x32b<WIDTH>(b2, row, n * WIDTH + BLOCK_N);\n        asm volatile(\"tcgen05.wait::ld.sync.aligned;\");\n\n        half2 out[WIDTH / 2];\n        for (int i = 0; i < WIDTH / 2; i++) {\n          float o0 = act(b1[i * 2 + 0], b2[i * 2 + 0]);\n          float o1 = act(b1[i * 2 + 1], b2[i * 2 + 1]);\n          out[i] = __float22half2_rn({o0, o1});\n        }\n\n        // iterate 16-byte over half2[WIDTH/2]\n        for (int i = 0; i < WIDTH / 8; i++) {\n          const int row = off_m + tid;\n          const int col = off_n + n * WIDTH + i * 8;\n          reinterpret_cast<int4 *>(C_ptr + row * N + col)[0] = reinterpret_cast<int4 *>(out)[i];\n        }\n      }\n    };\n\n    epilogue_16x256b();\n    //epilogue_32x32b();\n\n    //asm volatile(\"bar.sync %0, %1;\" :: \"n\"(bar_epilogue), \"r\"(4 * WARP_SIZE) : \"memory\");  // everyone is done with tmem\n\n    // .relaxed since we only need to synchronize execution, not memory\n    asm volatile(\"barrier.cluster.arrive.relaxed.aligned;\");\n    asm volatile(\"barrier.cluster.wait.acquire.aligned;\");\n\n    if (warp_id == 0)  // deallocate tmem. tmem address should be 0.\n      asm volatile(\"tcgen05.dealloc.cta_group::2.sync.aligned.b32 %0, %1;\" :: \"r\"(0), \"r\"(BLOCK_N * 4));\n  }\n}\n\nvoid check_cu(CUresult err) {\n  if (err == CUDA_SUCCESS) return;\n  const char *error_msg_ptr;\n  if (cuGetErrorString(err, &error_msg_ptr) != CUDA_SUCCESS)\n    error_msg_ptr = \"unable to get error string\";\n  TORCH_CHECK(false, \"cuTensorMapEncodeTiled error: \", error_msg_ptr);\n}\n\nvoid check_cuda(cudaError_t err) {\n  if (err == cudaSuccess) return;\n  TORCH_CHECK(false, cudaGetErrorString(err));\n}\n\nvoid init_AB_tmap(\n  CUtensorMap *tmap,\n  const char *ptr,\n  uint64_t global_height, uint64_t global_width,\n  uint32_t shared_height, uint32_t shared_width\n) {\n  constexpr uint32_t rank = 3;\n  uint64_t globalDim[rank]       = {256, global_height, global_width / 256};\n  uint64_t globalStrides[rank-1] = {global_width / 2, 128};  // in bytes\n  uint32_t boxDim[rank]          = {256, shared_height, shared_width / 256};\n  uint32_t elementStrides[rank]  = {1, 1, 1};\n\n  auto err = cuTensorMapEncodeTiled(\n    tmap,\n    CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_16U4_ALIGN8B,\n    rank,\n    (void *)ptr,\n    globalDim,\n    globalStrides,\n    boxDim,\n    elementStrides,\n    CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n    CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_128B,\n    CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n    CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n  );\n  //check_cu(err);\n}\n\nvoid init_SF_tmap(CUtensorMap *tmap, const char *ptr, uint64_t global_size, uint32_t shared_size) {\n  // use int64 as dtype, hence divide sizes by 8\n  constexpr uint32_t rank = 1;\n  uint64_t globalDim[rank]       = {global_size / 8};\n  uint64_t globalStrides[rank-1] = {};  // in bytes\n  uint32_t boxDim[rank]          = {shared_size / 8};\n  uint32_t elementStrides[rank]  = {1};\n\n  auto err = cuTensorMapEncodeTiled(\n    tmap,\n    CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_INT64,\n    rank,\n    (void *)ptr,\n    globalDim,\n    globalStrides,\n    boxDim,\n    elementStrides,\n    CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n    CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE,\n    CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n    CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n  );\n  //check_cu(err);\n}\n\ntemplate <int K, int BLOCK_N, bool ONE_MMA>\nvoid dual_gemm_launch(\n  const char *A_ptr,\n  const char *B1_ptr,\n  const char *B2_ptr,\n  const char *SFA_ptr,\n  const char *SFB1_ptr,\n  const char *SFB2_ptr,\n  half *C_ptr,\n  int M, int N\n) {\n  CUtensorMap A_tmap, B1_tmap, B2_tmap, SFA_tmap, SFB1_tmap, SFB2_tmap;\n  init_AB_tmap(&A_tmap, A_ptr, M, K, BLOCK_M, BLOCK_K);\n  if constexpr (ONE_MMA) {\n    init_AB_tmap(&B1_tmap, B1_ptr, N, K, BLOCK_N, BLOCK_K);\n    init_AB_tmap(&B2_tmap, B2_ptr, N, K, BLOCK_N, BLOCK_K);\n  } else {\n    init_AB_tmap(&B1_tmap, B1_ptr, N, K, BLOCK_N / 2, BLOCK_K);\n    init_AB_tmap(&B2_tmap, B2_ptr, N, K, BLOCK_N / 2, BLOCK_K);\n  }\n\n  init_SF_tmap(&SFA_tmap, SFA_ptr, M * K / 16, 128 * BLOCK_K / 16);\n  init_SF_tmap(&SFB1_tmap, SFB1_ptr, N * K / 16, 128 * BLOCK_K / 16);\n  init_SF_tmap(&SFB2_tmap, SFB2_ptr, N * K / 16, 128 * BLOCK_K / 16);\n\n  dim3 grid(M / BLOCK_M, N / BLOCK_N);\n\n  constexpr int AB_size = (BLOCK_M + BLOCK_N) * (BLOCK_K / 2);\n  constexpr int SF_size = 128 * (BLOCK_K / 16) * 3;  // SFB is still duplicated\n\n  constexpr int sm100_size = 227'000;\n  constexpr int dynamic_size = AB_size + SF_size + 2 * 8;  // 1 tma_mbar, 1 mma_mbar\n  constexpr int static_size = 8 + 4;  // 1 mainloop_mbar, tmem_addr\n  constexpr int NUM_STAGES = (sm100_size - static_size) / dynamic_size;\n\n  constexpr int smem_size = dynamic_size * NUM_STAGES + static_size;\n\n  // cutlass incantation (this affects ptxas)\n  auto this_kernel = kernel_cutlass<K, BLOCK_N, NUM_STAGES, ONE_MMA>;\n  cudaFuncSetAttribute(this_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);\n  this_kernel<<<grid, TB_SIZE, smem_size>>>(A_tmap, B1_tmap, B2_tmap, SFA_tmap, SFB1_tmap, SFB2_tmap, C_ptr, M, N);\n}\n\nat::Tensor dual_gemm(\n  const at::Tensor& A,\n  const at::Tensor& B1,\n  const at::Tensor& B2,\n  const at::Tensor& SFA,\n  const at::Tensor& SFB1,\n  const at::Tensor& SFB2,\n        at::Tensor& C\n) {\n  const int M = A.size(0);\n  const int N = B1.size(0);\n  const int K = A.size(1) * 2;\n\n  auto A_ptr    = reinterpret_cast<const char *>(A.data_ptr());\n  auto B1_ptr   = reinterpret_cast<const char *>(B1.data_ptr());\n  auto B2_ptr   = reinterpret_cast<const char *>(B2.data_ptr());\n  auto SFA_ptr  = reinterpret_cast<const char *>(SFA.data_ptr());\n  auto SFB1_ptr = reinterpret_cast<const char *>(SFB1.data_ptr());\n  auto SFB2_ptr = reinterpret_cast<const char *>(SFB2.data_ptr());\n  auto C_ptr    = reinterpret_cast<half *>(C.data_ptr());\n\n  constexpr bool ONE_MMA = false;\n\n#define LAUNCH(K_, BLOCK_N) \\\n  dual_gemm_launch<K_, BLOCK_N, ONE_MMA>(A_ptr, B1_ptr, B2_ptr, SFA_ptr, SFB1_ptr, SFB2_ptr, C_ptr, M, N);\n\n  if (false) {}\n  else if (M == 256 && K == 7168) LAUNCH(7168, 64)\n  else if (M == 512 && K == 7168) LAUNCH(7168, 128)\n  else if (M == 256 && K == 4096) LAUNCH(4096, 64)\n  // the rest\n  else if (K ==  256) LAUNCH( 256, 128)\n  else if (K ==  512) LAUNCH( 512, 128)\n  else if (K == 1536) LAUNCH(1536, 128)\n  else if (K == 2048) LAUNCH(2048, 128)\n  else if (K == 2304) LAUNCH(2304, 128)\n  else if (K == 7168) LAUNCH(7168, 128)\n\n#undef LAUNCH\n\n  return C;\n}\n\nTORCH_LIBRARY(my_module, m) {\n  m.def(\"dual_gemm(Tensor A, Tensor B1, Tensor B2, Tensor SFA, Tensor SFB1, Tensor SFB2, Tensor(a!) C) -> Tensor\");\n  m.impl(\"dual_gemm\", &dual_gemm);\n}\n\"\"\"\n\nload_inline(\n    \"dual_gemm\",\n    cpp_sources=\"\",\n    cuda_sources=CUDA_SRC,\n    verbose=True,\n    is_python_module=False,\n    no_implicit_headers=True,\n    extra_cuda_cflags=[\n        \"-O3\",\n        \"-gencode=arch=compute_100a,code=sm_100a\",\n        \"--use_fast_math\",\n        \"--expt-relaxed-constexpr\",\n        \"--relocatable-device-code=false\",\n        \"-lineinfo\",\n        \"-Xptxas=-v\",\n        # \"--keep\",\n        # \"--keep-dir\",\n        # f\"{Path(__file__).parent}/tmp\",\n    ],\n    extra_ldflags=[\"-lcuda\"],\n)\ndual_gemm = torch.ops.my_module.dual_gemm\n\n\ndef custom_kernel(data: input_t) -> output_t:\n    out = dual_gemm(data[0], data[1], data[2], data[6], data[7], data[8], data[9])\n    # torch.cuda.synchronize()\n    return out\n"
    },
    {
      "path": "binding.py",
      "content": "import torch\nfrom torch.utils.cpp_extension import load_inline\nfrom pathlib import Path\n\ntry:\n    from tvm.ffi import register_func\nexcept Exception:\n    def register_func(_name):\n        def deco(fn):\n            return fn\n\n        return deco\n\n_module = None\n_spill_workspace = {}\n\n\ndef _read_cuda_source() -> str:\n    here = Path(__file__).resolve().parent\n    for candidate in (\"dsa_index.cu\", \"kernel.cu\"):\n        path = here / candidate\n        if path.exists():\n            return path.read_text()\n    raise FileNotFoundError(f\"No CUDA source found in {here} (expected dsa_index.cu or kernel.cu)\")\n\n\ndef _get_module():\n    global _module\n    if _module is None:\n        cuda_src = _read_cuda_source()\n        _module = load_inline(\n            name=\"dsa_topk_indexer_ext\",\n            cpp_sources=\"\",\n            cuda_sources=cuda_src,\n            functions=[\"dsa_topk_indexer_launch\"],\n            extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n        )\n    return _module\n\n\ndef compile_kernel() -> None:\n    _get_module()\n\n\ndef _get_spill_workspace(device: torch.device, batch: int, spill_stride: int) -> torch.Tensor:\n    dev_key = (device.type, -1 if device.index is None else device.index)\n    ws = _spill_workspace.get(dev_key)\n\n    if ws is None or ws.shape[0] < batch or ws.shape[1] < spill_stride:\n        rows = batch if ws is None else max(batch, ws.shape[0])\n        cols = spill_stride if ws is None else max(spill_stride, ws.shape[1])\n        ws = torch.empty((rows, cols), dtype=torch.float32, device=device)\n        _spill_workspace[dev_key] = ws\n\n    return ws[:batch, :spill_stride]\n\n\ndef dsa_topk_indexer(\n    q_index_fp8: torch.Tensor,\n    k_index_cache_fp8: torch.Tensor,\n    weights: torch.Tensor,\n    seq_lens: torch.Tensor,\n    block_table: torch.Tensor,\n    topk_indices: torch.Tensor,\n) -> torch.Tensor:\n    batch = int(q_index_fp8.shape[0])\n    spill_stride = int(block_table.shape[1]) * 64\n    spill_scores = _get_spill_workspace(q_index_fp8.device, batch, spill_stride)\n\n    mod = _get_module()\n    mod.dsa_topk_indexer_launch(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        spill_scores,\n        topk_indices,\n    )\n\n    return topk_indices\n\n\n@register_func(\"flashinfer.kernel\")\ndef kernel(\n    q_index_fp8,\n    k_index_cache_fp8,\n    weights,\n    seq_lens,\n    block_table,\n    topk_indices,\n):\n    return dsa_topk_indexer(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        topk_indices,\n    )\n"
    },
    {
      "path": "dsa_index.cu",
      "content": "#include <ATen/cuda/CUDAContext.h>\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_fp16.h>\n#include <cuda_fp8.h>\n#include <cuda_runtime.h>\n\n#include <cfloat>\n#include <cstdint>\n\n// ---------------------------------------------------------------------------------\n// Constants\n// ---------------------------------------------------------------------------------\nconstexpr int kNumHeads = 64;\nconstexpr int kHeadDim = 128;\nconstexpr int kPageSize = 64;\nconstexpr int kTopKDefault = 2048;\n\nconstexpr int kPayloadBytesPerToken = 128;\nconstexpr int kScaleBytesPerToken = 4;\nconstexpr int kRowBytes = 132;\nconstexpr int kPageBytes = kPageSize * kRowBytes;      // 8448\nconstexpr int kPackedFp8Bytes = kPageSize * kHeadDim;  // 8192\n\nconstexpr int kStageTokens = 64;\nconstexpr int kNumStages = 10;\nconstexpr int kProducerWarp = 0;\nconstexpr int kConsumerWarp = 1;\nconstexpr int kThreadsPerBlock = 64;  // 2 warps exactly\nconstexpr int kDesiredDynamicSmemBytes = 228 * 1024;\n\n// Safety toggles for bring-up:\n// Keep async PTX paths off until descriptor/idesc/barrier protocol is fully validated.\nconstexpr bool kEnableAsyncTma = false;\nconstexpr bool kEnableTcgenMma = false;\n\n// ---------------------------------------------------------------------------------\n// Device helpers\n// ---------------------------------------------------------------------------------\n__device__ inline uint32_t lane_id() {\n    uint32_t lane;\n    asm volatile(\"mov.u32 %0, %laneid;\" : \"=r\"(lane));\n    return lane;\n}\n\n__device__ inline uint32_t cvta_to_shared_u32(const void* p) {\n    uint32_t out;\n    asm volatile(\"cvta.to.shared.u32 %0, %1;\" : \"=r\"(out) : \"l\"(p));\n    return out;\n}\n\n__device__ inline bool elect_lane0() {\n    return lane_id() == 0;\n}\n\n__device__ inline constexpr uint64_t desc_encode(uint64_t x) {\n    return (x & 0x3'FFFFULL) >> 4ULL;\n}\n\n__device__ inline uint64_t make_desc_kmajor_128b(int smem_addr) {\n    // Shared-memory descriptor, K-major, 128B swizzle.\n    const int sbo = 8 * 128;\n    return desc_encode(static_cast<uint64_t>(smem_addr)) |\n           (desc_encode(static_cast<uint64_t>(sbo)) << 32ULL) |\n           (1ULL << 46ULL) |\n           (2ULL << 61ULL);\n}\n\ntemplate <typename T>\n__device__ inline T* smem_alloc(unsigned char*& ptr, int n) {\n    uintptr_t p = reinterpret_cast<uintptr_t>(ptr);\n    constexpr uintptr_t kAlign = alignof(T);\n    p = (p + kAlign - 1) & ~(kAlign - 1);\n    T* out = reinterpret_cast<T*>(p);\n    ptr = reinterpret_cast<unsigned char*>(out + n);\n    return out;\n}\n\n__device__ inline void mbarrier_init(int mbar_addr, int count) {\n    asm volatile(\"mbarrier.init.shared::cta.b64 [%0], %1;\" :: \"r\"(mbar_addr), \"r\"(count));\n}\n\n__device__ inline void mbarrier_arrive_expect_tx(int mbar_addr, int size_bytes) {\n    asm volatile(\"mbarrier.arrive.expect_tx.release.cta.shared::cta.b64 _, [%0], %1;\"\n                 :: \"r\"(mbar_addr), \"r\"(size_bytes)\n                 : \"memory\");\n}\n\n__device__ inline void mbarrier_wait_parity(int mbar_addr, int phase) {\n    const uint32_t ticks = 0x989680;\n    asm volatile(\n        \"{\\n\\t\"\n        \".reg .pred p;\\n\\t\"\n        \"L_WAIT_%=: \\n\\t\"\n        \"mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 p, [%0], %1, %2;\\n\\t\"\n        \"@p bra.uni L_DONE_%=;\\n\\t\"\n        \"bra.uni L_WAIT_%=;\\n\\t\"\n        \"L_DONE_%=: \\n\\t\"\n        \"}\"\n        :: \"r\"(mbar_addr), \"r\"(phase), \"r\"(ticks)\n        : \"memory\");\n}\n\n__device__ inline void tma_3d_gmem2smem(\n    int dst_smem_addr,\n    const void* tmap_ptr,\n    int x,\n    int y,\n    int z,\n    int mbar_addr,\n    uint64_t cache_policy\n) {\n    if (!elect_lane0()) {\n        return;\n    }\n    asm volatile(\n        \"cp.async.bulk.tensor.3d.shared::cta.global.mbarrier::complete_tx::bytes.cta_group::1.L2::cache_hint \"\n        \"[%0], [%1, {%2, %3, %4}], [%5], %6;\"\n        :: \"r\"(dst_smem_addr), \"l\"(tmap_ptr), \"r\"(x), \"r\"(y), \"r\"(z), \"r\"(mbar_addr), \"l\"(cache_policy)\n        : \"memory\");\n}\n\n__device__ inline void tcgen05_mma_f16_ss(\n    uint32_t tmem_c,\n    uint64_t desc_a,\n    uint64_t desc_b,\n    uint32_t idesc,\n    int accumulate\n) {\n    if (!elect_lane0()) {\n        return;\n    }\n    uint32_t mask[4] = {0, 0, 0, 0};\n    asm volatile(\n        \"{\\n\\t\"\n        \".reg .pred p;\\n\\t\"\n        \"setp.ne.b32 p, %4, 0;\\n\\t\"\n        \"tcgen05.mma.cta_group::1.kind::f16 [%0], %1, %2, %3, {%5, %6, %7, %8}, p;\\n\\t\"\n        \"}\"\n        :: \"r\"(tmem_c), \"l\"(desc_a), \"l\"(desc_b), \"r\"(idesc), \"r\"(accumulate),\n           \"r\"(mask[0]), \"r\"(mask[1]), \"r\"(mask[2]), \"r\"(mask[3]));\n}\n\n__device__ inline void tcgen05_commit(int mbar_addr) {\n    if (!elect_lane0()) {\n        return;\n    }\n    asm volatile(\"tcgen05.commit.cta_group::1.mbarrier::arrive::one.shared::cluster.b64 [%0];\"\n                 :: \"r\"(mbar_addr)\n                 : \"memory\");\n}\n\n__device__ inline void tcgen05_wait_ld() {\n    asm volatile(\"tcgen05.wait::ld.sync.aligned;\" ::: \"memory\");\n}\n\n__device__ inline void tcgen05_fence_before_thread_sync() {\n    asm volatile(\"tcgen05.fence::before_thread_sync;\" ::: \"memory\");\n}\n\n__device__ inline float fp8_byte_to_float(uint8_t x) {\n    __nv_fp8_e4m3 v;\n    v.__x = x;\n    return static_cast<float>(v);\n}\n\n__device__ inline uint32_t read_u32_le(const uint8_t* p) {\n    return static_cast<uint32_t>(p[0]) |\n           (static_cast<uint32_t>(p[1]) << 8) |\n           (static_cast<uint32_t>(p[2]) << 16) |\n           (static_cast<uint32_t>(p[3]) << 24);\n}\n\n__device__ inline void insert_topk_desc(float score, int idx, float* scores, int* ids, int& count, int k) {\n    if (k <= 0 || idx < 0) {\n        return;\n    }\n\n    if (count < k) {\n        int pos = count;\n        while (pos > 0 && score > scores[pos - 1]) {\n            scores[pos] = scores[pos - 1];\n            ids[pos] = ids[pos - 1];\n            --pos;\n        }\n        scores[pos] = score;\n        ids[pos] = idx;\n        ++count;\n        return;\n    }\n\n    if (score <= scores[k - 1]) {\n        return;\n    }\n\n    int pos = k - 1;\n    while (pos > 0 && score > scores[pos - 1]) {\n        scores[pos] = scores[pos - 1];\n        ids[pos] = ids[pos - 1];\n        --pos;\n    }\n    scores[pos] = score;\n    ids[pos] = idx;\n}\n\n// ---------------------------------------------------------------------------------\n// Kernel: warp-specialized (2 warps)\n// ---------------------------------------------------------------------------------\n__global__ void dsa_topk_indexer_kernel(\n    const uint8_t* q_index_bytes,   // [B,64,128], elem_size in {1,2,4}\n    const int8_t* k_index_cache,    // [num_pages,64,1,132], deep_gemm packed\n    const float* weights,           // [B,64]\n    const int* seq_lens,            // [B]\n    const int* block_table,         // [B,max_num_pages]\n    const void* k_fp8_tmap,         // CUtensorMap in global memory\n    float* spill_scores,            // [B,spill_stride]\n    int* topk_indices,              // [B,topk]\n    int batch_size,\n    int num_pages,\n    int max_num_pages,\n    int spill_stride,\n    int topk,\n    int q_elem_size,\n    int dynamic_smem_bytes\n) {\n    const int b = blockIdx.x;\n    const int tid = threadIdx.x;\n    const int warp_id = tid >> 5;\n    const int lane = tid & 31;\n\n    if (b >= batch_size) {\n        return;\n    }\n\n    // Exactly two warps by contract.\n    if (warp_id > kConsumerWarp) {\n        return;\n    }\n\n    const int* block_table_b = block_table + static_cast<int64_t>(b) * max_num_pages;\n    const float* weights_b = weights + static_cast<int64_t>(b) * kNumHeads;\n    int* out_b = topk_indices + static_cast<int64_t>(b) * topk;\n\n    const int max_seq_by_pages = max_num_pages * kPageSize;\n    const int max_seq_by_spill = spill_stride;\n    int seq_len = seq_lens[b];\n    if (seq_len < 0) seq_len = 0;\n    if (seq_len > max_seq_by_pages) seq_len = max_seq_by_pages;\n    if (seq_len > max_seq_by_spill) seq_len = max_seq_by_spill;\n\n    for (int i = tid; i < topk; i += blockDim.x) {\n        out_b[i] = -1;\n    }\n    __syncthreads();\n\n    if (seq_len == 0 || topk <= 0) {\n        return;\n    }\n\n    extern __shared__ unsigned char smem_raw[];\n    unsigned char* smem_ptr = smem_raw;\n\n    // Producer-owned staged K buffers.\n    uint8_t* k_stage_payload = smem_alloc<uint8_t>(smem_ptr, kNumStages * kStageTokens * kPayloadBytesPerToken);\n    float* k_stage_scale = smem_alloc<float>(smem_ptr, kNumStages * kStageTokens);\n\n    // Shared Q/weights and metadata.\n    uint8_t* q_stage = smem_alloc<uint8_t>(smem_ptr, kNumHeads * kHeadDim);\n    float* w_stage = smem_alloc<float>(smem_ptr, kNumHeads);\n\n    int* stage_page_idx = smem_alloc<int>(smem_ptr, kNumStages);\n    int* stage_valid_tokens = smem_alloc<int>(smem_ptr, kNumStages);\n    int* stage_phase = smem_alloc<int>(smem_ptr, kNumStages);\n    uint64_t* stage_mbar = smem_alloc<uint64_t>(smem_ptr, kNumStages);\n\n    float* topk_scores = smem_alloc<float>(smem_ptr, topk);\n    int* topk_ids = smem_alloc<int>(smem_ptr, topk);\n\n    const int used_bytes = static_cast<int>(smem_ptr - smem_raw);\n    const int seq_cap_smem = (dynamic_smem_bytes > used_bytes)\n                                 ? (dynamic_smem_bytes - used_bytes) / static_cast<int>(sizeof(float))\n                                 : 0;\n    float* final_scores_smem = reinterpret_cast<float*>(smem_ptr);\n    const bool use_smem_scores = (seq_len <= seq_cap_smem);\n    float* spill_b = spill_scores + static_cast<int64_t>(b) * spill_stride;\n\n    // Initialize mbarriers + phase.\n    if (kEnableAsyncTma && warp_id == kProducerWarp && lane < kNumStages) {\n        const int mbar_addr = cvta_to_shared_u32(&stage_mbar[lane]);\n        mbarrier_init(mbar_addr, 1);\n        stage_phase[lane] = 0;\n    }\n    __syncthreads();\n\n    // Stage Q and weights once.\n    if (warp_id == kConsumerWarp) {\n        for (int idx = lane; idx < kNumHeads; idx += 32) {\n            w_stage[idx] = weights_b[idx];\n        }\n        if (q_elem_size == 1) {\n            for (int idx = lane; idx < kNumHeads * kHeadDim; idx += 32) {\n                const int64_t q_off = static_cast<int64_t>(b) * kNumHeads * kHeadDim + idx;\n                q_stage[idx] = q_index_bytes[q_off];\n            }\n        }\n    }\n    __syncthreads();\n\n    const uint8_t* k_bytes = reinterpret_cast<const uint8_t*>(k_index_cache);\n    const int num_tiles = (seq_len + kStageTokens - 1) / kStageTokens;\n\n    for (int tile_base = 0; tile_base < num_tiles; tile_base += kNumStages) {\n        // ---------------- Producer warp: all loads ----------------\n        if (warp_id == kProducerWarp) {\n            for (int s = 0; s < kNumStages; ++s) {\n                const int tile_id = tile_base + s;\n                const int tile_seq_start = tile_id * kStageTokens;\n                const int remain = seq_len - tile_seq_start;\n                const int valid_tokens = (remain > 0) ? ((remain < kStageTokens) ? remain : kStageTokens) : 0;\n\n                if (lane == 0) {\n                    stage_valid_tokens[s] = valid_tokens;\n                    int page_idx = -1;\n                    if (tile_id >= 0 && tile_id < max_num_pages) {\n                        page_idx = block_table_b[tile_id];\n                    }\n                    stage_page_idx[s] = page_idx;\n                }\n                __syncwarp();\n\n                const int page_idx = stage_page_idx[s];\n                uint8_t* stage_payload = k_stage_payload + s * kStageTokens * kPayloadBytesPerToken;\n                float* stage_scale = k_stage_scale + s * kStageTokens;\n\n                // 1) Issue TMA payload load (best-effort tensor-map path).\n                if (kEnableAsyncTma && k_fp8_tmap != nullptr && page_idx >= 0 && page_idx < num_pages && lane == 0) {\n                    const int dst = cvta_to_shared_u32(stage_payload);\n                    const int mbar_addr = cvta_to_shared_u32(&stage_mbar[s]);\n                    mbarrier_arrive_expect_tx(mbar_addr, kStageTokens * kPayloadBytesPerToken);\n                    tma_3d_gmem2smem(dst, k_fp8_tmap, 0, 0, page_idx, mbar_addr, 0ULL);\n                }\n\n                // 2) Manual fallback copy for payload + scales (ground-truth data path).\n                if (page_idx >= 0 && page_idx < num_pages && valid_tokens > 0) {\n                    const uint8_t* page_ptr = k_bytes + static_cast<int64_t>(page_idx) * kPageBytes;\n\n                    for (int i = lane; i < valid_tokens * kPayloadBytesPerToken; i += 32) {\n                        const int tok = i / kPayloadBytesPerToken;\n                        const int d = i % kPayloadBytesPerToken;\n                        stage_payload[i] = page_ptr[tok * kPayloadBytesPerToken + d];\n                    }\n\n                    for (int tok = lane; tok < valid_tokens; tok += 32) {\n                        const int scale_off = kPackedFp8Bytes + tok * kScaleBytesPerToken;\n                        const uint32_t bits = read_u32_le(page_ptr + scale_off);\n                        stage_scale[tok] = __uint_as_float(bits);\n                    }\n                }\n\n                for (int i = lane + valid_tokens * kPayloadBytesPerToken;\n                     i < kStageTokens * kPayloadBytesPerToken;\n                     i += 32) {\n                    stage_payload[i] = 0;\n                }\n                for (int tok = lane + valid_tokens; tok < kStageTokens; tok += 32) {\n                    stage_scale[tok] = 0.0f;\n                }\n\n                __syncwarp();\n            }\n        }\n\n        __syncthreads();\n\n        // ---------------- Consumer warp: all compute ----------------\n        if (warp_id == kConsumerWarp) {\n            // tcgen path setup (placeholder descriptors / idesc; kept explicit by design).\n            const uint32_t i_desc = (1U << 7U)   // placeholder atype field\n                                  | (1U << 10U)  // placeholder btype field\n                                  | ((uint32_t)kStageTokens >> 3U << 17U)\n                                  | ((uint32_t)128 >> 7U << 27U);\n\n            for (int s = 0; s < kNumStages; ++s) {\n                const int tile_id = tile_base + s;\n                if (tile_id >= num_tiles) {\n                    continue;\n                }\n\n                const int valid_tokens = stage_valid_tokens[s];\n                const int page_idx = stage_page_idx[s];\n                if (valid_tokens <= 0 || page_idx < 0 || page_idx >= num_pages) {\n                    continue;\n                }\n\n                // Wait for producer-issued TMA transaction for this stage.\n                if (kEnableAsyncTma && k_fp8_tmap != nullptr && lane == 0) {\n                    const int mbar_addr = cvta_to_shared_u32(&stage_mbar[s]);\n                    mbarrier_wait_parity(mbar_addr, stage_phase[s]);\n                    stage_phase[s] ^= 1;\n                }\n                __syncwarp();\n\n                // Explicit tcgen issue point in consumer warp.\n                if (kEnableTcgenMma && lane == 0) {\n                    const int q_addr = cvta_to_shared_u32(q_stage);\n                    const int k_addr = cvta_to_shared_u32(k_stage_payload + s * kStageTokens * kPayloadBytesPerToken);\n                    const uint64_t desc_a = make_desc_kmajor_128b(q_addr);\n                    const uint64_t desc_b = make_desc_kmajor_128b(k_addr);\n                    tcgen05_mma_f16_ss(/*tmem_c=*/0, desc_a, desc_b, i_desc, /*accumulate=*/0);\n                    tcgen05_commit(cvta_to_shared_u32(&stage_mbar[s]));\n                    tcgen05_wait_ld();\n                    tcgen05_fence_before_thread_sync();\n                }\n                __syncwarp();\n\n                // Scalar reduce path (kept as correctness anchor while tcgen path is refined).\n                const uint8_t* stage_payload = k_stage_payload + s * kStageTokens * kPayloadBytesPerToken;\n                const float* stage_scale = k_stage_scale + s * kStageTokens;\n\n                for (int t = lane; t < valid_tokens; t += 32) {\n                    const int seq_token = tile_id * kStageTokens + t;\n                    float reduced = 0.0f;\n                    const uint8_t* row = stage_payload + t * kPayloadBytesPerToken;\n                    const float scale = stage_scale[t];\n\n                    for (int h = 0; h < kNumHeads; ++h) {\n                        float dot = 0.0f;\n                        for (int d = 0; d < kHeadDim; ++d) {\n                            const int64_t q_off = (static_cast<int64_t>(b) * kNumHeads + h) * kHeadDim + d;\n                            float qv;\n                            if (q_elem_size == 1) {\n                                qv = fp8_byte_to_float(q_stage[h * kHeadDim + d]);\n                            } else if (q_elem_size == 2) {\n                                const __half* q_half = reinterpret_cast<const __half*>(q_index_bytes);\n                                qv = __half2float(q_half[q_off]);\n                            } else {\n                                const float* q_f32 = reinterpret_cast<const float*>(q_index_bytes);\n                                qv = q_f32[q_off];\n                            }\n                            const float kv = fp8_byte_to_float(row[d]) * scale;\n                            dot += qv * kv;\n                        }\n                        if (dot > 0.0f) {\n                            reduced += dot * w_stage[h];\n                        }\n                    }\n\n                    if (use_smem_scores) {\n                        final_scores_smem[seq_token] = reduced;\n                    } else {\n                        spill_b[seq_token] = reduced;\n                    }\n                }\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // Final exact top-k in consumer lane 0.\n    if (warp_id == kConsumerWarp && lane == 0) {\n        const int actual_topk = (topk < seq_len) ? topk : seq_len;\n        for (int i = 0; i < actual_topk; ++i) {\n            topk_scores[i] = -FLT_MAX;\n            topk_ids[i] = -1;\n        }\n\n        int topk_count = 0;\n        for (int seq_token = 0; seq_token < seq_len; ++seq_token) {\n            const float score = use_smem_scores ? final_scores_smem[seq_token] : spill_b[seq_token];\n            const int page_slot = seq_token / kPageSize;\n            const int offset = seq_token % kPageSize;\n\n            int page_idx = -1;\n            if (page_slot >= 0 && page_slot < max_num_pages) {\n                page_idx = block_table_b[page_slot];\n            }\n            if (page_idx < 0 || page_idx >= num_pages) {\n                continue;\n            }\n\n            const int token_idx = page_idx * kPageSize + offset;\n            insert_topk_desc(score, token_idx, topk_scores, topk_ids, topk_count, actual_topk);\n        }\n\n        for (int i = 0; i < actual_topk; ++i) {\n            out_b[i] = (i < topk_count) ? topk_ids[i] : -1;\n        }\n    }\n}\n\n// ---------------------------------------------------------------------------------\n// Host-side tensor-map encode helpers\n// ---------------------------------------------------------------------------------\nstatic void* g_k_fp8_tmap_dev = nullptr;\n\nstatic void ensure_k_fp8_tmap_on_device(const int8_t* k_index_cache_ptr, int num_pages) {\n    if (g_k_fp8_tmap_dev == nullptr) {\n        cudaMalloc(&g_k_fp8_tmap_dev, sizeof(CUtensorMap));\n    }\n\n    CUtensorMap k_fp8_tmap{};\n    cuInit(0);\n\n    constexpr cuuint32_t rank = 3;\n    const cuuint64_t globalDim[rank] = {\n        static_cast<cuuint64_t>(kPayloadBytesPerToken),\n        static_cast<cuuint64_t>(kPageSize),\n        static_cast<cuuint64_t>(num_pages),\n    };\n    const cuuint64_t globalStrides[rank - 1] = {\n        static_cast<cuuint64_t>(kPayloadBytesPerToken),\n        static_cast<cuuint64_t>(kPageBytes),\n    };\n    const cuuint32_t boxDim[rank] = {\n        static_cast<cuuint32_t>(kPayloadBytesPerToken),\n        static_cast<cuuint32_t>(kPageSize),\n        1U,\n    };\n    const cuuint32_t elementStrides[rank] = {1U, 1U, 1U};\n\n    const CUresult st = cuTensorMapEncodeTiled(\n        &k_fp8_tmap,\n        CU_TENSOR_MAP_DATA_TYPE_UINT8,\n        rank,\n        const_cast<int8_t*>(k_index_cache_ptr),\n        globalDim,\n        globalStrides,\n        boxDim,\n        elementStrides,\n        CU_TENSOR_MAP_INTERLEAVE_NONE,\n        CU_TENSOR_MAP_SWIZZLE_NONE,\n        CU_TENSOR_MAP_L2_PROMOTION_L2_128B,\n        CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n    );\n\n    if (st == CUDA_SUCCESS) {\n        cudaMemcpy(g_k_fp8_tmap_dev, &k_fp8_tmap, sizeof(CUtensorMap), cudaMemcpyHostToDevice);\n    } else {\n        // Best-effort mode: keep pointer null to skip TMA path if encode fails.\n        g_k_fp8_tmap_dev = nullptr;\n    }\n}\n\n// ---------------------------------------------------------------------------------\n// Launch entry\n// ---------------------------------------------------------------------------------\nvoid dsa_topk_indexer_launch(\n    torch::Tensor q_index_fp8,\n    torch::Tensor k_index_cache_fp8,\n    torch::Tensor weights,\n    torch::Tensor seq_lens,\n    torch::Tensor block_table,\n    torch::Tensor spill_scores,\n    torch::Tensor topk_indices\n) {\n    const int batch_size = static_cast<int>(q_index_fp8.size(0));\n    const int num_pages = static_cast<int>(k_index_cache_fp8.size(0));\n    const int max_num_pages = static_cast<int>(block_table.size(1));\n    const int spill_stride = static_cast<int>(spill_scores.size(1));\n    const int topk = static_cast<int>(topk_indices.size(1));\n    const int q_elem_size = static_cast<int>(q_index_fp8.element_size());\n\n    if (batch_size == 0 || topk == 0) {\n        return;\n    }\n\n    ensure_k_fp8_tmap_on_device(reinterpret_cast<const int8_t*>(k_index_cache_fp8.data_ptr()), num_pages);\n\n    int device = -1;\n    cudaGetDevice(&device);\n\n    int max_optin_smem = 0;\n    cudaDeviceGetAttribute(&max_optin_smem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    int dynamic_smem_bytes = kDesiredDynamicSmemBytes;\n    if (max_optin_smem > 0 && dynamic_smem_bytes > max_optin_smem) {\n        dynamic_smem_bytes = max_optin_smem;\n    }\n\n    cudaFuncSetAttribute(\n        dsa_topk_indexer_kernel,\n        cudaFuncAttributeMaxDynamicSharedMemorySize,\n        dynamic_smem_bytes\n    );\n    cudaFuncSetAttribute(\n        dsa_topk_indexer_kernel,\n        cudaFuncAttributePreferredSharedMemoryCarveout,\n        100\n    );\n\n    const int blocks = batch_size;\n    dsa_topk_indexer_kernel<<<blocks, kThreadsPerBlock, dynamic_smem_bytes>>>(\n        reinterpret_cast<const uint8_t*>(q_index_fp8.data_ptr()),\n        reinterpret_cast<const int8_t*>(k_index_cache_fp8.data_ptr()),\n        reinterpret_cast<const float*>(weights.data_ptr()),\n        reinterpret_cast<const int*>(seq_lens.data_ptr()),\n        reinterpret_cast<const int*>(block_table.data_ptr()),\n        g_k_fp8_tmap_dev,\n        reinterpret_cast<float*>(spill_scores.data_ptr()),\n        reinterpret_cast<int*>(topk_indices.data_ptr()),\n        batch_size,\n        num_pages,\n        max_num_pages,\n        spill_stride,\n        topk,\n        q_elem_size,\n        dynamic_smem_bytes\n    );\n}\n"
    },
    {
      "path": "dsa_index.py",
      "content": "import torch\nfrom torch.utils.cpp_extension import load_inline\nfrom pathlib import Path\n\ntry:\n    from tvm.ffi import register_func\nexcept Exception:\n    def register_func(_name):\n        def deco(fn):\n            return fn\n\n        return deco\n\n_module = None\n_spill_workspace = {}\n\n\ndef _read_cuda_source() -> str:\n    here = Path(__file__).resolve().parent\n    for candidate in (\"dsa_index.cu\", \"kernel.cu\"):\n        path = here / candidate\n        if path.exists():\n            return path.read_text()\n    raise FileNotFoundError(f\"No CUDA source found in {here} (expected dsa_index.cu or kernel.cu)\")\n\n\ndef _get_module():\n    global _module\n    if _module is None:\n        cuda_src = _read_cuda_source()\n        _module = load_inline(\n            name=\"dsa_topk_indexer_ext\",\n            cpp_sources=\"\",\n            cuda_sources=cuda_src,\n            functions=[\"dsa_topk_indexer_launch\"],\n            extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n        )\n    return _module\n\n\ndef compile_kernel() -> None:\n    _get_module()\n\n\ndef _get_spill_workspace(device: torch.device, batch: int, spill_stride: int) -> torch.Tensor:\n    dev_key = (device.type, -1 if device.index is None else device.index)\n    ws = _spill_workspace.get(dev_key)\n\n    if ws is None or ws.shape[0] < batch or ws.shape[1] < spill_stride:\n        rows = batch if ws is None else max(batch, ws.shape[0])\n        cols = spill_stride if ws is None else max(spill_stride, ws.shape[1])\n        ws = torch.empty((rows, cols), dtype=torch.float32, device=device)\n        _spill_workspace[dev_key] = ws\n\n    return ws[:batch, :spill_stride]\n\n\ndef dsa_topk_indexer(\n    q_index_fp8: torch.Tensor,\n    k_index_cache_fp8: torch.Tensor,\n    weights: torch.Tensor,\n    seq_lens: torch.Tensor,\n    block_table: torch.Tensor,\n    topk_indices: torch.Tensor,\n) -> torch.Tensor:\n    batch = int(q_index_fp8.shape[0])\n    spill_stride = int(block_table.shape[1]) * 64\n    spill_scores = _get_spill_workspace(q_index_fp8.device, batch, spill_stride)\n\n    mod = _get_module()\n    mod.dsa_topk_indexer_launch(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        spill_scores,\n        topk_indices,\n    )\n\n    return topk_indices\n\n\n@register_func(\"flashinfer.kernel\")\ndef kernel(\n    q_index_fp8,\n    k_index_cache_fp8,\n    weights,\n    seq_lens,\n    block_table,\n    topk_indices,\n):\n    return dsa_topk_indexer(\n        q_index_fp8,\n        k_index_cache_fp8,\n        weights,\n        seq_lens,\n        block_table,\n        topk_indices,\n    )\n"
    },
    {
      "path": "kernel.cu",
      "content": "#include <ATen/cuda/CUDAContext.h>\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_fp16.h>\n#include <cuda_fp8.h>\n#include <cuda_runtime.h>\n\n#include <cfloat>\n#include <cstdint>\n\n// ---------------------------------------------------------------------------------\n// Constants\n// ---------------------------------------------------------------------------------\nconstexpr int kNumHeads = 64;\nconstexpr int kHeadDim = 128;\nconstexpr int kPageSize = 64;\nconstexpr int kTopKDefault = 2048;\n\nconstexpr int kPayloadBytesPerToken = 128;\nconstexpr int kScaleBytesPerToken = 4;\nconstexpr int kRowBytes = 132;\nconstexpr int kPageBytes = kPageSize * kRowBytes;      // 8448\nconstexpr int kPackedFp8Bytes = kPageSize * kHeadDim;  // 8192\n\nconstexpr int kStageTokens = 64;\nconstexpr int kNumStages = 10;\nconstexpr int kProducerWarp = 0;\nconstexpr int kConsumerWarp = 1;\nconstexpr int kThreadsPerBlock = 64;  // 2 warps exactly\nconstexpr int kDesiredDynamicSmemBytes = 228 * 1024;\n\n// Safety toggles for bring-up:\n// Keep async PTX paths off until descriptor/idesc/barrier protocol is fully validated.\nconstexpr bool kEnableAsyncTma = false;\nconstexpr bool kEnableTcgenMma = false;\n\n// ---------------------------------------------------------------------------------\n// Device helpers\n// ---------------------------------------------------------------------------------\n__device__ inline uint32_t lane_id() {\n    uint32_t lane;\n    asm volatile(\"mov.u32 %0, %laneid;\" : \"=r\"(lane));\n    return lane;\n}\n\n__device__ inline uint32_t cvta_to_shared_u32(const void* p) {\n    uint32_t out;\n    asm volatile(\"cvta.to.shared.u32 %0, %1;\" : \"=r\"(out) : \"l\"(p));\n    return out;\n}\n\n__device__ inline bool elect_lane0() {\n    return lane_id() == 0;\n}\n\n__device__ inline constexpr uint64_t desc_encode(uint64_t x) {\n    return (x & 0x3'FFFFULL) >> 4ULL;\n}\n\n__device__ inline uint64_t make_desc_kmajor_128b(int smem_addr) {\n    // Shared-memory descriptor, K-major, 128B swizzle.\n    const int sbo = 8 * 128;\n    return desc_encode(static_cast<uint64_t>(smem_addr)) |\n           (desc_encode(static_cast<uint64_t>(sbo)) << 32ULL) |\n           (1ULL << 46ULL) |\n           (2ULL << 61ULL);\n}\n\ntemplate <typename T>\n__device__ inline T* smem_alloc(unsigned char*& ptr, int n) {\n    uintptr_t p = reinterpret_cast<uintptr_t>(ptr);\n    constexpr uintptr_t kAlign = alignof(T);\n    p = (p + kAlign - 1) & ~(kAlign - 1);\n    T* out = reinterpret_cast<T*>(p);\n    ptr = reinterpret_cast<unsigned char*>(out + n);\n    return out;\n}\n\n__device__ inline void mbarrier_init(int mbar_addr, int count) {\n    asm volatile(\"mbarrier.init.shared::cta.b64 [%0], %1;\" :: \"r\"(mbar_addr), \"r\"(count));\n}\n\n__device__ inline void mbarrier_arrive_expect_tx(int mbar_addr, int size_bytes) {\n    asm volatile(\"mbarrier.arrive.expect_tx.release.cta.shared::cta.b64 _, [%0], %1;\"\n                 :: \"r\"(mbar_addr), \"r\"(size_bytes)\n                 : \"memory\");\n}\n\n__device__ inline void mbarrier_wait_parity(int mbar_addr, int phase) {\n    const uint32_t ticks = 0x989680;\n    asm volatile(\n        \"{\\n\\t\"\n        \".reg .pred p;\\n\\t\"\n        \"L_WAIT_%=: \\n\\t\"\n        \"mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 p, [%0], %1, %2;\\n\\t\"\n        \"@p bra.uni L_DONE_%=;\\n\\t\"\n        \"bra.uni L_WAIT_%=;\\n\\t\"\n        \"L_DONE_%=: \\n\\t\"\n        \"}\"\n        :: \"r\"(mbar_addr), \"r\"(phase), \"r\"(ticks)\n        : \"memory\");\n}\n\n__device__ inline void tma_3d_gmem2smem(\n    int dst_smem_addr,\n    const void* tmap_ptr,\n    int x,\n    int y,\n    int z,\n    int mbar_addr,\n    uint64_t cache_policy\n) {\n    if (!elect_lane0()) {\n        return;\n    }\n    asm volatile(\n        \"cp.async.bulk.tensor.3d.shared::cta.global.mbarrier::complete_tx::bytes.cta_group::1.L2::cache_hint \"\n        \"[%0], [%1, {%2, %3, %4}], [%5], %6;\"\n        :: \"r\"(dst_smem_addr), \"l\"(tmap_ptr), \"r\"(x), \"r\"(y), \"r\"(z), \"r\"(mbar_addr), \"l\"(cache_policy)\n        : \"memory\");\n}\n\n__device__ inline void tcgen05_mma_f16_ss(\n    uint32_t tmem_c,\n    uint64_t desc_a,\n    uint64_t desc_b,\n    uint32_t idesc,\n    int accumulate\n) {\n    if (!elect_lane0()) {\n        return;\n    }\n    uint32_t mask[4] = {0, 0, 0, 0};\n    asm volatile(\n        \"{\\n\\t\"\n        \".reg .pred p;\\n\\t\"\n        \"setp.ne.b32 p, %4, 0;\\n\\t\"\n        \"tcgen05.mma.cta_group::1.kind::f16 [%0], %1, %2, %3, {%5, %6, %7, %8}, p;\\n\\t\"\n        \"}\"\n        :: \"r\"(tmem_c), \"l\"(desc_a), \"l\"(desc_b), \"r\"(idesc), \"r\"(accumulate),\n           \"r\"(mask[0]), \"r\"(mask[1]), \"r\"(mask[2]), \"r\"(mask[3]));\n}\n\n__device__ inline void tcgen05_commit(int mbar_addr) {\n    if (!elect_lane0()) {\n        return;\n    }\n    asm volatile(\"tcgen05.commit.cta_group::1.mbarrier::arrive::one.shared::cluster.b64 [%0];\"\n                 :: \"r\"(mbar_addr)\n                 : \"memory\");\n}\n\n__device__ inline void tcgen05_wait_ld() {\n    asm volatile(\"tcgen05.wait::ld.sync.aligned;\" ::: \"memory\");\n}\n\n__device__ inline void tcgen05_fence_before_thread_sync() {\n    asm volatile(\"tcgen05.fence::before_thread_sync;\" ::: \"memory\");\n}\n\n__device__ inline float fp8_byte_to_float(uint8_t x) {\n    __nv_fp8_e4m3 v;\n    v.__x = x;\n    return static_cast<float>(v);\n}\n\n__device__ inline uint32_t read_u32_le(const uint8_t* p) {\n    return static_cast<uint32_t>(p[0]) |\n           (static_cast<uint32_t>(p[1]) << 8) |\n           (static_cast<uint32_t>(p[2]) << 16) |\n           (static_cast<uint32_t>(p[3]) << 24);\n}\n\n__device__ inline void insert_topk_desc(float score, int idx, float* scores, int* ids, int& count, int k) {\n    if (k <= 0 || idx < 0) {\n        return;\n    }\n\n    if (count < k) {\n        int pos = count;\n        while (pos > 0 && score > scores[pos - 1]) {\n            scores[pos] = scores[pos - 1];\n            ids[pos] = ids[pos - 1];\n            --pos;\n        }\n        scores[pos] = score;\n        ids[pos] = idx;\n        ++count;\n        return;\n    }\n\n    if (score <= scores[k - 1]) {\n        return;\n    }\n\n    int pos = k - 1;\n    while (pos > 0 && score > scores[pos - 1]) {\n        scores[pos] = scores[pos - 1];\n        ids[pos] = ids[pos - 1];\n        --pos;\n    }\n    scores[pos] = score;\n    ids[pos] = idx;\n}\n\n// ---------------------------------------------------------------------------------\n// Kernel: warp-specialized (2 warps)\n// ---------------------------------------------------------------------------------\n__global__ void dsa_topk_indexer_kernel(\n    const uint8_t* q_index_bytes,   // [B,64,128], elem_size in {1,2,4}\n    const int8_t* k_index_cache,    // [num_pages,64,1,132], deep_gemm packed\n    const float* weights,           // [B,64]\n    const int* seq_lens,            // [B]\n    const int* block_table,         // [B,max_num_pages]\n    const void* k_fp8_tmap,         // CUtensorMap in global memory\n    float* spill_scores,            // [B,spill_stride]\n    int* topk_indices,              // [B,topk]\n    int batch_size,\n    int num_pages,\n    int max_num_pages,\n    int spill_stride,\n    int topk,\n    int q_elem_size,\n    int dynamic_smem_bytes\n) {\n    const int b = blockIdx.x;\n    const int tid = threadIdx.x;\n    const int warp_id = tid >> 5;\n    const int lane = tid & 31;\n\n    if (b >= batch_size) {\n        return;\n    }\n\n    // Exactly two warps by contract.\n    if (warp_id > kConsumerWarp) {\n        return;\n    }\n\n    const int* block_table_b = block_table + static_cast<int64_t>(b) * max_num_pages;\n    const float* weights_b = weights + static_cast<int64_t>(b) * kNumHeads;\n    int* out_b = topk_indices + static_cast<int64_t>(b) * topk;\n\n    const int max_seq_by_pages = max_num_pages * kPageSize;\n    const int max_seq_by_spill = spill_stride;\n    int seq_len = seq_lens[b];\n    if (seq_len < 0) seq_len = 0;\n    if (seq_len > max_seq_by_pages) seq_len = max_seq_by_pages;\n    if (seq_len > max_seq_by_spill) seq_len = max_seq_by_spill;\n\n    for (int i = tid; i < topk; i += blockDim.x) {\n        out_b[i] = -1;\n    }\n    __syncthreads();\n\n    if (seq_len == 0 || topk <= 0) {\n        return;\n    }\n\n    extern __shared__ unsigned char smem_raw[];\n    unsigned char* smem_ptr = smem_raw;\n\n    // Producer-owned staged K buffers.\n    uint8_t* k_stage_payload = smem_alloc<uint8_t>(smem_ptr, kNumStages * kStageTokens * kPayloadBytesPerToken);\n    float* k_stage_scale = smem_alloc<float>(smem_ptr, kNumStages * kStageTokens);\n\n    // Shared Q/weights and metadata.\n    uint8_t* q_stage = smem_alloc<uint8_t>(smem_ptr, kNumHeads * kHeadDim);\n    float* w_stage = smem_alloc<float>(smem_ptr, kNumHeads);\n\n    int* stage_page_idx = smem_alloc<int>(smem_ptr, kNumStages);\n    int* stage_valid_tokens = smem_alloc<int>(smem_ptr, kNumStages);\n    int* stage_phase = smem_alloc<int>(smem_ptr, kNumStages);\n    uint64_t* stage_mbar = smem_alloc<uint64_t>(smem_ptr, kNumStages);\n\n    float* topk_scores = smem_alloc<float>(smem_ptr, topk);\n    int* topk_ids = smem_alloc<int>(smem_ptr, topk);\n\n    const int used_bytes = static_cast<int>(smem_ptr - smem_raw);\n    const int seq_cap_smem = (dynamic_smem_bytes > used_bytes)\n                                 ? (dynamic_smem_bytes - used_bytes) / static_cast<int>(sizeof(float))\n                                 : 0;\n    float* final_scores_smem = reinterpret_cast<float*>(smem_ptr);\n    const bool use_smem_scores = (seq_len <= seq_cap_smem);\n    float* spill_b = spill_scores + static_cast<int64_t>(b) * spill_stride;\n\n    // Initialize mbarriers + phase.\n    if (kEnableAsyncTma && warp_id == kProducerWarp && lane < kNumStages) {\n        const int mbar_addr = cvta_to_shared_u32(&stage_mbar[lane]);\n        mbarrier_init(mbar_addr, 1);\n        stage_phase[lane] = 0;\n    }\n    __syncthreads();\n\n    // Stage Q and weights once.\n    if (warp_id == kConsumerWarp) {\n        for (int idx = lane; idx < kNumHeads; idx += 32) {\n            w_stage[idx] = weights_b[idx];\n        }\n        if (q_elem_size == 1) {\n            for (int idx = lane; idx < kNumHeads * kHeadDim; idx += 32) {\n                const int64_t q_off = static_cast<int64_t>(b) * kNumHeads * kHeadDim + idx;\n                q_stage[idx] = q_index_bytes[q_off];\n            }\n        }\n    }\n    __syncthreads();\n\n    const uint8_t* k_bytes = reinterpret_cast<const uint8_t*>(k_index_cache);\n    const int num_tiles = (seq_len + kStageTokens - 1) / kStageTokens;\n\n    for (int tile_base = 0; tile_base < num_tiles; tile_base += kNumStages) {\n        // ---------------- Producer warp: all loads ----------------\n        if (warp_id == kProducerWarp) {\n            for (int s = 0; s < kNumStages; ++s) {\n                const int tile_id = tile_base + s;\n                const int tile_seq_start = tile_id * kStageTokens;\n                const int remain = seq_len - tile_seq_start;\n                const int valid_tokens = (remain > 0) ? ((remain < kStageTokens) ? remain : kStageTokens) : 0;\n\n                if (lane == 0) {\n                    stage_valid_tokens[s] = valid_tokens;\n                    int page_idx = -1;\n                    if (tile_id >= 0 && tile_id < max_num_pages) {\n                        page_idx = block_table_b[tile_id];\n                    }\n                    stage_page_idx[s] = page_idx;\n                }\n                __syncwarp();\n\n                const int page_idx = stage_page_idx[s];\n                uint8_t* stage_payload = k_stage_payload + s * kStageTokens * kPayloadBytesPerToken;\n                float* stage_scale = k_stage_scale + s * kStageTokens;\n\n                // 1) Issue TMA payload load (best-effort tensor-map path).\n                if (kEnableAsyncTma && k_fp8_tmap != nullptr && page_idx >= 0 && page_idx < num_pages && lane == 0) {\n                    const int dst = cvta_to_shared_u32(stage_payload);\n                    const int mbar_addr = cvta_to_shared_u32(&stage_mbar[s]);\n                    mbarrier_arrive_expect_tx(mbar_addr, kStageTokens * kPayloadBytesPerToken);\n                    tma_3d_gmem2smem(dst, k_fp8_tmap, 0, 0, page_idx, mbar_addr, 0ULL);\n                }\n\n                // 2) Manual fallback copy for payload + scales (ground-truth data path).\n                if (page_idx >= 0 && page_idx < num_pages && valid_tokens > 0) {\n                    const uint8_t* page_ptr = k_bytes + static_cast<int64_t>(page_idx) * kPageBytes;\n\n                    for (int i = lane; i < valid_tokens * kPayloadBytesPerToken; i += 32) {\n                        const int tok = i / kPayloadBytesPerToken;\n                        const int d = i % kPayloadBytesPerToken;\n                        stage_payload[i] = page_ptr[tok * kPayloadBytesPerToken + d];\n                    }\n\n                    for (int tok = lane; tok < valid_tokens; tok += 32) {\n                        const int scale_off = kPackedFp8Bytes + tok * kScaleBytesPerToken;\n                        const uint32_t bits = read_u32_le(page_ptr + scale_off);\n                        stage_scale[tok] = __uint_as_float(bits);\n                    }\n                }\n\n                for (int i = lane + valid_tokens * kPayloadBytesPerToken;\n                     i < kStageTokens * kPayloadBytesPerToken;\n                     i += 32) {\n                    stage_payload[i] = 0;\n                }\n                for (int tok = lane + valid_tokens; tok < kStageTokens; tok += 32) {\n                    stage_scale[tok] = 0.0f;\n                }\n\n                __syncwarp();\n            }\n        }\n\n        __syncthreads();\n\n        // ---------------- Consumer warp: all compute ----------------\n        if (warp_id == kConsumerWarp) {\n            // tcgen path setup (placeholder descriptors / idesc; kept explicit by design).\n            const uint32_t i_desc = (1U << 7U)   // placeholder atype field\n                                  | (1U << 10U)  // placeholder btype field\n                                  | ((uint32_t)kStageTokens >> 3U << 17U)\n                                  | ((uint32_t)128 >> 7U << 27U);\n\n            for (int s = 0; s < kNumStages; ++s) {\n                const int tile_id = tile_base + s;\n                if (tile_id >= num_tiles) {\n                    continue;\n                }\n\n                const int valid_tokens = stage_valid_tokens[s];\n                const int page_idx = stage_page_idx[s];\n                if (valid_tokens <= 0 || page_idx < 0 || page_idx >= num_pages) {\n                    continue;\n                }\n\n                // Wait for producer-issued TMA transaction for this stage.\n                if (kEnableAsyncTma && k_fp8_tmap != nullptr && lane == 0) {\n                    const int mbar_addr = cvta_to_shared_u32(&stage_mbar[s]);\n                    mbarrier_wait_parity(mbar_addr, stage_phase[s]);\n                    stage_phase[s] ^= 1;\n                }\n                __syncwarp();\n\n                // Explicit tcgen issue point in consumer warp.\n                if (kEnableTcgenMma && lane == 0) {\n                    const int q_addr = cvta_to_shared_u32(q_stage);\n                    const int k_addr = cvta_to_shared_u32(k_stage_payload + s * kStageTokens * kPayloadBytesPerToken);\n                    const uint64_t desc_a = make_desc_kmajor_128b(q_addr);\n                    const uint64_t desc_b = make_desc_kmajor_128b(k_addr);\n                    tcgen05_mma_f16_ss(/*tmem_c=*/0, desc_a, desc_b, i_desc, /*accumulate=*/0);\n                    tcgen05_commit(cvta_to_shared_u32(&stage_mbar[s]));\n                    tcgen05_wait_ld();\n                    tcgen05_fence_before_thread_sync();\n                }\n                __syncwarp();\n\n                // Scalar reduce path (kept as correctness anchor while tcgen path is refined).\n                const uint8_t* stage_payload = k_stage_payload + s * kStageTokens * kPayloadBytesPerToken;\n                const float* stage_scale = k_stage_scale + s * kStageTokens;\n\n                for (int t = lane; t < valid_tokens; t += 32) {\n                    const int seq_token = tile_id * kStageTokens + t;\n                    float reduced = 0.0f;\n                    const uint8_t* row = stage_payload + t * kPayloadBytesPerToken;\n                    const float scale = stage_scale[t];\n\n                    for (int h = 0; h < kNumHeads; ++h) {\n                        float dot = 0.0f;\n                        for (int d = 0; d < kHeadDim; ++d) {\n                            const int64_t q_off = (static_cast<int64_t>(b) * kNumHeads + h) * kHeadDim + d;\n                            float qv;\n                            if (q_elem_size == 1) {\n                                qv = fp8_byte_to_float(q_stage[h * kHeadDim + d]);\n                            } else if (q_elem_size == 2) {\n                                const __half* q_half = reinterpret_cast<const __half*>(q_index_bytes);\n                                qv = __half2float(q_half[q_off]);\n                            } else {\n                                const float* q_f32 = reinterpret_cast<const float*>(q_index_bytes);\n                                qv = q_f32[q_off];\n                            }\n                            const float kv = fp8_byte_to_float(row[d]) * scale;\n                            dot += qv * kv;\n                        }\n                        if (dot > 0.0f) {\n                            reduced += dot * w_stage[h];\n                        }\n                    }\n\n                    if (use_smem_scores) {\n                        final_scores_smem[seq_token] = reduced;\n                    } else {\n                        spill_b[seq_token] = reduced;\n                    }\n                }\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // Final exact top-k in consumer lane 0.\n    if (warp_id == kConsumerWarp && lane == 0) {\n        const int actual_topk = (topk < seq_len) ? topk : seq_len;\n        for (int i = 0; i < actual_topk; ++i) {\n            topk_scores[i] = -FLT_MAX;\n            topk_ids[i] = -1;\n        }\n\n        int topk_count = 0;\n        for (int seq_token = 0; seq_token < seq_len; ++seq_token) {\n            const float score = use_smem_scores ? final_scores_smem[seq_token] : spill_b[seq_token];\n            const int page_slot = seq_token / kPageSize;\n            const int offset = seq_token % kPageSize;\n\n            int page_idx = -1;\n            if (page_slot >= 0 && page_slot < max_num_pages) {\n                page_idx = block_table_b[page_slot];\n            }\n            if (page_idx < 0 || page_idx >= num_pages) {\n                continue;\n            }\n\n            const int token_idx = page_idx * kPageSize + offset;\n            insert_topk_desc(score, token_idx, topk_scores, topk_ids, topk_count, actual_topk);\n        }\n\n        for (int i = 0; i < actual_topk; ++i) {\n            out_b[i] = (i < topk_count) ? topk_ids[i] : -1;\n        }\n    }\n}\n\n// ---------------------------------------------------------------------------------\n// Host-side tensor-map encode helpers\n// ---------------------------------------------------------------------------------\nstatic void* g_k_fp8_tmap_dev = nullptr;\n\nstatic void ensure_k_fp8_tmap_on_device(const int8_t* k_index_cache_ptr, int num_pages) {\n    if (g_k_fp8_tmap_dev == nullptr) {\n        cudaMalloc(&g_k_fp8_tmap_dev, sizeof(CUtensorMap));\n    }\n\n    CUtensorMap k_fp8_tmap{};\n    cuInit(0);\n\n    constexpr cuuint32_t rank = 3;\n    const cuuint64_t globalDim[rank] = {\n        static_cast<cuuint64_t>(kPayloadBytesPerToken),\n        static_cast<cuuint64_t>(kPageSize),\n        static_cast<cuuint64_t>(num_pages),\n    };\n    const cuuint64_t globalStrides[rank - 1] = {\n        static_cast<cuuint64_t>(kPayloadBytesPerToken),\n        static_cast<cuuint64_t>(kPageBytes),\n    };\n    const cuuint32_t boxDim[rank] = {\n        static_cast<cuuint32_t>(kPayloadBytesPerToken),\n        static_cast<cuuint32_t>(kPageSize),\n        1U,\n    };\n    const cuuint32_t elementStrides[rank] = {1U, 1U, 1U};\n\n    const CUresult st = cuTensorMapEncodeTiled(\n        &k_fp8_tmap,\n        CU_TENSOR_MAP_DATA_TYPE_UINT8,\n        rank,\n        const_cast<int8_t*>(k_index_cache_ptr),\n        globalDim,\n        globalStrides,\n        boxDim,\n        elementStrides,\n        CU_TENSOR_MAP_INTERLEAVE_NONE,\n        CU_TENSOR_MAP_SWIZZLE_NONE,\n        CU_TENSOR_MAP_L2_PROMOTION_L2_128B,\n        CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n    );\n\n    if (st == CUDA_SUCCESS) {\n        cudaMemcpy(g_k_fp8_tmap_dev, &k_fp8_tmap, sizeof(CUtensorMap), cudaMemcpyHostToDevice);\n    } else {\n        // Best-effort mode: keep pointer null to skip TMA path if encode fails.\n        g_k_fp8_tmap_dev = nullptr;\n    }\n}\n\n// ---------------------------------------------------------------------------------\n// Launch entry\n// ---------------------------------------------------------------------------------\nvoid dsa_topk_indexer_launch(\n    torch::Tensor q_index_fp8,\n    torch::Tensor k_index_cache_fp8,\n    torch::Tensor weights,\n    torch::Tensor seq_lens,\n    torch::Tensor block_table,\n    torch::Tensor spill_scores,\n    torch::Tensor topk_indices\n) {\n    const int batch_size = static_cast<int>(q_index_fp8.size(0));\n    const int num_pages = static_cast<int>(k_index_cache_fp8.size(0));\n    const int max_num_pages = static_cast<int>(block_table.size(1));\n    const int spill_stride = static_cast<int>(spill_scores.size(1));\n    const int topk = static_cast<int>(topk_indices.size(1));\n    const int q_elem_size = static_cast<int>(q_index_fp8.element_size());\n\n    if (batch_size == 0 || topk == 0) {\n        return;\n    }\n\n    ensure_k_fp8_tmap_on_device(reinterpret_cast<const int8_t*>(k_index_cache_fp8.data_ptr()), num_pages);\n\n    int device = -1;\n    cudaGetDevice(&device);\n\n    int max_optin_smem = 0;\n    cudaDeviceGetAttribute(&max_optin_smem, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n    int dynamic_smem_bytes = kDesiredDynamicSmemBytes;\n    if (max_optin_smem > 0 && dynamic_smem_bytes > max_optin_smem) {\n        dynamic_smem_bytes = max_optin_smem;\n    }\n\n    cudaFuncSetAttribute(\n        dsa_topk_indexer_kernel,\n        cudaFuncAttributeMaxDynamicSharedMemorySize,\n        dynamic_smem_bytes\n    );\n    cudaFuncSetAttribute(\n        dsa_topk_indexer_kernel,\n        cudaFuncAttributePreferredSharedMemoryCarveout,\n        100\n    );\n\n    const int blocks = batch_size;\n    dsa_topk_indexer_kernel<<<blocks, kThreadsPerBlock, dynamic_smem_bytes>>>(\n        reinterpret_cast<const uint8_t*>(q_index_fp8.data_ptr()),\n        reinterpret_cast<const int8_t*>(k_index_cache_fp8.data_ptr()),\n        reinterpret_cast<const float*>(weights.data_ptr()),\n        reinterpret_cast<const int*>(seq_lens.data_ptr()),\n        reinterpret_cast<const int*>(block_table.data_ptr()),\n        g_k_fp8_tmap_dev,\n        reinterpret_cast<float*>(spill_scores.data_ptr()),\n        reinterpret_cast<int*>(topk_indices.data_ptr()),\n        batch_size,\n        num_pages,\n        max_num_pages,\n        spill_stride,\n        topk,\n        q_elem_size,\n        dynamic_smem_bytes\n    );\n}\n"
    }
  ],
  "description": ""
}