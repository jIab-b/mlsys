"**Modal ‚ùå failure**

Running on:
* GPU: `NVIDIA B200`
* CPU: `x86_64`
* Device count: `1`
* Runtime: `CUDA`
* Platform: `Linux-4.4.0-x86_64-with-glibc2.39`
* Torch: `2.9.1+cu129`
* Hostname: `modal`


## Tests:
```
batch: 1; num_pages: 8; seq_len: 512; seed: 42
```

## Program stdout:
```
[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=dsa_topk_indexer_singlefile_ext -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.12/dist-packages/torch/include -isystem /usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py312_cu129/dsa_topk_indexer_singlefile_ext/main.cpp -o main.o 
[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=dsa_topk_indexer_singlefile_ext -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.12/dist-packages/torch/include -isystem /usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -gencode=arch=compute_100a,code=sm_100a -std=c++17 -c /root/.cache/torch_extensions/py312_cu129/dsa_topk_indexer_singlefile_ext/cuda.cu -o cuda.cuda.o 
[3/3] c++ main.o cuda.cuda.o -shared -lcuda -L/usr/local/lib/python3.12/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o dsa_topk_indexer_singlefile_ext.so
--- stderr ---
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
                    ^^^^^^^^^^^^^^^^^^^
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/common/eval_base.py", line 569, in _make_test_runner
    return runner.run_single_test(test)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/common/eval_base.py", line 362, in run_single_test
    torch.cuda.synchronize()
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
    return torch._C._cuda_synchronize()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.AcceleratorError: CUDA error: misaligned address
Search for `cudaErrorMisalignedAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/test_kernels/dsa_index_2048/eval.py", line 43, in <module>
    sys.exit(main(DsaIndex2048EvalRunner()))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/common/eval_base.py", line 719, in main
    return run_testing(logger, pool, tests, runner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/__modal/volumes/vo-fpJzsF5Vh3fXKFeKjqGFTf/eval_suite/common/eval_base.py", line 606, in run_testing
    good, message = pool.apply(_make_test_runner, (runner, test))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/pool.py", line 360, in apply
    return self.apply_async(func, args, kwds).get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/pool.py", line 774, in get
    raise self._value
torch.AcceleratorError: CUDA error: misaligned address
Search for `cudaErrorMisalignedAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
```"